{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Implementation of CLD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Pierre Nugues\n",
    "\n",
    "Reimplementation of Google's _Compact language detector_ (CLD3) from a high-level description. Source: ``https://github.com/google/cld3``\n",
    "\n",
    "Still missing:\n",
    "* weighted average of embeddings, instead of average\n",
    "* generator or train_on_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Dataset: *Tatoeba*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tatoeba is a database of texts with language tags. The corpus is available here: https://tatoeba.org/eng/downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the dataset and we split the lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\tcmn\\t我們試試看！',\n",
       " '2\\tcmn\\t我该去睡觉了。',\n",
       " '3\\tcmn\\t你在干什麼啊？',\n",
       " '4\\tcmn\\t這是什麼啊？',\n",
       " '5\\tcmn\\t今天是６月１８号，也是Muiriel的生日！',\n",
       " '6\\tcmn\\t生日快乐，Muiriel！',\n",
       " '7\\tcmn\\tMuiriel现在20岁了。',\n",
       " '8\\tcmn\\t密码是\"Muiriel\"。',\n",
       " '9\\tcmn\\t我很快就會回來。',\n",
       " '10\\tcmn\\t我不知道。']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_raw = open('sentences.csv', encoding='utf8').read().strip()\n",
    "dataset_raw = dataset_raw.split('\\n')\n",
    "dataset_raw[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the fields and we remove possible whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8023136 texts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', 'cmn', '我們試試看！'), ('2', 'cmn', '我该去睡觉了。'), ('3', 'cmn', '你在干什麼啊？')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_raw = list(map(lambda x: tuple(x.split('\\t')), dataset_raw))\n",
    "dataset_raw = list(map(lambda x: tuple(map(str.strip, x)), dataset_raw))\n",
    "print(len(dataset_raw), 'texts')\n",
    "dataset_raw[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pad strings that are less than three characters. If not done, training will crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_raw)):\n",
    "    if len(dataset_raw[i][2]) == 0:\n",
    "        dataset_raw[i] = (dataset_raw[i][0], dataset_raw[i][1], dataset_raw[i][2] + '   ')\n",
    "    if len(dataset_raw[i][2]) == 1: \n",
    "        dataset_raw[i] = (dataset_raw[i][0], dataset_raw[i][1], dataset_raw[i][2] + '  ')\n",
    "    if len(dataset_raw[i][2]) == 2:\n",
    "        dataset_raw[i] = (dataset_raw[i][0], dataset_raw[i][1], dataset_raw[i][2] + ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle, seed\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "shuffle(dataset_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decimate the dataset to have faster training times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4116828', 'ara', 'حمّد الشمس!'),\n",
       " ('3422297', 'eng', 'Tom rarely goes to church.'),\n",
       " ('5875935', 'ukr', 'Я одягнув фартук.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DECIMATE = False\n",
    "if DECIMATE:\n",
    "    dataset_raw = dataset_raw[:int(len(dataset_raw)/10)]\n",
    "dataset_raw[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The languages. Some texts have no language, and some others are marked with \\\\\\\\N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = set([x[1] for x in dataset_raw])\n",
    "len(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the texts per language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_texts(dataset):\n",
    "    text_counts = {}\n",
    "    for record in dataset:\n",
    "        lang = record[1]\n",
    "        if lang in text_counts:\n",
    "            text_counts[lang] += 1\n",
    "        else:\n",
    "            text_counts[lang] = 1\n",
    "    return text_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Languages with the most examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eng', 1264754),\n",
       " ('ita', 738799),\n",
       " ('rus', 732078),\n",
       " ('tur', 684619),\n",
       " ('epo', 609518),\n",
       " ('deu', 488568),\n",
       " ('fra', 402078),\n",
       " ('por', 347430),\n",
       " ('spa', 314873),\n",
       " ('hun', 269208),\n",
       " ('ber', 226376),\n",
       " ('heb', 195329),\n",
       " ('jpn', 187684),\n",
       " ('ukr', 154466),\n",
       " ('kab', 129245),\n",
       " ('fin', 106936),\n",
       " ('nld', 104337),\n",
       " ('pol', 99754),\n",
       " ('mkd', 77778),\n",
       " ('cmn', 60993),\n",
       " ('mar', 54656),\n",
       " ('dan', 43995),\n",
       " ('lit', 38408),\n",
       " ('ces', 37596),\n",
       " ('toki', 35017)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_counts = count_texts(dataset_raw)\n",
    "langs = sorted(text_counts.keys(), key=text_counts.get, reverse=True)\n",
    "[(lang, text_counts[lang]) for lang in langs][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider languages that have more than 3,000 examples in the dataset or we only use those in French, English, and Swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eng', 'ita', 'rus', 'tur', 'epo', 'deu', 'fra', 'por', 'spa', 'hun', 'ber', 'heb', 'jpn', 'ukr', 'kab', 'fin', 'nld', 'pol', 'mkd', 'cmn', 'mar', 'dan', 'lit', 'ces', 'toki', 'swe', 'ara', 'lat', 'ell', 'srp', 'ina', 'bul', 'pes', 'ron', 'nds', 'tlh', 'jbo', 'nob', 'tat', 'tgl', 'ind', 'bel', 'hin', 'isl', 'vie', 'lfn', 'uig', 'bre', 'tuk', 'kor', 'ile', 'eus', 'cat', 'yue', 'oci', 'hrv', 'ido', 'aze', 'ben', 'glg', 'wuu', 'mhr', 'slk', 'afr', 'avk', 'cor', 'run', 'gos', 'vol', 'est']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMALL_LANGUAGE_SET = False\n",
    "considered_langs = [lang for lang in langs if text_counts[lang] > 3000]\n",
    "if SMALL_LANGUAGE_SET:\n",
    "    considered_langs = ['fra', 'eng', 'swe']\n",
    "print(considered_langs)\n",
    "LANG_NBR = len(considered_langs)\n",
    "LANG_NBR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the texts in these languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7934544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('4116828', 'ara', 'حمّد الشمس!'),\n",
       " ('3422297', 'eng', 'Tom rarely goes to church.'),\n",
       " ('5875935', 'ukr', 'Я одягнув фартук.'),\n",
       " ('5012913', 'por', 'Foi uma história estranha.'),\n",
       " ('108532', 'jpn', '彼は義務の観念がすっかりなくなっている。')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(filter(lambda x: x[1] in considered_langs, dataset_raw))\n",
    "print(len(dataset))\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Count Characters Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use hash codes to convert ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of codes we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARS = 4096\n",
    "MAX_BIGRAMS = 4096\n",
    "MAX_TRIGRAMS = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the counts as in CLD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(d):\n",
    "    sum_chars = sum(d.values())\n",
    "    d = {k:v/sum_chars for k, v in d.items()}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the hash code and we add one to avoid a value of 0 as it is padding symbol in the subsequent matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def hash_chars(string, lc=True):\n",
    "    if lc:\n",
    "        string = string.lower()\n",
    "    hash_codes = map(lambda x: hash(x) % MAX_CHARS + 1, string)\n",
    "    d = dict(Counter(hash_codes))\n",
    "    d = normalize(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_bigrams(string, lc=True):\n",
    "    if lc:\n",
    "        string = string.lower()\n",
    "    bigrams = [string[i:i + 2] for i in range(len(string) - 1)]\n",
    "    hash_codes = map(lambda x: hash(x) % MAX_BIGRAMS + 1, bigrams)\n",
    "    d = dict(Counter(hash_codes))\n",
    "    d = normalize(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_trigrams(string, lc=True):\n",
    "    if lc:\n",
    "        string = string.lower()\n",
    "    trigrams = [string[i:i + 3] for i in range(len(string) - 2)]\n",
    "    hash_codes = map(lambda x: hash(x) % MAX_TRIGRAMS + 1, trigrams)\n",
    "    d = dict(Counter(hash_codes))\n",
    "    d = normalize(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1720: 0.05,\n",
       " 912: 0.1,\n",
       " 1174: 0.15,\n",
       " 1286: 0.05,\n",
       " 1626: 0.1,\n",
       " 2081: 0.1,\n",
       " 1644: 0.05,\n",
       " 1625: 0.05,\n",
       " 962: 0.05,\n",
       " 549: 0.05,\n",
       " 3287: 0.05,\n",
       " 2507: 0.05,\n",
       " 298: 0.05,\n",
       " 3436: 0.05,\n",
       " 2847: 0.05}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_chars(\"Let's try something.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{612: 0.05263157894736842,\n",
       " 212: 0.10526315789473684,\n",
       " 1199: 0.05263157894736842,\n",
       " 1637: 0.05263157894736842,\n",
       " 3578: 0.05263157894736842,\n",
       " 834: 0.05263157894736842,\n",
       " 1967: 0.05263157894736842,\n",
       " 354: 0.05263157894736842,\n",
       " 2240: 0.05263157894736842,\n",
       " 4024: 0.05263157894736842,\n",
       " 1038: 0.05263157894736842,\n",
       " 52: 0.05263157894736842,\n",
       " 3847: 0.05263157894736842,\n",
       " 2421: 0.05263157894736842,\n",
       " 1845: 0.05263157894736842,\n",
       " 3235: 0.05263157894736842,\n",
       " 406: 0.05263157894736842,\n",
       " 1295: 0.05263157894736842}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_bigrams(\"Let's try something.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2225: 0.05555555555555555,\n",
       " 3547: 0.05555555555555555,\n",
       " 3312: 0.05555555555555555,\n",
       " 2471: 0.05555555555555555,\n",
       " 707: 0.05555555555555555,\n",
       " 1164: 0.05555555555555555,\n",
       " 2432: 0.05555555555555555,\n",
       " 2986: 0.05555555555555555,\n",
       " 1721: 0.05555555555555555,\n",
       " 642: 0.05555555555555555,\n",
       " 50: 0.05555555555555555,\n",
       " 3620: 0.05555555555555555,\n",
       " 3686: 0.05555555555555555,\n",
       " 1255: 0.05555555555555555,\n",
       " 1127: 0.05555555555555555,\n",
       " 3327: 0.05555555555555555,\n",
       " 280: 0.05555555555555555,\n",
       " 3610: 0.05555555555555555}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_trigrams(\"Let's try something.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the ngrams in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the character, bigram, and trigram counts to the texts. The format is:\n",
    "`(text_id, language_id, text, char_cnt, bigram_cnt, trigram_cnt)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4116828',\n",
       "  'ara',\n",
       "  'حمّد الشمس!',\n",
       "  {107: 0.09090909090909091,\n",
       "   1924: 0.18181818181818182,\n",
       "   2548: 0.09090909090909091,\n",
       "   3809: 0.09090909090909091,\n",
       "   2081: 0.09090909090909091,\n",
       "   1499: 0.09090909090909091,\n",
       "   4001: 0.09090909090909091,\n",
       "   876: 0.09090909090909091,\n",
       "   725: 0.09090909090909091,\n",
       "   1642: 0.09090909090909091},\n",
       "  {64: 0.1,\n",
       "   1215: 0.1,\n",
       "   70: 0.1,\n",
       "   823: 0.1,\n",
       "   423: 0.1,\n",
       "   224: 0.1,\n",
       "   4086: 0.1,\n",
       "   2854: 0.1,\n",
       "   2700: 0.1,\n",
       "   1239: 0.1},\n",
       "  {21: 0.1111111111111111,\n",
       "   3898: 0.1111111111111111,\n",
       "   3814: 0.1111111111111111,\n",
       "   1660: 0.1111111111111111,\n",
       "   1512: 0.1111111111111111,\n",
       "   2122: 0.1111111111111111,\n",
       "   484: 0.1111111111111111,\n",
       "   3715: 0.1111111111111111,\n",
       "   1440: 0.1111111111111111}),\n",
       " ('3422297',\n",
       "  'eng',\n",
       "  'Tom rarely goes to church.',\n",
       "  {1174: 0.07692307692307693,\n",
       "   962: 0.11538461538461539,\n",
       "   549: 0.038461538461538464,\n",
       "   2081: 0.15384615384615385,\n",
       "   1644: 0.11538461538461539,\n",
       "   641: 0.038461538461538464,\n",
       "   912: 0.07692307692307693,\n",
       "   1720: 0.038461538461538464,\n",
       "   1625: 0.038461538461538464,\n",
       "   3436: 0.038461538461538464,\n",
       "   1626: 0.038461538461538464,\n",
       "   146: 0.07692307692307693,\n",
       "   3287: 0.07692307692307693,\n",
       "   2503: 0.038461538461538464,\n",
       "   2847: 0.038461538461538464},\n",
       "  {1556: 0.08,\n",
       "   52: 0.04,\n",
       "   1340: 0.04,\n",
       "   1016: 0.04,\n",
       "   1512: 0.04,\n",
       "   3940: 0.04,\n",
       "   1959: 0.04,\n",
       "   2670: 0.04,\n",
       "   2771: 0.04,\n",
       "   2240: 0.04,\n",
       "   2046: 0.04,\n",
       "   2248: 0.04,\n",
       "   3001: 0.04,\n",
       "   836: 0.04,\n",
       "   3578: 0.04,\n",
       "   834: 0.04,\n",
       "   1553: 0.04,\n",
       "   1832: 0.04,\n",
       "   3127: 0.08,\n",
       "   1008: 0.04,\n",
       "   1982: 0.04,\n",
       "   516: 0.04,\n",
       "   3510: 0.04},\n",
       "  {309: 0.041666666666666664,\n",
       "   3069: 0.041666666666666664,\n",
       "   3183: 0.041666666666666664,\n",
       "   1941: 0.041666666666666664,\n",
       "   1134: 0.041666666666666664,\n",
       "   3592: 0.041666666666666664,\n",
       "   469: 0.041666666666666664,\n",
       "   1722: 0.041666666666666664,\n",
       "   3063: 0.041666666666666664,\n",
       "   859: 0.041666666666666664,\n",
       "   803: 0.041666666666666664,\n",
       "   271: 0.041666666666666664,\n",
       "   36: 0.041666666666666664,\n",
       "   2472: 0.041666666666666664,\n",
       "   707: 0.041666666666666664,\n",
       "   789: 0.041666666666666664,\n",
       "   2385: 0.041666666666666664,\n",
       "   667: 0.041666666666666664,\n",
       "   1082: 0.041666666666666664,\n",
       "   2526: 0.041666666666666664,\n",
       "   1814: 0.041666666666666664,\n",
       "   1762: 0.041666666666666664,\n",
       "   2440: 0.041666666666666664,\n",
       "   2699: 0.041666666666666664})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_feat = list(map(lambda x: x + (hash_chars(x[-1]), hash_bigrams(x[-1]), hash_trigrams(x[-1])), \n",
    "                              dataset))\n",
    "dataset_feat[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([107, 1924, 2548, 3809, 2081, 1499, 4001, 876, 725, 1642])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_feat[0][3].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([64, 1215, 70, 823, 423, 224, 4086, 2854, 2700, 1239])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_feat[0][4].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract all the unique ngrams (hash codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = set([item for text in dataset_feat for item in list(text[3].keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_bigrams = set([item for text in dataset_feat for item in list(text[4].keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_trigrams = set([item for text in dataset_feat for item in list(text[5].keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Keras Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of the embedding indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3507"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_char_cnt = len(unique_chars)\n",
    "unique_char_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the max length of the feature vectors for the characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def length(i, j):\n",
    "    return len(dataset_feat[i][j].keys())\n",
    "\n",
    "max([length(i, 3) for i in range(len(dataset_feat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.240892608321285\n",
      "3.7984960581432814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "print(statistics.mean([length(i, 3) for i in range(len(dataset_feat))]))\n",
    "print(statistics.stdev([length(i, 3) for i in range(len(dataset_feat))]))\n",
    "statistics.median([length(i, 3) for i in range(len(dataset_feat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_bigram_cnt = len(unique_bigrams)\n",
    "unique_bigram_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the max length of the feature vectors for the bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([length(i, 4) for i in range(len(dataset_feat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.877754915720423\n",
      "14.138026362815763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(statistics.mean([length(i, 4) for i in range(len(dataset_feat))]))\n",
    "print(statistics.stdev([length(i, 4) for i in range(len(dataset_feat))]))\n",
    "statistics.median([length(i, 4) for i in range(len(dataset_feat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_trigram_cnt = len(unique_trigrams)\n",
    "unique_trigram_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the max length of the feature vectors for the trigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "748"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([length(i, 5) for i in range(len(dataset_feat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.24697600769496\n",
      "19.16247970967665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(statistics.mean([length(i, 5) for i in range(len(dataset_feat))]))\n",
    "print(statistics.stdev([length(i, 5) for i in range(len(dataset_feat))]))\n",
    "statistics.median([length(i, 5) for i in range(len(dataset_feat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN_CHARS = 20\n",
    "MAXLEN_BIGRAMS = 50\n",
    "MAXLEN_TRIGRAMS = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bigram_input (InputLayer)       [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "trigram_input (InputLayer)      [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 64)       262208      char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 64)       262208      bigram_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 70, 64)       262208      trigram_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 64)           0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 192)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          98816       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 70)           35910       dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 921,350\n",
      "Trainable params: 921,350\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import layers, optimizers, backend\n",
    "\n",
    "# Char input\n",
    "char_input = Input(shape=(20,), dtype='int32', name='char_input')\n",
    "embedded_chars = layers.Embedding(MAX_CHARS + 1, 64, mask_zero=True)(char_input)\n",
    "flattened_chars = layers.GlobalAveragePooling1D()(embedded_chars)\n",
    "#flattened_chars = layers.Lambda(lambda x: backend.mean(x, axis=1))(embedded_chars)\n",
    "\n",
    "# Bigram input\n",
    "bigram_input = Input(shape=(50,), dtype='int32', name='bigram_input')\n",
    "embedded_bigrams = layers.Embedding(MAX_BIGRAMS + 1, 64, mask_zero=True)(bigram_input)\n",
    "flattened_bigrams = layers.GlobalAveragePooling1D()(embedded_bigrams)\n",
    "#flattened_bigrams = layers.Lambda(lambda x: backend.mean(x, axis=1))(embedded_bigrams)\n",
    "\n",
    "# Trigram input\n",
    "trigram_input = Input(shape=(70,), dtype='int32', name='trigram_input')\n",
    "embedded_trigrams = layers.Embedding(MAX_TRIGRAMS + 1, 64, mask_zero=True)(trigram_input)\n",
    "flattened_trigrams = layers.GlobalAveragePooling1D()(embedded_trigrams)\n",
    "#flattened_trigrams = layers.Lambda(lambda x: backend.mean(x, axis=1))(embedded_trigrams)\n",
    "\n",
    "flattened = layers.concatenate([flattened_chars, flattened_bigrams, flattened_trigrams], axis=-1)\n",
    "dense_layer = layers.Dense(512, activation='relu')(flattened)\n",
    "# dense_layer = layers.Dropout(0.6)(dense_layer)\n",
    "lang_output = layers.Dense(LANG_NBR, activation='softmax')(dense_layer)\n",
    "model = Model([char_input, bigram_input, trigram_input], lang_output)\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building $\\mathbf{X}$ and $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $\\mathbf{X}$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the keys (hash codes) from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107, 1924, 2548, 3809, 2081, 1499, 4001, 876, 725, 1642], [1174, 962, 549, 2081, 1644, 641, 912, 1720, 1625, 3436, 1626, 146, 3287, 2503, 2847]]\n"
     ]
    }
   ],
   "source": [
    "X_list_chars = [list(dataset_feat[i][3].keys()) for i in range(len(dataset_feat))]\n",
    "print(X_list_chars[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[64, 1215, 70, 823, 423, 224, 4086, 2854, 2700, 1239], [1556, 52, 1340, 1016, 1512, 3940, 1959, 2670, 2771, 2240, 2046, 2248, 3001, 836, 3578, 834, 1553, 1832, 3127, 1008, 1982, 516, 3510]]\n"
     ]
    }
   ],
   "source": [
    "X_list_bigrams = [list(dataset_feat[i][4].keys()) for i in range(len(dataset_feat))]\n",
    "print(X_list_bigrams[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21, 3898, 3814, 1660, 1512, 2122, 484, 3715, 1440], [309, 3069, 3183, 1941, 1134, 3592, 469, 1722, 3063, 859, 803, 271, 36, 2472, 707, 789, 2385, 667, 1082, 2526, 1814, 1762, 2440, 2699]]\n"
     ]
    }
   ],
   "source": [
    "X_list_trigrams = [list(dataset_feat[i][5].keys()) for i in range(len(dataset_feat))]\n",
    "print(X_list_trigrams[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we pad the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 107, 1924, 2548, 3809, 2081, 1499, 4001,  876,  725, 1642,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [1174,  962,  549, 2081, 1644,  641,  912, 1720, 1625, 3436, 1626,\n",
       "         146, 3287, 2503, 2847,    0,    0,    0,    0,    0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_chars = pad_sequences(X_list_chars, padding='post', maxlen=MAXLEN_CHARS)\n",
    "X_chars[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  64, 1215,   70,  823,  423,  224, 4086, 2854, 2700, 1239,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [1556,   52, 1340, 1016, 1512, 3940, 1959, 2670, 2771, 2240, 2046,\n",
       "        2248, 3001,  836, 3578,  834, 1553, 1832, 3127, 1008, 1982,  516,\n",
       "        3510,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bigrams = pad_sequences(X_list_bigrams, padding='post', maxlen=MAXLEN_BIGRAMS)\n",
    "X_bigrams[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  21, 3898, 3814, 1660, 1512, 2122,  484, 3715, 1440,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [ 309, 3069, 3183, 1941, 1134, 3592,  469, 1722, 3063,  859,  803,\n",
       "         271,   36, 2472,  707,  789, 2385,  667, 1082, 2526, 1814, 1762,\n",
       "        2440, 2699,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trigrams = pad_sequences(X_list_trigrams, padding='post', maxlen=MAXLEN_TRIGRAMS)\n",
    "X_trigrams[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $\\mathbf{y}$ vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ara', 'eng', 'ukr', 'por', 'jpn', 'ita', 'fra', 'eng', 'ita', 'eng']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list = [dataset_feat[i][1] for i in range(len(dataset_feat))]\n",
    "y_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rus': 0,\n",
       " 'eus': 1,\n",
       " 'lat': 2,\n",
       " 'hrv': 3,\n",
       " 'heb': 4,\n",
       " 'vol': 5,\n",
       " 'glg': 6,\n",
       " 'pes': 7,\n",
       " 'tlh': 8,\n",
       " 'mar': 9,\n",
       " 'ber': 10,\n",
       " 'epo': 11,\n",
       " 'toki': 12,\n",
       " 'ron': 13,\n",
       " 'fin': 14,\n",
       " 'nob': 15,\n",
       " 'lfn': 16,\n",
       " 'nld': 17,\n",
       " 'wuu': 18,\n",
       " 'nds': 19,\n",
       " 'srp': 20,\n",
       " 'kor': 21,\n",
       " 'pol': 22,\n",
       " 'deu': 23,\n",
       " 'eng': 24,\n",
       " 'vie': 25,\n",
       " 'cat': 26,\n",
       " 'aze': 27,\n",
       " 'dan': 28,\n",
       " 'avk': 29,\n",
       " 'ita': 30,\n",
       " 'spa': 31,\n",
       " 'bel': 32,\n",
       " 'tgl': 33,\n",
       " 'mkd': 34,\n",
       " 'est': 35,\n",
       " 'ben': 36,\n",
       " 'mhr': 37,\n",
       " 'jpn': 38,\n",
       " 'oci': 39,\n",
       " 'uig': 40,\n",
       " 'tuk': 41,\n",
       " 'jbo': 42,\n",
       " 'tat': 43,\n",
       " 'swe': 44,\n",
       " 'bre': 45,\n",
       " 'kab': 46,\n",
       " 'ukr': 47,\n",
       " 'hin': 48,\n",
       " 'por': 49,\n",
       " 'ile': 50,\n",
       " 'ara': 51,\n",
       " 'cmn': 52,\n",
       " 'run': 53,\n",
       " 'slk': 54,\n",
       " 'ind': 55,\n",
       " 'afr': 56,\n",
       " 'ces': 57,\n",
       " 'lit': 58,\n",
       " 'ido': 59,\n",
       " 'cor': 60,\n",
       " 'yue': 61,\n",
       " 'ina': 62,\n",
       " 'bul': 63,\n",
       " 'gos': 64,\n",
       " 'isl': 65,\n",
       " 'tur': 66,\n",
       " 'ell': 67,\n",
       " 'hun': 68,\n",
       " 'fra': 69}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_set = set(y_list)\n",
    "inx2lang = dict(enumerate(y_set))\n",
    "lang2inx = {v: k for k, v in inx2lang.items()}\n",
    "lang2inx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51, 24, 47]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list_num = list(map(lambda x: lang2inx[x], y_list))\n",
    "y_list_num[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode them as one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y_list_num)\n",
    "y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history = model.fit([X_chars, X_bigrams, X_trigrams], y, \\n                    epochs=3,\\n                   validation_split=0.2)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"history = model.fit([X_chars, X_bigrams, X_trigrams], y, \n",
    "                    epochs=3,\n",
    "                   validation_split=0.2)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We shuffle the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1794002, 2745398, 5006546, 365595, 5105077, 6536124, 2713909, 1837034, 6722572, 2354267]\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(X_chars.shape[0]))\n",
    "np.random.shuffle(indices)\n",
    "print(indices[:10])\n",
    "X_chars = X_chars[indices, :]\n",
    "X_bigrams = X_bigrams[indices, :]\n",
    "X_trigrams = X_trigrams[indices, :]\n",
    "y = np.array(y)[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = int(X_chars.shape[0] * 0.8)\n",
    "\n",
    "X_train_chars = X_chars[:training_examples, :]\n",
    "X_train_bigrams = X_bigrams[:training_examples, :]\n",
    "X_train_trigrams = X_trigrams[:training_examples, :]\n",
    "\n",
    "y_train = y[:training_examples]\n",
    "\n",
    "X_val_chars = X_chars[training_examples:, :]\n",
    "X_val_bigrams = X_bigrams[training_examples:, :]\n",
    "X_val_trigrams = X_trigrams[training_examples:, :]\n",
    "\n",
    "y_val = y[training_examples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 632, 2503, 1174,  641, 1626, 1816, 3436, 2081, 3942,  962, 1644,\n",
       "       2847,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_chars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6347635 samples, validate on 1586909 samples\n",
      "Epoch 1/3\n",
      "6347635/6347635 [==============================] - 1299s 205us/sample - loss: 0.0792 - acc: 0.9751 - val_loss: 0.0598 - val_acc: 0.9804\n",
      "Epoch 2/3\n",
      "6347635/6347635 [==============================] - 1285s 202us/sample - loss: 0.0554 - acc: 0.9825 - val_loss: 0.0586 - val_acc: 0.9822\n",
      "Epoch 3/3\n",
      "6347635/6347635 [==============================] - 1298s 205us/sample - loss: 0.0527 - acc: 0.9837 - val_loss: 0.0584 - val_acc: 0.9831\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_chars, X_train_bigrams, X_train_trigrams], \n",
    "                    y_train, \n",
    "                    epochs=3,\n",
    "                    validation_data=([X_val_chars, X_val_bigrams, X_val_trigrams], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.29119604e-34 2.06736934e-29 6.38253725e-29 4.93435618e-37\n",
      "  9.85048162e-32 0.00000000e+00 0.00000000e+00 1.02865266e-26\n",
      "  1.76564762e-34 4.02209204e-33 3.07658352e-18 4.25958699e-24\n",
      "  9.27007677e-37 1.46505669e-36 1.40878355e-32 2.63277501e-19\n",
      "  0.00000000e+00 1.00000000e+00 2.08595164e-31 3.37861718e-11\n",
      "  6.21861170e-27 1.72227218e-29 1.79640911e-28 2.35137519e-21\n",
      "  1.92654682e-19 7.97181881e-37 4.27708297e-35 0.00000000e+00\n",
      "  1.01088055e-16 5.38404625e-32 5.82194742e-30 1.93429564e-32\n",
      "  4.05722683e-33 1.18793228e-38 5.78134080e-36 1.18771087e-37\n",
      "  8.75117237e-26 1.45657480e-29 2.36645729e-28 7.64476208e-35\n",
      "  1.61438659e-36 2.98597700e-27 1.62687678e-26 3.09930782e-32\n",
      "  1.20343621e-22 2.77655788e-33 7.37242169e-21 1.00032606e-34\n",
      "  1.81362810e-30 1.54944651e-28 0.00000000e+00 2.20623508e-28\n",
      "  1.58873267e-22 0.00000000e+00 1.94891035e-38 1.13907377e-36\n",
      "  1.50884300e-14 1.95457018e-29 8.73848467e-36 0.00000000e+00\n",
      "  0.00000000e+00 7.94853075e-29 0.00000000e+00 1.22689496e-38\n",
      "  5.48426686e-11 1.92406042e-38 8.43173900e-24 7.57802240e-32\n",
      "  5.18225369e-24 3.56027088e-22]\n",
      " [3.59717233e-05 1.29615082e-05 3.41252926e-05 1.84829223e-05\n",
      "  2.70123213e-09 1.44874894e-05 1.03703037e-07 5.61684146e-07\n",
      "  2.18490095e-04 2.00443950e-09 1.07858737e-04 9.94255602e-01\n",
      "  4.16259338e-09 3.51584163e-07 3.37606762e-05 3.22294691e-05\n",
      "  4.16152204e-07 4.79615619e-06 5.51241630e-09 2.42538221e-08\n",
      "  1.03815109e-04 8.74042297e-11 1.12637048e-04 1.79535418e-04\n",
      "  4.02251862e-05 1.91018486e-07 2.59924786e-06 7.63355104e-07\n",
      "  6.00088424e-05 5.37990200e-05 1.20271761e-04 5.85268775e-04\n",
      "  7.25689233e-08 2.38254524e-05 2.53112420e-09 4.65811745e-08\n",
      "  1.41215040e-10 2.30561863e-08 4.34519738e-12 4.40331678e-06\n",
      "  2.62368283e-09 1.36115466e-08 1.74143897e-05 4.08136742e-08\n",
      "  3.33073585e-05 1.02915055e-05 2.19449321e-05 2.02692740e-06\n",
      "  9.96971394e-08 3.17442813e-04 1.73558556e-06 7.52739143e-05\n",
      "  1.90692404e-08 1.26266661e-06 3.33774369e-05 2.11492061e-05\n",
      "  1.02119415e-07 2.35028841e-04 3.97151482e-04 3.00591841e-04\n",
      "  6.67107045e-08 1.93477977e-11 2.58127443e-06 8.50764598e-12\n",
      "  2.83164603e-08 1.27789355e-03 2.62668123e-04 5.50285639e-09\n",
      "  4.78470747e-06 9.26137087e-04]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "1586909/1586909 [==============================] - 56s 35us/sample - loss: 0.0584 - acc: 0.9831\n",
      "Scores: [0.0584027066929787, 0.9831017]\n",
      "loss: 5.84%\n",
      "acc: 98.31%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = model.predict([X_val_chars, X_val_bigrams, X_val_trigrams])\n",
    "print(y_predicted[:2])\n",
    "print(y_val[:2])\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate([X_val_chars, X_val_bigrams, X_val_trigrams], y_val)\n",
    "print('Scores:', scores)\n",
    "list(map(lambda x: print(\"%s: %.2f%%\" % (x[0], x[1] * 100)), zip(model.metrics_names, scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indices of the predicted and true classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nld', 'epo', 'jpn', 'eng', 'fra', 'rus', 'epo', 'por', 'heb', 'fin']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_predicted, axis=-1)\n",
    "list(map(inx2lang.get, y_pred))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nld', 'epo', 'jpn', 'eng', 'fra', 'rus', 'epo', 'por', 'heb', 'fin']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_symb = np.argmax(y_val, axis=-1)\n",
    "list(map(inx2lang.get, y_val_symb))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The detailed F1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rus': 0,\n",
       " 'eus': 1,\n",
       " 'lat': 2,\n",
       " 'hrv': 3,\n",
       " 'heb': 4,\n",
       " 'vol': 5,\n",
       " 'glg': 6,\n",
       " 'pes': 7,\n",
       " 'tlh': 8,\n",
       " 'mar': 9,\n",
       " 'ber': 10,\n",
       " 'epo': 11,\n",
       " 'toki': 12,\n",
       " 'ron': 13,\n",
       " 'fin': 14,\n",
       " 'nob': 15,\n",
       " 'lfn': 16,\n",
       " 'nld': 17,\n",
       " 'wuu': 18,\n",
       " 'nds': 19,\n",
       " 'srp': 20,\n",
       " 'kor': 21,\n",
       " 'pol': 22,\n",
       " 'deu': 23,\n",
       " 'eng': 24,\n",
       " 'vie': 25,\n",
       " 'cat': 26,\n",
       " 'aze': 27,\n",
       " 'dan': 28,\n",
       " 'avk': 29,\n",
       " 'ita': 30,\n",
       " 'spa': 31,\n",
       " 'bel': 32,\n",
       " 'tgl': 33,\n",
       " 'mkd': 34,\n",
       " 'est': 35,\n",
       " 'ben': 36,\n",
       " 'mhr': 37,\n",
       " 'jpn': 38,\n",
       " 'oci': 39,\n",
       " 'uig': 40,\n",
       " 'tuk': 41,\n",
       " 'jbo': 42,\n",
       " 'tat': 43,\n",
       " 'swe': 44,\n",
       " 'bre': 45,\n",
       " 'kab': 46,\n",
       " 'ukr': 47,\n",
       " 'hin': 48,\n",
       " 'por': 49,\n",
       " 'ile': 50,\n",
       " 'ara': 51,\n",
       " 'cmn': 52,\n",
       " 'run': 53,\n",
       " 'slk': 54,\n",
       " 'ind': 55,\n",
       " 'afr': 56,\n",
       " 'ces': 57,\n",
       " 'lit': 58,\n",
       " 'ido': 59,\n",
       " 'cor': 60,\n",
       " 'yue': 61,\n",
       " 'ina': 62,\n",
       " 'bul': 63,\n",
       " 'gos': 64,\n",
       " 'isl': 65,\n",
       " 'tur': 66,\n",
       " 'ell': 67,\n",
       " 'hun': 68,\n",
       " 'fra': 69}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang2inx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_names = sorted(list(lang2inx.keys()), key=lambda x: lang2inx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         rus       0.99      1.00      0.99    146566\n",
      "         eus       0.89      0.92      0.91      1224\n",
      "         lat       0.95      0.95      0.95      6682\n",
      "         hrv       0.60      0.31      0.41      1024\n",
      "         heb       1.00      1.00      1.00     38896\n",
      "         vol       0.96      0.90      0.93       601\n",
      "         glg       0.94      0.48      0.64       865\n",
      "         pes       1.00      0.99      0.99      4353\n",
      "         tlh       0.98      0.99      0.98      3514\n",
      "         mar       0.99      1.00      1.00     10747\n",
      "         ber       0.88      0.91      0.89     45489\n",
      "         epo       1.00      1.00      1.00    122253\n",
      "        toki       0.99      1.00      1.00      6909\n",
      "         ron       0.99      0.93      0.96      3792\n",
      "         fin       0.99      0.99      0.99     21341\n",
      "         nob       0.88      0.68      0.77      2711\n",
      "         lfn       0.89      0.83      0.86      1630\n",
      "         nld       0.98      0.97      0.97     20888\n",
      "         wuu       0.90      0.78      0.84       859\n",
      "         nds       0.94      0.94      0.94      3566\n",
      "         srp       0.84      0.91      0.87      6147\n",
      "         kor       1.00      0.98      0.99      1396\n",
      "         pol       0.99      0.99      0.99     20061\n",
      "         deu       1.00      1.00      1.00     97695\n",
      "         eng       1.00      1.00      1.00    253346\n",
      "         vie       0.98      0.99      0.99      2042\n",
      "         cat       0.89      0.82      0.86      1169\n",
      "         aze       0.98      0.83      0.90       951\n",
      "         dan       0.87      0.97      0.92      8764\n",
      "         avk       0.87      0.91      0.89       781\n",
      "         ita       0.99      0.99      0.99    147846\n",
      "         spa       0.98      0.98      0.98     63107\n",
      "         bel       0.92      0.96      0.94      2451\n",
      "         tgl       0.96      0.98      0.97      2675\n",
      "         mkd       0.96      0.97      0.97     15704\n",
      "         est       0.88      0.81      0.84       564\n",
      "         ben       1.00      0.99      1.00       906\n",
      "         mhr       0.95      0.97      0.96       844\n",
      "         jpn       1.00      1.00      1.00     37446\n",
      "         oci       0.85      0.88      0.87      1107\n",
      "         uig       0.99      1.00      0.99      1549\n",
      "         tuk       0.96      0.91      0.93      1314\n",
      "         jbo       0.99      0.98      0.99      3064\n",
      "         tat       0.98      0.96      0.97      2663\n",
      "         swe       0.96      0.95      0.95      6896\n",
      "         bre       0.91      0.91      0.91      1384\n",
      "         kab       0.84      0.77      0.81     25670\n",
      "         ukr       0.99      0.96      0.98     30795\n",
      "         hin       0.99      0.98      0.98      2433\n",
      "         por       0.98      0.99      0.99     69735\n",
      "         ile       0.80      0.82      0.81      1306\n",
      "         ara       0.99      0.99      0.99      6635\n",
      "         cmn       0.97      0.99      0.98     12214\n",
      "         run       0.91      0.93      0.92       680\n",
      "         slk       0.88      0.68      0.77       843\n",
      "         ind       0.97      0.95      0.96      2556\n",
      "         afr       0.78      0.85      0.81       756\n",
      "         ces       0.95      0.98      0.97      7542\n",
      "         lit       0.98      0.98      0.98      7739\n",
      "         ido       0.90      0.79      0.84      1017\n",
      "         cor       0.95      0.93      0.94       775\n",
      "         yue       0.96      0.87      0.91      1175\n",
      "         ina       0.93      0.89      0.91      4898\n",
      "         bul       0.97      0.87      0.92      4910\n",
      "         gos       0.92      0.76      0.83       642\n",
      "         isl       0.99      0.97      0.98      2159\n",
      "         tur       1.00      1.00      1.00    136550\n",
      "         ell       1.00      1.00      1.00      6072\n",
      "         hun       1.00      0.99      1.00     53460\n",
      "         fra       0.99      1.00      0.99     80565\n",
      "\n",
      "    accuracy                           0.98   1586909\n",
      "   macro avg       0.94      0.92      0.93   1586909\n",
      "weighted avg       0.98      0.98      0.98   1586909\n",
      "\n",
      "0.9831017405534911\n",
      "0.9276806073745819\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "print(classification_report(y_val_symb, y_pred, target_names=lang_names))\n",
    "print(f1_score(y_val_symb, y_pred, average='micro'))\n",
    "print(f1_score(y_val_symb, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rus': 0, 'eus': 1, 'lat': 2, 'hrv': 3, 'heb': 4, 'vol': 5, 'glg': 6, 'pes': 7, 'tlh': 8, 'mar': 9, 'ber': 10, 'epo': 11, 'toki': 12, 'ron': 13, 'fin': 14, 'nob': 15, 'lfn': 16, 'nld': 17, 'wuu': 18, 'nds': 19, 'srp': 20, 'kor': 21, 'pol': 22, 'deu': 23, 'eng': 24, 'vie': 25, 'cat': 26, 'aze': 27, 'dan': 28, 'avk': 29, 'ita': 30, 'spa': 31, 'bel': 32, 'tgl': 33, 'mkd': 34, 'est': 35, 'ben': 36, 'mhr': 37, 'jpn': 38, 'oci': 39, 'uig': 40, 'tuk': 41, 'jbo': 42, 'tat': 43, 'swe': 44, 'bre': 45, 'kab': 46, 'ukr': 47, 'hin': 48, 'por': 49, 'ile': 50, 'ara': 51, 'cmn': 52, 'run': 53, 'slk': 54, 'ind': 55, 'afr': 56, 'ces': 57, 'lit': 58, 'ido': 59, 'cor': 60, 'yue': 61, 'ina': 62, 'bul': 63, 'gos': 64, 'isl': 65, 'tur': 66, 'ell': 67, 'hun': 68, 'fra': 69}\n",
      "[[146035      0      0 ...      0      0      0]\n",
      " [     0   1129      2 ...      0      4      0]\n",
      " [     1      4   6343 ...      0      3     40]\n",
      " ...\n",
      " [     0      0      0 ...   6069      0      0]\n",
      " [     1      1      5 ...      0  53173     10]\n",
      " [     1      1      8 ...      0      2  80189]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(lang2inx)\n",
    "cf = confusion_matrix(y_val_symb, y_pred)\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent confusions for some languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['fra', 'eng', 'swe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: fra\n",
      "Confusions: [    1     1     8     0     0     0     0     0     5     0    10     4\n",
      "     0     4     6     1     5     3     0     0     1     0     0    10\n",
      "    27     0    13     0     8    10    70    44     0     2     0     0\n",
      "     0     0     0    39     0     0     0     0     2    15     3     0\n",
      "     0    22    22     0     1     1     1     0     3     5     4     2\n",
      "     3     0    15     0     1     0     2     0     2 80189]\n",
      "Most confused: ita 0.0008688636504685657\n",
      "====\n",
      "Language: eng\n",
      "Confusions: [     2      2     21      0      1      3      0      0      7      0\n",
      "     27      5      1      4     20      2      2     55      0     15\n",
      "      4      0      7     36 252676      8     11      0     45      6\n",
      "     58     37      0     12      0      2      0      0      0      0\n",
      "      0      4      0      0      9     18      5      0      2     29\n",
      "     21      1      2      4      2      3     21     13      3      3\n",
      "      8      0     21      0      4      0     30      0      7     67]\n",
      "Most confused: fra 0.00026446046118746696\n",
      "====\n",
      "Language: swe\n",
      "Confusions: [   0    1    0    0    0    0    0    0    2    0    4    7    0    0\n",
      "   10   60    0    8    0    0    3    0    1    8   13    0    0    0\n",
      "  186    1    1    0    0    0    0    4    0    0    0    0    0    2\n",
      "    0    1 6556    4    2    0    0    4    2    0    0    0    0    1\n",
      "    0    0    1    0    0    0    0    0    0    2   10    0    1    1]\n",
      "Most confused: dan 0.02697215777262181\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "for language in languages:\n",
    "    if language not in lang2inx:\n",
    "        continue\n",
    "    print('Language:', language)\n",
    "    print('Confusions:', cf[lang2inx[language]])\n",
    "    print('Most confused:',\n",
    "          inx2lang[np.argsort(cf[lang2inx[language]])[-2]], \n",
    "          np.sort(cf[lang2inx[language]])[-2] / np.sum(cf[lang2inx[language]]))\n",
    "    print('====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fra'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Salut les gars !\"\n",
    "preds= model.predict(\n",
    "    [pad_sequences([list(hash_chars(sentence).keys())], padding='post', maxlen=MAXLEN_CHARS),\n",
    "    pad_sequences([list(hash_bigrams(sentence).keys())], padding='post', maxlen=MAXLEN_BIGRAMS),\n",
    "    pad_sequences([list(hash_trigrams(sentence).keys())], padding='post', maxlen=MAXLEN_TRIGRAMS)])\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eng'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello guys!\"\n",
    "preds= model.predict(\n",
    "    [pad_sequences([list(hash_chars(sentence).keys())], padding='post', maxlen=MAXLEN_CHARS),\n",
    "    pad_sequences([list(hash_bigrams(sentence).keys())], padding='post', maxlen=MAXLEN_BIGRAMS),\n",
    "    pad_sequences([list(hash_trigrams(sentence).keys())], padding='post', maxlen=MAXLEN_TRIGRAMS)])\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dan'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hejsan grabbar!\"\n",
    "preds= model.predict(\n",
    "    [pad_sequences([list(hash_chars(sentence).keys())], padding='post', maxlen=MAXLEN_CHARS),\n",
    "    pad_sequences([list(hash_bigrams(sentence).keys())], padding='post', maxlen=MAXLEN_BIGRAMS),\n",
    "    pad_sequences([list(hash_trigrams(sentence).keys())], padding='post', maxlen=MAXLEN_TRIGRAMS)])\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
