{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Implementation of CLD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Pierre Nugues\n",
    "\n",
    "Reimplementation of Google's _Compact language detector_ (CLD3) from a high-level description. Source: ``https://github.com/google/cld3``\n",
    "\n",
    "Still missing:\n",
    "* generator or train_on_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Dataset: *Tatoeba*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As dataset, we use Tatoeba: A database of texts with language tags. The corpus is available here: https://tatoeba.org/eng/downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the dataset and we split the lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\tcmn\\t我們試試看！',\n",
       " '2\\tcmn\\t我该去睡觉了。',\n",
       " '3\\tcmn\\t你在干什麼啊？',\n",
       " '4\\tcmn\\t這是什麼啊？',\n",
       " '5\\tcmn\\t今天是６月１８号，也是Muiriel的生日！',\n",
       " '6\\tcmn\\t生日快乐，Muiriel！',\n",
       " '7\\tcmn\\tMuiriel现在20岁了。',\n",
       " '8\\tcmn\\t密码是\"Muiriel\"。',\n",
       " '9\\tcmn\\t我很快就會回來。',\n",
       " '10\\tcmn\\t我不知道。']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_raw = open('sentences.csv', encoding='utf8').read().strip()\n",
    "dataset_raw = dataset_raw.split('\\n')\n",
    "dataset_raw[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the fields and we remove possible whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8023136 texts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', 'cmn', '我們試試看！'), ('2', 'cmn', '我该去睡觉了。'), ('3', 'cmn', '你在干什麼啊？')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_raw = list(map(lambda x: tuple(x.split('\\t')), dataset_raw))\n",
    "dataset_raw = list(map(lambda x: tuple(map(str.strip, x)), dataset_raw))\n",
    "print(len(dataset_raw), 'texts')\n",
    "dataset_raw[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pad strings that are less than three characters. If not done, training will crash. We also limit the length of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN_TEXT = 200\n",
    "for i in range(len(dataset_raw)):\n",
    "    if len(dataset_raw[i][2]) == 0:\n",
    "        dataset_raw[i] = (dataset_raw[i][0], dataset_raw[i][1], dataset_raw[i][2] + '   ')\n",
    "    if len(dataset_raw[i][2]) == 1: \n",
    "        dataset_raw[i] = (dataset_raw[i][0], dataset_raw[i][1], dataset_raw[i][2] + '  ')\n",
    "    if len(dataset_raw[i][2]) == 2:\n",
    "        dataset_raw[i] = (dataset_raw[i][0], dataset_raw[i][1], dataset_raw[i][2] + ' ')\n",
    "    dataset_raw[i] = (dataset_raw[i][0], dataset_raw[i][1], dataset_raw[i][2][:MAXLEN_TEXT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle, seed\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "shuffle(dataset_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decimate the dataset to have faster training times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3246907', 'ita', 'Non avete altro di cui parlare?'),\n",
       " ('6959233', 'fra', \"Quelqu'un a volé mon déjeuner.\"),\n",
       " ('1180391', 'deu', 'Ich habe nochmal überlegt und meine Meinung geändert.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DECIMATE = False\n",
    "if DECIMATE:\n",
    "    dataset_raw = dataset_raw[:int(len(dataset_raw)/10)]\n",
    "dataset_raw[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The languages. Some texts have no language tag, and some others are marked with the cryptic \\\\\\\\N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many languages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = set([x[1] for x in dataset_raw])\n",
    "len(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the texts per language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_texts(dataset):\n",
    "    text_counts = {}\n",
    "    for record in dataset:\n",
    "        lang = record[1]\n",
    "        if lang in text_counts:\n",
    "            text_counts[lang] += 1\n",
    "        else:\n",
    "            text_counts[lang] = 1\n",
    "    return text_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Languages with the most examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eng', 1264754),\n",
       " ('ita', 738799),\n",
       " ('rus', 732078),\n",
       " ('tur', 684619),\n",
       " ('epo', 609518),\n",
       " ('deu', 488568),\n",
       " ('fra', 402078),\n",
       " ('por', 347430),\n",
       " ('spa', 314873),\n",
       " ('hun', 269208),\n",
       " ('ber', 226376),\n",
       " ('heb', 195329),\n",
       " ('jpn', 187684),\n",
       " ('ukr', 154466),\n",
       " ('kab', 129245),\n",
       " ('fin', 106936),\n",
       " ('nld', 104337),\n",
       " ('pol', 99754),\n",
       " ('mkd', 77778),\n",
       " ('cmn', 60993),\n",
       " ('mar', 54656),\n",
       " ('dan', 43995),\n",
       " ('lit', 38408),\n",
       " ('ces', 37596),\n",
       " ('toki', 35017)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_counts = count_texts(dataset_raw)\n",
    "langs = sorted(text_counts.keys(), key=text_counts.get, reverse=True)\n",
    "[(lang, text_counts[lang]) for lang in langs][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider languages that have more than 3,000 examples in the dataset or we only use those in French, English, and Swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eng', 'ita', 'rus', 'tur', 'epo', 'deu', 'fra', 'por', 'spa', 'hun', 'ber', 'heb', 'jpn', 'ukr', 'kab', 'fin', 'nld', 'pol', 'mkd', 'cmn', 'mar', 'dan', 'lit', 'ces', 'toki', 'swe', 'ara', 'lat', 'ell', 'srp', 'ina', 'bul', 'pes', 'ron', 'nds', 'tlh', 'jbo', 'nob', 'tat', 'tgl', 'ind', 'bel', 'hin', 'isl', 'vie', 'lfn', 'uig', 'bre', 'tuk', 'kor', 'ile', 'eus', 'cat', 'yue', 'oci', 'hrv', 'ido', 'aze', 'ben', 'glg', 'wuu', 'mhr', 'slk', 'afr', 'avk', 'cor', 'run', 'gos', 'vol', 'est']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMALL_LANGUAGE_SET = False\n",
    "considered_langs = [lang for lang in langs if text_counts[lang] > 3000]\n",
    "if SMALL_LANGUAGE_SET:\n",
    "    considered_langs = ['fra', 'eng', 'swe']\n",
    "print(considered_langs)\n",
    "LANG_NBR = len(considered_langs)\n",
    "LANG_NBR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the texts in these languages. This will form our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7934544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('3246907', 'ita', 'Non avete altro di cui parlare?'),\n",
       " ('6959233', 'fra', \"Quelqu'un a volé mon déjeuner.\"),\n",
       " ('1180391', 'deu', 'Ich habe nochmal überlegt und meine Meinung geändert.'),\n",
       " ('5465336', 'epo', 'Ŝia nomo estas amuza.'),\n",
       " ('339801',\n",
       "  'spa',\n",
       "  'Están haciendo planes para colonizar el estado de Missouri.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(filter(lambda x: x[1] in considered_langs, dataset_raw))\n",
    "print(len(dataset))\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Count Characters Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use hash codes to convert ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of codes we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARS = 4096\n",
    "MAX_BIGRAMS = 8192\n",
    "MAX_TRIGRAMS = 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the counts as in CLD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(d):\n",
    "    sum_chars = sum(d.values())\n",
    "    d = {k:v/sum_chars for k, v in d.items()}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the hash code and we add one to avoid a value of 0 as it is padding symbol in the subsequent matrices.\n",
    "\n",
    "By default, we set the characters in lowercase and we sort the ngrams by frequency order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def hash_chars(string, lc=True, freq_sort=True):\n",
    "    if lc:\n",
    "        string = string.lower()\n",
    "    hash_codes = map(lambda x: hash(x) % MAX_CHARS + 1, string)\n",
    "    d = dict(Counter(hash_codes))\n",
    "    d = normalize(d)\n",
    "    if freq_sort:\n",
    "        k, v = zip(*sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "    else:\n",
    "        k, v = zip(*d.items())\n",
    "    return k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_bigrams(string, lc=True, freq_sort=True):\n",
    "    if lc:\n",
    "        string = string.lower()\n",
    "    bigrams = [string[i:i + 2] for i in range(len(string) - 1)]\n",
    "    hash_codes = map(lambda x: hash(x) % MAX_BIGRAMS + 1, bigrams)\n",
    "    d = dict(Counter(hash_codes))\n",
    "    d = normalize(d)\n",
    "    if freq_sort:\n",
    "        k, v = zip(*sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "    else:\n",
    "        k, v = zip(*d.items())\n",
    "    return k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_trigrams(string, lc=True, freq_sort=True):\n",
    "    if lc:\n",
    "        string = string.lower()\n",
    "    trigrams = [string[i:i + 3] for i in range(len(string) - 2)]\n",
    "    hash_codes = map(lambda x: hash(x) % MAX_TRIGRAMS + 1, trigrams)\n",
    "    d = dict(Counter(hash_codes))\n",
    "    d = normalize(d)\n",
    "    if freq_sort:\n",
    "        k, v = zip(*sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "    else:\n",
    "        k, v = zip(*d.items())\n",
    "    return k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google's example in CLD3's presentation's text (``https://github.com/google/cld3``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chars: ((208, 324, 541), (0.5, 0.3333333333333333, 0.16666666666666666))\n",
      "Bigrams: ((4366, 3769, 8123), (0.4, 0.4, 0.2))\n",
      "Trigrams: ((7381, 5853, 1734), (0.5, 0.25, 0.25))\n"
     ]
    }
   ],
   "source": [
    "print('Chars:', hash_chars('Banana'))\n",
    "print('Bigrams:', hash_bigrams('Banana'))\n",
    "print('Trigrams:', hash_trigrams('Banana'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chars: ((1798, 3017, 1439, 1376, 1222, 2656, 582, 1139, 3558, 3207, 1137, 1998, 324, 360, 3942), (0.15, 0.1, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05))\n",
      "Bigrams: ((7361, 6888, 5951, 4396, 170, 2820, 7025, 5300, 2004, 2358, 2661, 3195, 3169, 295, 7062, 4618, 6726, 3741), (0.10526315789473684, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842))\n",
      "Trigrams: ((2381, 1664, 7147, 2312, 6912, 1265, 7845, 2982, 6866, 328, 4308, 6065, 6090, 7248, 3746, 1145, 4175, 6034), (0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555))\n"
     ]
    }
   ],
   "source": [
    "print('Chars:', hash_chars(\"Let's try something.\"))\n",
    "print('Bigrams:', hash_bigrams(\"Let's try something.\"))\n",
    "print('Trigrams:', hash_trigrams(\"Let's try something.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the $\\mathbf{X}$ lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the character, bigram, and trigram counts of the texts and we create $\\mathbf{X}$ lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7934544/7934544 [14:26<00:00, 9161.50it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "X_list_inx_chars = []\n",
    "X_list_freq_chars = []\n",
    "X_list_inx_bigrams = []\n",
    "X_list_freq_bigrams = []\n",
    "X_list_inx_trigrams = []\n",
    "X_list_freq_trigrams = []\n",
    "\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    k, v = hash_chars(dataset[i][-1])\n",
    "    X_list_inx_chars.append(k)\n",
    "    X_list_freq_chars.append(v)\n",
    "    \n",
    "    k, v = hash_bigrams(dataset[i][-1])\n",
    "    X_list_inx_bigrams.append(k)\n",
    "    X_list_freq_bigrams.append(v)\n",
    "    \n",
    "    k, v = hash_trigrams(dataset[i][-1])\n",
    "    X_list_inx_trigrams.append(k)\n",
    "    X_list_freq_trigrams.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1376, 208, 3017, 582, 324, 3558, 1798, 1222, 1998, 3393, 3769, 3050, 3416, 1087, 585), (3416, 1376, 3017, 324, 499, 1222, 3558, 2112, 2656, 208, 3393, 3207, 3769, 2809, 582, 3942)]\n",
      "[(0.16129032258064516, 0.12903225806451613, 0.0967741935483871, 0.0967741935483871, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903), (0.13333333333333333, 0.13333333333333333, 0.1, 0.1, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333)]\n",
      "[(7312, 2879, 3512, 7948, 4768, 5074, 1556, 8133, 7361, 112, 6891, 5723, 707, 7025, 6774, 7800, 3904, 3949, 7235, 7220, 3832, 1329, 2610, 7198, 5408, 4609, 5108), (676, 3234, 5074, 5858, 6986, 3461, 1053, 5661, 7312, 1518, 3678, 6711, 7530, 1372, 2467, 1016, 1910, 4768, 3904, 6679, 7697, 796, 5322, 1030, 7451, 3252)]\n",
      "[(0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333), (0.06896551724137931, 0.06896551724137931, 0.06896551724137931, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655)]\n",
      "[(499, 6750, 1770, 2488, 6574, 498, 8058, 3663, 301, 1326, 7967, 4412, 2643, 5409, 1759, 6589, 1243, 2992, 5966, 5363, 241, 2102, 3884, 4721, 2077, 1693, 2964, 2081, 6522), (2509, 6276, 4046, 3162, 4184, 2306, 3028, 439, 1770, 4132, 334, 2884, 874, 7645, 6768, 1903, 1515, 7786, 6750, 206, 5262, 6470, 2310, 4941, 1525, 227, 639, 6782)]\n",
      "[(0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655), (0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571, 0.03571428571428571)]\n"
     ]
    }
   ],
   "source": [
    "print(X_list_inx_chars[:2])\n",
    "print(X_list_freq_chars[:2])\n",
    "print(X_list_inx_bigrams[:2])\n",
    "print(X_list_freq_bigrams[:2])\n",
    "print(X_list_inx_trigrams[:2])\n",
    "print(X_list_freq_trigrams[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract all the unique ngrams (hash codes). This part is not necessary to train the model and it could be skipped. We use it to determine if we have enough hash codes and the feature vector lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = set()\n",
    "for x_list_inx_chars in X_list_inx_chars:\n",
    "    unique_chars.update(set(x_list_inx_chars))\n",
    "\n",
    "unique_bigrams = set()\n",
    "for x_list_inx_bigrams in X_list_inx_bigrams:\n",
    "    unique_bigrams.update(set(x_list_inx_bigrams))\n",
    "\n",
    "unique_trigrams = set()\n",
    "for x_list_inx_trigrams in X_list_inx_trigrams:\n",
    "    unique_trigrams.update(set(x_list_inx_trigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the hash coding capacity. Have we used all the hash codes? Will there be collisions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3488"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_char_cnt = len(unique_chars)\n",
    "unique_char_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_bigram_cnt = len(unique_bigrams)\n",
    "unique_bigram_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_trigram_cnt = len(unique_trigrams)\n",
    "unique_trigram_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of the index vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How will we align the $\\mathbf{X}$ matrices? \n",
    "\n",
    "We will compute the maximal length of the exhaustive feature lists and we use: max(median, mean) plus two standard deviations (roughly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the max length of the feature vectors for the characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vect_lengths = [len(x_list_inx_chars) for x_list_inx_chars in X_list_inx_chars]\n",
    "max(char_vect_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.216995960952513\n",
      "3.767972324072555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "print(statistics.mean(char_vect_lengths))\n",
    "print(statistics.stdev(char_vect_lengths))\n",
    "statistics.median(char_vect_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the max length of the feature vectors for the bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vect_lengths = [len(x_list_inx_bigrams) for x_list_inx_bigrams in X_list_inx_bigrams]\n",
    "max(bigram_vect_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.785678672901682\n",
      "13.752314194807772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(statistics.mean(bigram_vect_lengths))\n",
    "print(statistics.stdev(bigram_vect_lengths))\n",
    "statistics.median(bigram_vect_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the max length of the feature vectors for the trigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_vect_lengths = [len(x_list_inx_trigrams) for x_list_inx_trigrams in X_list_inx_trigrams]\n",
    "max(trigram_vect_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.21706263145053\n",
      "18.270657997285003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(statistics.mean(trigram_vect_lengths))\n",
    "print(statistics.stdev(trigram_vect_lengths))\n",
    "statistics.median(trigram_vect_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the maximal lengths: max(median, mean) + 2 x stdev and a small margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN_CHARS = 30\n",
    "MAXLEN_BIGRAMS = 60\n",
    "MAXLEN_TRIGRAMS = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building $\\mathbf{X}$ and $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the matrices by converting the $\\mathbf{X}$ lists into arrays and padding them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $\\mathbf{X}$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pad the character sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1376,  208, 3017,  582,  324, 3558, 1798, 1222, 1998, 3393, 3769,\n",
       "        3050, 3416, 1087,  585,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [3416, 1376, 3017,  324,  499, 1222, 3558, 2112, 2656,  208, 3393,\n",
       "        3207, 3769, 2809,  582, 3942,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_inx_chars = pad_sequences(X_list_inx_chars, padding='post', maxlen=MAXLEN_CHARS)\n",
    "X_inx_chars[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The character frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16129032, 0.12903225, 0.09677419, 0.09677419, 0.06451613,\n",
       "        0.06451613, 0.06451613, 0.06451613, 0.06451613, 0.03225806,\n",
       "        0.03225806, 0.03225806, 0.03225806, 0.03225806, 0.03225806,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.13333334, 0.13333334, 0.1       , 0.1       , 0.06666667,\n",
       "        0.06666667, 0.06666667, 0.06666667, 0.03333334, 0.03333334,\n",
       "        0.03333334, 0.03333334, 0.03333334, 0.03333334, 0.03333334,\n",
       "        0.03333334, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_freq_chars = pad_sequences(X_list_freq_chars, padding='post', dtype='float32', maxlen=MAXLEN_CHARS)\n",
    "X_freq_chars[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for the bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7312, 2879, 3512, 7948, 4768, 5074, 1556, 8133, 7361,  112, 6891,\n",
       "        5723,  707, 7025, 6774, 7800, 3904, 3949, 7235, 7220, 3832, 1329,\n",
       "        2610, 7198, 5408, 4609, 5108,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0],\n",
       "       [ 676, 3234, 5074, 5858, 6986, 3461, 1053, 5661, 7312, 1518, 3678,\n",
       "        6711, 7530, 1372, 2467, 1016, 1910, 4768, 3904, 6679, 7697,  796,\n",
       "        5322, 1030, 7451, 3252,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_inx_bigrams = pad_sequences(X_list_inx_bigrams, padding='post', maxlen=MAXLEN_BIGRAMS)\n",
    "X_inx_bigrams[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06666667, 0.06666667, 0.06666667, 0.03333334, 0.03333334,\n",
       "        0.03333334, 0.03333334, 0.03333334, 0.03333334, 0.03333334,\n",
       "        0.03333334, 0.03333334, 0.03333334, 0.03333334, 0.03333334,\n",
       "        0.03333334, 0.03333334, 0.03333334, 0.03333334, 0.03333334,\n",
       "        0.03333334, 0.03333334, 0.03333334, 0.03333334, 0.03333334,\n",
       "        0.03333334, 0.03333334, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.06896552, 0.06896552, 0.06896552, 0.03448276, 0.03448276,\n",
       "        0.03448276, 0.03448276, 0.03448276, 0.03448276, 0.03448276,\n",
       "        0.03448276, 0.03448276, 0.03448276, 0.03448276, 0.03448276,\n",
       "        0.03448276, 0.03448276, 0.03448276, 0.03448276, 0.03448276,\n",
       "        0.03448276, 0.03448276, 0.03448276, 0.03448276, 0.03448276,\n",
       "        0.03448276, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_freq_bigrams = pad_sequences(X_list_freq_bigrams, padding='post', dtype='float32', maxlen=MAXLEN_BIGRAMS)\n",
    "X_freq_bigrams[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 499, 6750, 1770, 2488, 6574,  498, 8058, 3663,  301, 1326, 7967,\n",
       "        4412, 2643, 5409, 1759, 6589, 1243, 2992, 5966, 5363,  241, 2102,\n",
       "        3884, 4721, 2077, 1693, 2964, 2081, 6522,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [2509, 6276, 4046, 3162, 4184, 2306, 3028,  439, 1770, 4132,  334,\n",
       "        2884,  874, 7645, 6768, 1903, 1515, 7786, 6750,  206, 5262, 6470,\n",
       "        2310, 4941, 1525,  227,  639, 6782,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_inx_trigrams = pad_sequences(X_list_inx_trigrams, padding='post', maxlen=MAXLEN_TRIGRAMS)\n",
    "X_inx_trigrams[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03448276, 0.03448276, 0.03448276, 0.03448276, 0.03448276,\n",
       "        0.03448276, 0.03448276, 0.03448276, 0.03448276, 0.03448276,\n",
       "        0.03448276, 0.03448276, 0.03448276, 0.03448276, 0.03448276,\n",
       "        0.03448276, 0.03448276, 0.03448276, 0.03448276, 0.03448276,\n",
       "        0.03448276, 0.03448276, 0.03448276, 0.03448276, 0.03448276,\n",
       "        0.03448276, 0.03448276, 0.03448276, 0.03448276, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,\n",
       "        0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,\n",
       "        0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,\n",
       "        0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,\n",
       "        0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,\n",
       "        0.03571429, 0.03571429, 0.03571429, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_freq_trigrams = pad_sequences(X_list_freq_trigrams, padding='post', dtype='float32', maxlen=MAXLEN_TRIGRAMS)\n",
    "X_freq_trigrams[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $\\mathbf{y}$ vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ita', 'fra', 'deu', 'epo', 'spa', 'fra', 'deu', 'ile', 'tur', 'heb']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list = [dataset[i][1] for i in range(len(dataset))]\n",
    "y_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an index of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0,\n",
       " 'afr': 1,\n",
       " 'run': 2,\n",
       " 'gos': 3,\n",
       " 'pol': 4,\n",
       " 'uig': 5,\n",
       " 'fra': 6,\n",
       " 'lat': 7,\n",
       " 'mhr': 8,\n",
       " 'nob': 9,\n",
       " 'dan': 10,\n",
       " 'nds': 11,\n",
       " 'vie': 12,\n",
       " 'ron': 13,\n",
       " 'fin': 14,\n",
       " 'hun': 15,\n",
       " 'ukr': 16,\n",
       " 'bul': 17,\n",
       " 'ind': 18,\n",
       " 'avk': 19,\n",
       " 'vol': 20,\n",
       " 'pes': 21,\n",
       " 'glg': 22,\n",
       " 'bel': 23,\n",
       " 'slk': 24,\n",
       " 'eng': 25,\n",
       " 'bre': 26,\n",
       " 'wuu': 27,\n",
       " 'ido': 28,\n",
       " 'isl': 29,\n",
       " 'ben': 30,\n",
       " 'cmn': 31,\n",
       " 'tat': 32,\n",
       " 'toki': 33,\n",
       " 'jbo': 34,\n",
       " 'oci': 35,\n",
       " 'mar': 36,\n",
       " 'ara': 37,\n",
       " 'kab': 38,\n",
       " 'kor': 39,\n",
       " 'lfn': 40,\n",
       " 'est': 41,\n",
       " 'ita': 42,\n",
       " 'eus': 43,\n",
       " 'nld': 44,\n",
       " 'mkd': 45,\n",
       " 'rus': 46,\n",
       " 'ile': 47,\n",
       " 'tuk': 48,\n",
       " 'ber': 49,\n",
       " 'spa': 50,\n",
       " 'deu': 51,\n",
       " 'heb': 52,\n",
       " 'epo': 53,\n",
       " 'ces': 54,\n",
       " 'jpn': 55,\n",
       " 'tgl': 56,\n",
       " 'srp': 57,\n",
       " 'ell': 58,\n",
       " 'yue': 59,\n",
       " 'por': 60,\n",
       " 'hrv': 61,\n",
       " 'ina': 62,\n",
       " 'lit': 63,\n",
       " 'swe': 64,\n",
       " 'tur': 65,\n",
       " 'tlh': 66,\n",
       " 'aze': 67,\n",
       " 'cor': 68,\n",
       " 'hin': 69}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_set = set(y_list)\n",
    "inx2lang = dict(enumerate(y_set))\n",
    "lang2inx = {v: k for k, v in inx2lang.items()}\n",
    "lang2inx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 6, 51]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list_num = list(map(lambda x: lang2inx[x], y_list))\n",
    "y_list_num[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode them as one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y_list_num)\n",
    "y[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Sets\n",
    "We create a training and a validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We shuffle the indices\n",
    "We shuffle them again. This is not necessary, as we already shuffled the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1794002, 2745398, 5006546, 365595, 5105077, 6536124, 2713909, 1837034, 6722572, 2354267]\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(X_inx_chars.shape[0]))\n",
    "np.random.shuffle(indices)\n",
    "print(indices[:10])\n",
    "X_inx_chars = X_inx_chars[indices, :]\n",
    "X_freq_chars = X_freq_chars[indices, :]\n",
    "X_inx_bigrams = X_inx_bigrams[indices, :]\n",
    "X_freq_bigrams = X_freq_bigrams[indices, :]\n",
    "X_inx_trigrams = X_inx_trigrams[indices, :]\n",
    "X_freq_trigrams = X_freq_trigrams[indices, :]\n",
    "y = y[indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6347635"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_examples = int(X_inx_chars.shape[0] * 0.8)\n",
    "training_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_inx_chars = X_inx_chars[:training_examples, :]\n",
    "X_train_freq_chars = X_freq_chars[:training_examples, :]\n",
    "\n",
    "X_train_inx_bigrams = X_inx_bigrams[:training_examples, :]\n",
    "X_train_freq_bigrams = X_freq_bigrams[:training_examples, :]\n",
    "\n",
    "X_train_inx_trigrams = X_inx_trigrams[:training_examples, :]\n",
    "X_train_freq_trigrams = X_freq_trigrams[:training_examples, :]\n",
    "\n",
    "y_train = y[:training_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1376  582 3017 1137 1798 2809  360  208 2934 3393 3558 3769 3942    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.1923077 , 0.15384616, 0.11538462, 0.11538462, 0.11538462,\n",
       "       0.03846154, 0.03846154, 0.03846154, 0.03846154, 0.03846154,\n",
       "       0.03846154, 0.03846154, 0.03846154, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "print(X_train_inx_chars[0])\n",
    "X_train_freq_chars[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_inx_chars = X_inx_chars[training_examples:, :]\n",
    "X_val_freq_chars = X_freq_chars[training_examples:, :]\n",
    "\n",
    "X_val_inx_bigrams = X_inx_bigrams[training_examples:, :]\n",
    "X_val_freq_bigrams = X_freq_bigrams[training_examples:, :]\n",
    "\n",
    "X_val_inx_trigrams = X_inx_trigrams[training_examples:, :]\n",
    "X_val_freq_trigrams = X_freq_trigrams[training_examples:, :]\n",
    "\n",
    "y_val = y[training_examples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Keras Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create an architecture resembling CLD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_inx_input (InputLayer)     [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bigram_inx_input (InputLayer)   [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "trigram_inx_input (InputLayer)  [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_freq_input (InputLayer)    [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 64)       262208      char_inx_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bigram_freq_input (InputLayer)  [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 60, 64)       524352      bigram_inx_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "trigram_freq_input (InputLayer) [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 70, 64)       524352      trigram_inx_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose (TensorFl [(30, None, 64)]     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           char_freq_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(3,)]               0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_1 (Tensor [(60, None, 64)]     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           bigram_freq_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(3,)]               0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_2 (Tensor [(70, None, 64)]     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           trigram_freq_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(3,)]               0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 30)]         0           char_freq_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(30, None)]         0           tf_op_layer_transpose[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_unstack (TensorFlow [(), ()]             0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_unstack_1 (TensorFl [(), (), ()]         0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 60)]         0           bigram_freq_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(60, None)]         0           tf_op_layer_transpose_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_unstack_2 (TensorFl [(), ()]             0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_unstack_3 (TensorFl [(), (), ()]         0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 70)]         0           trigram_freq_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(70, None)]         0           tf_op_layer_transpose_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_unstack_4 (TensorFl [(), ()]             0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_unstack_5 (TensorFl [(), (), ()]         0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul (TensorFlowO [(None, None)]       0           tf_op_layer_Reshape[0][0]        \n",
      "                                                                 tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_unstack[0][0]        \n",
      "                                                                 tf_op_layer_unstack_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_1 (TensorFlo [(None, None)]       0           tf_op_layer_Reshape_3[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_unstack_2[0][0]      \n",
      "                                                                 tf_op_layer_unstack_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_2 (TensorFlo [(None, None)]       0           tf_op_layer_Reshape_6[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_unstack_4[0][0]      \n",
      "                                                                 tf_op_layer_unstack_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, None, 64)]   0           tf_op_layer_MatMul[0][0]         \n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, None, 64)]   0           tf_op_layer_MatMul_1[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, None, 64)]   0           tf_op_layer_MatMul_2[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 64)]         0           tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 64)]         0           tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 64)]         0           tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 192)          0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
      "                                                                 tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          98816       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 70)           35910       dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,445,638\n",
      "Trainable params: 1,445,638\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import layers, optimizers, backend\n",
    "\n",
    "# Char frequency input\n",
    "char_freq_input = Input(shape=(MAXLEN_CHARS,), dtype='float32', name='char_freq_input')\n",
    "\n",
    "# Char index input\n",
    "char_inx_input = Input(shape=(MAXLEN_CHARS,), dtype='int32', name='char_inx_input')\n",
    "embedded_chars = layers.Embedding(MAX_CHARS + 1, 64, mask_zero=True)(char_inx_input)\n",
    "\n",
    "# The weighted mean\n",
    "flattened_chars = backend.dot(char_freq_input, embedded_chars)[0]\n",
    "\n",
    "# Bigram freq input\n",
    "bigram_freq_input = Input(shape=(MAXLEN_BIGRAMS,), dtype='float32', name='bigram_freq_input')\n",
    "\n",
    "# Bigram index input\n",
    "bigram_inx_input = Input(shape=(MAXLEN_BIGRAMS,), dtype='int32', name='bigram_inx_input')\n",
    "embedded_bigrams = layers.Embedding(MAX_BIGRAMS + 1, 64, mask_zero=True)(bigram_inx_input)\n",
    "\n",
    "# The weighted mean\n",
    "flattened_bigrams = backend.dot(bigram_freq_input, embedded_bigrams)[0]\n",
    "\n",
    "# Trigram freq input\n",
    "trigram_freq_input = Input(shape=(MAXLEN_TRIGRAMS,), dtype='float32', name='trigram_freq_input')\n",
    "\n",
    "# Trigram index input\n",
    "trigram_inx_input = Input(shape=(MAXLEN_TRIGRAMS,), dtype='int32', name='trigram_inx_input')\n",
    "embedded_trigrams = layers.Embedding(MAX_TRIGRAMS + 1, 64, mask_zero=True)(trigram_inx_input)\n",
    "\n",
    "# The weighted mean\n",
    "flattened_trigrams = backend.dot(trigram_freq_input, embedded_trigrams)[0]\n",
    "\n",
    "flattened = layers.concatenate([flattened_chars, flattened_bigrams, flattened_trigrams], axis=-1)\n",
    "\n",
    "dense_layer = layers.Dense(512, activation='relu')(flattened)\n",
    "lang_output = layers.Dense(LANG_NBR, activation='softmax')(dense_layer)\n",
    "\n",
    "model = Model([char_inx_input, char_freq_input, \n",
    "               bigram_inx_input, bigram_freq_input, \n",
    "               trigram_inx_input, trigram_freq_input], lang_output)\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history = model.fit([X_chars, X_bigrams, X_trigrams], y, \\n                    epochs=3,\\n                   validation_split=0.2)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"history = model.fit([X_chars, X_bigrams, X_trigrams], y, \n",
    "                    epochs=3,\n",
    "                   validation_split=0.2)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6347635 samples, validate on 1586909 samples\n",
      "Epoch 1/3\n",
      "6347635/6347635 [==============================] - 1738s 274us/sample - loss: 0.1094 - acc: 0.9675 - val_loss: 0.0875 - val_acc: 0.9737\n",
      "Epoch 2/3\n",
      "6347635/6347635 [==============================] - 2037s 321us/sample - loss: 0.0786 - acc: 0.9767 - val_loss: 0.0811 - val_acc: 0.9763\n",
      "Epoch 3/3\n",
      "6347635/6347635 [==============================] - 2109s 332us/sample - loss: 0.0740 - acc: 0.9783 - val_loss: 0.0769 - val_acc: 0.9779\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_train_inx_chars, X_train_freq_chars, \n",
    "     X_train_inx_bigrams, X_train_freq_bigrams, \n",
    "     X_train_inx_trigrams, X_train_freq_trigrams], \n",
    "    y_train, \n",
    "    epochs=3,\n",
    "    validation_data=(\n",
    "        [X_val_inx_chars, X_val_freq_chars, \n",
    "         X_val_inx_bigrams, X_val_freq_bigrams,\n",
    "         X_val_inx_trigrams, X_val_freq_trigrams], \n",
    "        y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting and Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1586909/1586909 [==============================] - 121s 77us/sample - loss: 0.0769 - acc: 0.9779\n",
      "Scores: [0.076903877699372, 0.9778626]\n",
      "loss: 7.69%\n",
      "acc: 97.79%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate([X_val_inx_chars, X_val_freq_chars, \n",
    "                         X_val_inx_bigrams, X_val_freq_bigrams,\n",
    "                         X_val_inx_trigrams, X_val_freq_trigrams], y_val)\n",
    "print('Scores:', scores)\n",
    "list(map(lambda x: print(\"%s: %.2f%%\" % (x[0], x[1] * 100)), zip(model.metrics_names, scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the whole validation set and we get the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 1.95579659e-25\n",
      "  1.21977515e-20 8.12010985e-32 3.36048942e-20 6.49913382e-31\n",
      "  0.00000000e+00 0.00000000e+00 2.77644821e-35 3.83825223e-28\n",
      "  0.00000000e+00 1.27309104e-21 4.31282118e-32 1.72053009e-23\n",
      "  8.73432520e-19 3.46895509e-24 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 9.88139164e-29 0.00000000e+00 2.33776544e-18\n",
      "  0.00000000e+00 4.72338386e-20 5.96887584e-32 6.04027685e-38\n",
      "  0.00000000e+00 0.00000000e+00 3.23008743e-35 3.97554853e-34\n",
      "  9.03226323e-27 0.00000000e+00 3.81964463e-35 1.94402977e-32\n",
      "  3.13051901e-26 2.00930390e-29 9.99999642e-01 4.82056492e-37\n",
      "  0.00000000e+00 0.00000000e+00 1.35817392e-29 0.00000000e+00\n",
      "  2.77424462e-21 5.46700589e-28 2.42822219e-18 0.00000000e+00\n",
      "  0.00000000e+00 3.61760527e-07 2.04510976e-27 2.49610926e-23\n",
      "  0.00000000e+00 1.60811790e-27 2.02422896e-29 1.08059305e-31\n",
      "  0.00000000e+00 2.18431677e-30 0.00000000e+00 0.00000000e+00\n",
      "  1.61889934e-22 0.00000000e+00 0.00000000e+00 3.50571469e-37\n",
      "  1.06617790e-35 1.29855610e-30 2.39269575e-23 0.00000000e+00\n",
      "  2.29181549e-32 7.90321839e-26]\n",
      " [2.84003292e-08 2.85214491e-11 2.67184651e-14 3.86795520e-08\n",
      "  9.74591146e-08 1.99046400e-12 5.02723560e-04 1.95297432e-08\n",
      "  4.61910730e-07 1.81205149e-04 4.49169369e-04 2.44547755e-06\n",
      "  1.18953680e-10 6.66840592e-08 4.40279173e-08 2.77543499e-04\n",
      "  3.76836296e-09 2.56850513e-10 1.01353940e-12 5.49055135e-10\n",
      "  1.44627843e-12 4.15046118e-16 2.83257529e-10 7.80775236e-11\n",
      "  9.31011255e-06 3.08283009e-02 1.65505512e-07 1.24430050e-10\n",
      "  1.16336045e-10 6.64230465e-05 2.00729908e-13 9.53467097e-11\n",
      "  6.47246807e-08 9.52262315e-18 2.32137714e-13 1.19173622e-08\n",
      "  3.31619950e-17 7.28559635e-15 6.22890980e-07 6.60172750e-11\n",
      "  6.17995488e-10 3.90635835e-09 3.03118043e-07 2.02112709e-08\n",
      "  9.33954311e-07 1.79244897e-10 3.33972935e-06 1.08063574e-07\n",
      "  3.82458243e-09 1.60542072e-06 6.03645458e-05 9.67381060e-01\n",
      "  4.55114699e-15 3.42909261e-05 2.23114330e-06 1.31250323e-16\n",
      "  8.11763004e-15 2.22198491e-08 1.38974856e-15 3.27428189e-11\n",
      "  3.36162452e-06 4.71570560e-10 5.32536646e-08 1.23086996e-09\n",
      "  1.38896925e-04 5.45876901e-05 1.09237591e-10 2.14512364e-15\n",
      "  2.99569695e-13 4.57270578e-16]\n",
      " [2.29519376e-10 3.77245839e-11 1.05237160e-16 5.86052951e-12\n",
      "  1.61023195e-07 2.31704084e-18 9.99777257e-01 8.93547014e-09\n",
      "  5.05360139e-21 1.51985056e-11 7.41921469e-10 4.24296154e-11\n",
      "  7.77698218e-12 6.52089138e-09 3.45254803e-09 8.19479762e-10\n",
      "  3.14567328e-13 2.35123173e-18 4.29270775e-14 2.91486590e-09\n",
      "  6.19498679e-15 4.94261767e-15 2.99364711e-09 2.47559379e-13\n",
      "  1.58714986e-10 9.19852828e-05 5.72370425e-07 8.58530966e-13\n",
      "  5.39307621e-08 3.80390303e-11 1.42720430e-18 8.24977239e-12\n",
      "  1.25136446e-14 5.87194209e-16 9.25334137e-13 9.30606880e-09\n",
      "  2.89873897e-12 1.59706085e-11 7.89778915e-06 5.81150471e-13\n",
      "  1.56839418e-12 5.79946030e-12 1.98360084e-08 2.25877303e-11\n",
      "  4.64141401e-08 8.70305157e-19 7.27106960e-13 1.95252010e-08\n",
      "  1.66342404e-13 1.14904324e-04 9.81031576e-07 1.49579489e-06\n",
      "  1.32152232e-11 2.32179741e-06 1.11738663e-08 9.31142048e-15\n",
      "  4.41957228e-14 5.75473500e-08 8.10333484e-12 4.17138400e-15\n",
      "  1.95950150e-07 1.18579968e-09 1.12868781e-08 4.58586558e-10\n",
      "  2.73189871e-09 2.05330207e-06 7.73376432e-11 4.03907578e-18\n",
      "  6.74905390e-13 4.34559805e-13]\n",
      " [2.59869895e-03 2.10292555e-18 1.89053693e-19 8.22852225e-18\n",
      "  5.49188872e-09 3.11803696e-20 2.38772782e-06 3.74165854e-09\n",
      "  5.35773943e-23 3.89864952e-13 1.13762271e-13 5.85361007e-15\n",
      "  1.33137175e-12 9.00443262e-11 1.90124930e-10 1.19455126e-06\n",
      "  5.67439481e-17 1.32827500e-23 7.16260722e-17 1.70677934e-11\n",
      "  1.41187740e-15 5.25479378e-15 2.66435472e-05 2.45758106e-21\n",
      "  7.24482072e-11 2.42643718e-08 3.28672636e-13 7.94410273e-13\n",
      "  1.09428796e-12 1.62921502e-13 4.20336163e-17 9.32652888e-09\n",
      "  8.73032371e-19 6.12792386e-11 3.87857257e-09 6.31494856e-11\n",
      "  2.41116527e-09 3.14481080e-10 1.17411780e-09 1.33546378e-16\n",
      "  1.88244528e-07 4.28776237e-13 2.76465216e-05 9.14978163e-13\n",
      "  3.46708274e-12 4.72142551e-17 3.83418232e-13 5.61583931e-07\n",
      "  1.19434615e-10 1.09235918e-08 9.97321069e-01 4.35597045e-11\n",
      "  1.46688009e-19 9.58971391e-09 7.67755637e-10 8.57912688e-17\n",
      "  1.31921189e-14 1.53283809e-12 6.88603862e-15 1.46762709e-08\n",
      "  2.15791115e-05 8.71773736e-13 2.05299089e-09 3.41623228e-14\n",
      "  8.92447249e-13 5.44134027e-09 4.30414339e-16 3.35102964e-19\n",
      "  7.93444374e-20 1.41177651e-13]\n",
      " [2.20584515e-38 0.00000000e+00 6.09702226e-37 2.13065091e-38\n",
      "  6.74196139e-17 1.37975831e-35 2.47064609e-23 6.80308833e-24\n",
      "  0.00000000e+00 1.04694623e-26 5.28897248e-20 9.95948389e-33\n",
      "  0.00000000e+00 5.86892595e-24 1.16786001e-25 1.23778149e-17\n",
      "  1.64578967e-34 0.00000000e+00 2.57435261e-31 6.45096386e-33\n",
      "  0.00000000e+00 3.34276410e-31 1.62869852e-38 1.45313578e-34\n",
      "  3.95696022e-24 7.17408333e-21 1.12153016e-35 1.24373386e-35\n",
      "  5.42922614e-35 2.55387338e-29 3.05314684e-30 2.39403315e-35\n",
      "  0.00000000e+00 2.41642849e-36 1.58895696e-22 0.00000000e+00\n",
      "  0.00000000e+00 2.51199475e-31 3.81776714e-20 1.38136841e-37\n",
      "  3.82775629e-35 1.03528383e-30 1.15486030e-20 1.38887883e-34\n",
      "  1.54802296e-20 7.41021170e-34 3.04768455e-33 6.37813036e-37\n",
      "  6.09223939e-35 3.59999845e-15 3.69308300e-22 5.16930933e-21\n",
      "  0.00000000e+00 1.00000000e+00 8.09791725e-21 0.00000000e+00\n",
      "  0.00000000e+00 1.15998954e-22 1.14592117e-35 0.00000000e+00\n",
      "  1.10388081e-21 2.26480194e-27 1.34951145e-32 7.88159626e-28\n",
      "  7.17447546e-26 9.37717880e-27 1.21774237e-30 0.00000000e+00\n",
      "  2.09576265e-38 0.00000000e+00]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict([X_val_inx_chars, X_val_freq_chars, \n",
    "                             X_val_inx_bigrams, X_val_freq_bigrams,\n",
    "                             X_val_inx_trigrams, X_val_freq_trigrams])\n",
    "print(y_predicted[:5])\n",
    "print(y_val[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names of the predicted and true classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kab', 'deu', 'fra', 'spa', 'epo', 'deu', 'por', 'hun', 'eng', 'heb']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_predicted, axis=-1)\n",
    "list(map(inx2lang.get, y_pred))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kab', 'deu', 'fra', 'spa', 'epo', 'deu', 'por', 'hun', 'eng', 'heb']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_symb = np.argmax(y_val, axis=-1)\n",
    "list(map(inx2lang.get, y_val_symb))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The detailed F1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the precision and recall by language as well as the micro and macro F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_names = sorted(list(lang2inx.keys()), key=lambda x: lang2inx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.88      0.76      0.81      1212\n",
      "         afr       0.79      0.76      0.77       754\n",
      "         run       0.80      0.96      0.88       730\n",
      "         gos       0.95      0.69      0.80       662\n",
      "         pol       0.99      0.99      0.99     20176\n",
      "         uig       1.00      0.99      1.00      1530\n",
      "         fra       0.99      0.99      0.99     80124\n",
      "         lat       0.95      0.90      0.93      6570\n",
      "         mhr       0.94      0.94      0.94       844\n",
      "         nob       0.88      0.60      0.71      2784\n",
      "         dan       0.87      0.94      0.90      8765\n",
      "         nds       0.92      0.92      0.92      3471\n",
      "         vie       1.00      0.99      0.99      2093\n",
      "         ron       0.98      0.94      0.96      3867\n",
      "         fin       0.99      0.99      0.99     21335\n",
      "         hun       0.99      0.99      0.99     53914\n",
      "         ukr       0.97      0.97      0.97     30964\n",
      "         bul       0.89      0.89      0.89      4788\n",
      "         ind       0.95      0.95      0.95      2524\n",
      "         avk       0.86      0.91      0.88       805\n",
      "         vol       0.93      0.91      0.92       618\n",
      "         pes       1.00      0.99      0.99      4317\n",
      "         glg       0.91      0.38      0.54       836\n",
      "         bel       0.95      0.94      0.94      2504\n",
      "         slk       0.86      0.57      0.68       829\n",
      "         eng       0.99      1.00      0.99    253229\n",
      "         bre       0.91      0.90      0.91      1381\n",
      "         wuu       0.75      0.82      0.78       923\n",
      "         ido       0.76      0.77      0.76      1041\n",
      "         isl       1.00      0.96      0.98      2239\n",
      "         ben       1.00      1.00      1.00       996\n",
      "         cmn       0.97      0.97      0.97     12185\n",
      "         tat       0.97      0.97      0.97      2719\n",
      "        toki       0.99      0.99      0.99      6961\n",
      "         jbo       0.97      0.98      0.97      3060\n",
      "         oci       0.82      0.86      0.84      1095\n",
      "         mar       0.99      1.00      0.99     10881\n",
      "         ara       0.99      1.00      1.00      6663\n",
      "         kab       0.81      0.76      0.79     25785\n",
      "         kor       0.99      0.99      0.99      1311\n",
      "         lfn       0.84      0.81      0.82      1615\n",
      "         est       0.86      0.86      0.86       581\n",
      "         ita       0.99      0.99      0.99    148080\n",
      "         eus       0.96      0.86      0.91      1218\n",
      "         nld       0.97      0.96      0.97     20918\n",
      "         mkd       0.94      0.97      0.95     15445\n",
      "         rus       0.99      0.99      0.99    147015\n",
      "         ile       0.86      0.71      0.78      1284\n",
      "         tuk       0.93      0.88      0.90      1368\n",
      "         ber       0.87      0.90      0.88     45447\n",
      "         spa       0.97      0.98      0.97     62694\n",
      "         deu       0.99      0.99      0.99     97675\n",
      "         heb       1.00      1.00      1.00     38816\n",
      "         epo       0.99      0.99      0.99    122122\n",
      "         ces       0.96      0.96      0.96      7430\n",
      "         jpn       1.00      1.00      1.00     37553\n",
      "         tgl       0.98      0.94      0.96      2703\n",
      "         srp       0.83      0.90      0.86      6077\n",
      "         ell       1.00      1.00      1.00      6042\n",
      "         yue       0.95      0.91      0.93      1244\n",
      "         por       0.98      0.98      0.98     68908\n",
      "         hrv       0.75      0.20      0.31      1029\n",
      "         ina       0.89      0.83      0.86      4937\n",
      "         lit       0.98      0.98      0.98      7658\n",
      "         swe       0.95      0.95      0.95      6989\n",
      "         tur       1.00      0.99      0.99    136916\n",
      "         tlh       0.96      0.99      0.98      3448\n",
      "         aze       0.84      0.85      0.84      1012\n",
      "         cor       0.97      0.94      0.95       774\n",
      "         hin       0.98      0.95      0.96      2426\n",
      "\n",
      "    accuracy                           0.98   1586909\n",
      "   macro avg       0.93      0.90      0.91   1586909\n",
      "weighted avg       0.98      0.98      0.98   1586909\n",
      "\n",
      "Micro F1: 0.9778626247629826\n",
      "Macro F1 0.9115563801954688\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "print(classification_report(y_val_symb, y_pred, target_names=lang_names))\n",
    "print('Micro F1:', f1_score(y_val_symb, y_pred, average='micro'))\n",
    "print('Macro F1', f1_score(y_val_symb, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 0, 'afr': 1, 'run': 2, 'gos': 3, 'pol': 4, 'uig': 5, 'fra': 6, 'lat': 7, 'mhr': 8, 'nob': 9, 'dan': 10, 'nds': 11, 'vie': 12, 'ron': 13, 'fin': 14, 'hun': 15, 'ukr': 16, 'bul': 17, 'ind': 18, 'avk': 19, 'vol': 20, 'pes': 21, 'glg': 22, 'bel': 23, 'slk': 24, 'eng': 25, 'bre': 26, 'wuu': 27, 'ido': 28, 'isl': 29, 'ben': 30, 'cmn': 31, 'tat': 32, 'toki': 33, 'jbo': 34, 'oci': 35, 'mar': 36, 'ara': 37, 'kab': 38, 'kor': 39, 'lfn': 40, 'est': 41, 'ita': 42, 'eus': 43, 'nld': 44, 'mkd': 45, 'rus': 46, 'ile': 47, 'tuk': 48, 'ber': 49, 'spa': 50, 'deu': 51, 'heb': 52, 'epo': 53, 'ces': 54, 'jpn': 55, 'tgl': 56, 'srp': 57, 'ell': 58, 'yue': 59, 'por': 60, 'hrv': 61, 'ina': 62, 'lit': 63, 'swe': 64, 'tur': 65, 'tlh': 66, 'aze': 67, 'cor': 68, 'hin': 69}\n",
      "[[ 922    0    0 ...    0    0    0]\n",
      " [   0  570    0 ...    0    0    0]\n",
      " [   0    0  702 ...    0    0    0]\n",
      " ...\n",
      " [   0    0    0 ...  863    0    0]\n",
      " [   0    0    0 ...    0  726    0]\n",
      " [   0    0    0 ...    0    0 2294]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(lang2inx)\n",
    "cf = confusion_matrix(y_val_symb, y_pred)\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent confusions for some languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['fra', 'eng', 'swe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: fra\n",
      "Confusions: [   25     1     1     0     1     0 79430    27     0     1     2     2\n",
      "     0     4     2    14     0     0     0     4     0     0     1     0\n",
      "     0   130    14     0     4     0     0     0     0     1     2    46\n",
      "     0     0     4     0     5     1    99     1     9     0     0    17\n",
      "     0    18    91    29     0    23     1     0     1     3     0     0\n",
      "    63     0    35     1     0     9     2     0     0     0]\n",
      "Most confused: eng 0.0016224851480205681\n",
      "====\n",
      "Language: eng\n",
      "Confusions: [     3     13      7      2     15      0     56     34      0      0\n",
      "     14     15      0      4     12     20      0      0      3      4\n",
      "      0      0      0      0      0 252564      6      0      9      0\n",
      "      0      1      0      6      0      1      0      0     10      0\n",
      "      2      6     81      0     52      0      0     11      1     22\n",
      "     42     93      0     23      5      0      1      3      0      0\n",
      "     33      0     13      2      6     22      7      0      5      0]\n",
      "Most confused: deu 0.00036725651485414385\n",
      "====\n",
      "Language: swe\n",
      "Confusions: [   0    0    1    0    3    0    7    3    0   35  163    8    0    0\n",
      "    5    5    0    0    0    2    0    0    0    0    0   29    3    0\n",
      "    3    1    0    0    1    0    0    0    0    0    3    0    0    1\n",
      "    8    0    6    0    0    1    3    2    2   30    0    4    1    0\n",
      "    0    3    0    0    4    0    0    2 6644    5    1    0    0    0]\n",
      "Most confused: dan 0.023322363714408358\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "for language in languages:\n",
    "    if language not in lang2inx:\n",
    "        continue\n",
    "    print('Language:', language)\n",
    "    print('Confusions:', cf[lang2inx[language]])\n",
    "    print('Most confused:',\n",
    "          inx2lang[np.argsort(cf[lang2inx[language]])[-2]], \n",
    "          np.sort(cf[lang2inx[language]])[-2] / np.sum(cf[lang2inx[language]]))\n",
    "    print('====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us use the model to predict some short texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sentence):\n",
    "    char_items = hash_chars(sentence)\n",
    "    bigram_items = hash_bigrams(sentence)\n",
    "    trigram_items = hash_trigrams(sentence)\n",
    "    return [pad_sequences([char_items[0]], padding='post', maxlen=MAXLEN_CHARS),\n",
    "            pad_sequences([char_items[1]], dtype='float32', padding='post', maxlen=MAXLEN_CHARS),\n",
    "            pad_sequences([bigram_items[0]], padding='post', maxlen=MAXLEN_BIGRAMS),\n",
    "            pad_sequences([bigram_items[1]], dtype='float32', padding='post', maxlen=MAXLEN_BIGRAMS),\n",
    "            pad_sequences([trigram_items[0]], padding='post', maxlen=MAXLEN_TRIGRAMS),\n",
    "            pad_sequences([trigram_items[1]], dtype='float32', padding='post', maxlen=MAXLEN_TRIGRAMS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[208, 324, 541,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0]], dtype=int32),\n",
       " array([[0.5       , 0.33333334, 0.16666667, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "       dtype=float32),\n",
       " array([[4366, 3769, 8123,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0]], dtype=int32),\n",
       " array([[0.4, 0.4, 0.2, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "         0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "         0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "         0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "         0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]], dtype=float32),\n",
       " array([[7381, 5853, 1734,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]], dtype=int32),\n",
       " array([[0.5 , 0.25, 0.25, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "         0.  , 0.  , 0.  , 0.  ]], dtype=float32)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Banana\"\n",
    "extract_features(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fra'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Salut les gars !\"\n",
    "preds = model.predict(extract_features(sentence))\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eng'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hi guys!\"\n",
    "preds = model.predict(extract_features(sentence))\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dan'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hejsan grabbar!\"\n",
    "preds = model.predict(extract_features(sentence))\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simplifed model, where we do not use the ngram frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_inx_input (InputLayer)     [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bigram_inx_input (InputLayer)   [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "trigram_inx_input (InputLayer)  [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 30, 64)       262208      char_inx_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 60, 64)       524352      bigram_inx_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 70, 64)       524352      trigram_inx_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 64)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          98816       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 70)           35910       dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,445,638\n",
      "Trainable params: 1,445,638\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Char index input\n",
    "char_inx_input = Input(shape=(MAXLEN_CHARS,), dtype='int32', name='char_inx_input')\n",
    "embedded_chars = layers.Embedding(MAX_CHARS + 1, 64, mask_zero=True)(char_inx_input)\n",
    "\n",
    "# The weighted mean\n",
    "flattened_chars = layers.GlobalAveragePooling1D()(embedded_chars)\n",
    "\n",
    "# Bigram index input\n",
    "bigram_inx_input = Input(shape=(MAXLEN_BIGRAMS,), dtype='int32', name='bigram_inx_input')\n",
    "embedded_bigrams = layers.Embedding(MAX_BIGRAMS + 1, 64, mask_zero=True)(bigram_inx_input)\n",
    "\n",
    "# The weighted mean\n",
    "flattened_bigrams = layers.GlobalAveragePooling1D()(embedded_bigrams)\n",
    "\n",
    "# Trigram index input\n",
    "trigram_inx_input = Input(shape=(MAXLEN_TRIGRAMS,), dtype='int32', name='trigram_inx_input')\n",
    "embedded_trigrams = layers.Embedding(MAX_TRIGRAMS + 1, 64, mask_zero=True)(trigram_inx_input)\n",
    "\n",
    "# The weighted mean\n",
    "flattened_trigrams = layers.GlobalAveragePooling1D()(embedded_trigrams)\n",
    "\n",
    "flattened = layers.concatenate([flattened_chars, flattened_bigrams, flattened_trigrams], axis=-1)\n",
    "\n",
    "dense_layer = layers.Dense(512, activation='relu')(flattened)\n",
    "lang_output = layers.Dense(LANG_NBR, activation='softmax')(dense_layer)\n",
    "\n",
    "model2 = Model([char_inx_input, \n",
    "               bigram_inx_input, \n",
    "               trigram_inx_input], lang_output)\n",
    "model2.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6347635 samples, validate on 1586909 samples\n",
      "Epoch 1/3\n",
      "6347635/6347635 [==============================] - 1601s 252us/sample - loss: 0.0736 - acc: 0.9770 - val_loss: 0.0548 - val_acc: 0.9826\n",
      "Epoch 2/3\n",
      "6347635/6347635 [==============================] - 1269s 200us/sample - loss: 0.0510 - acc: 0.9839 - val_loss: 0.0533 - val_acc: 0.9837\n",
      "Epoch 3/3\n",
      "6347635/6347635 [==============================] - 1273s 201us/sample - loss: 0.0482 - acc: 0.9851 - val_loss: 0.0574 - val_acc: 0.9837\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(\n",
    "    [X_train_inx_chars, \n",
    "     X_train_inx_bigrams, \n",
    "     X_train_inx_trigrams], \n",
    "    y_train, \n",
    "    epochs=3,\n",
    "    validation_data=(\n",
    "        [X_val_inx_chars, \n",
    "         X_val_inx_bigrams,\n",
    "         X_val_inx_trigrams], \n",
    "        y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate this new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1586909/1586909 [==============================] - 65s 41us/sample - loss: 0.0574 - acc: 0.9837\n",
      "Scores: [0.05736815190206216, 0.9836935]\n",
      "loss: 5.74%\n",
      "acc: 98.37%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model2.evaluate([X_val_inx_chars, \n",
    "                         X_val_inx_bigrams,\n",
    "                         X_val_inx_trigrams], y_val)\n",
    "print('Scores:', scores)\n",
    "list(map(lambda x: print(\"%s: %.2f%%\" % (x[0], x[1] * 100)), zip(model.metrics_names, scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.26628581e-38 0.00000000e+00 1.13446156e-34 1.70438958e-31\n",
      "  4.01623771e-33 1.31119443e-17 6.80804981e-17 2.65705128e-27\n",
      "  1.16511981e-32 2.13565283e-34 1.03613338e-31 2.26511209e-29\n",
      "  5.25932629e-25 1.59582940e-28 1.46535750e-31 9.02106790e-30\n",
      "  4.02487991e-22 3.43379150e-32 5.07940595e-24 2.41717550e-37\n",
      "  4.01911506e-35 2.73342065e-17 0.00000000e+00 4.98672451e-27\n",
      "  0.00000000e+00 8.68558142e-21 5.72089356e-32 1.31907237e-22\n",
      "  0.00000000e+00 2.70744764e-38 8.59820916e-24 3.59971215e-15\n",
      "  7.73362204e-25 1.08744549e-32 9.56700920e-22 6.26743949e-26\n",
      "  1.33666579e-20 5.88734062e-16 9.99992967e-01 4.05791351e-24\n",
      "  1.57189526e-31 0.00000000e+00 6.47023838e-26 1.46521844e-30\n",
      "  1.74412693e-26 2.26482518e-23 9.89106904e-19 1.96691115e-38\n",
      "  3.53252461e-38 7.05105276e-06 6.34146361e-26 1.91933125e-25\n",
      "  2.11844344e-26 1.92480937e-24 9.80019834e-30 1.61290777e-24\n",
      "  0.00000000e+00 3.21921871e-26 3.74367185e-19 1.17527554e-24\n",
      "  3.32372522e-27 0.00000000e+00 2.99842560e-31 4.22535211e-28\n",
      "  7.17133839e-32 1.35776798e-26 2.17513110e-22 0.00000000e+00\n",
      "  0.00000000e+00 8.20560391e-22]\n",
      " [4.61430297e-20 4.14054222e-21 5.64655427e-36 1.70614144e-20\n",
      "  6.78551063e-18 1.28567130e-24 1.59669141e-15 6.75056888e-21\n",
      "  3.69347084e-33 3.59686665e-16 2.65947521e-18 8.82364959e-10\n",
      "  6.84124218e-31 1.23703004e-26 1.52467481e-12 4.78461021e-12\n",
      "  3.96742441e-29 1.24218685e-37 1.09272845e-20 2.66422807e-25\n",
      "  1.90125422e-14 3.32845607e-27 4.16197933e-23 2.32490228e-29\n",
      "  4.01407888e-24 1.52199676e-11 5.00612602e-19 8.55370754e-25\n",
      "  2.25632008e-24 1.52738797e-19 7.92513547e-27 2.28206077e-19\n",
      "  3.22917994e-18 1.00007555e-24 9.81510512e-18 3.29335478e-21\n",
      "  1.09790226e-33 9.11685349e-23 9.45597089e-16 2.64462715e-23\n",
      "  5.03813073e-22 3.29369480e-13 6.69102262e-18 4.20211642e-16\n",
      "  3.53380388e-15 8.99471288e-36 7.17698129e-22 2.97778623e-20\n",
      "  2.33248100e-13 7.45864077e-14 1.17651445e-14 1.00000000e+00\n",
      "  0.00000000e+00 6.21665729e-15 2.38607803e-24 4.32303505e-32\n",
      "  5.38757547e-26 5.63297628e-22 1.44470577e-28 1.07198460e-22\n",
      "  2.66750197e-18 6.68121348e-30 7.82301598e-18 2.11499663e-24\n",
      "  3.72657070e-12 3.20245783e-13 3.17297824e-30 0.00000000e+00\n",
      "  7.76378953e-27 1.27310156e-28]\n",
      " [8.27283353e-10 1.53280177e-12 6.17013797e-27 2.27421726e-20\n",
      "  5.40457969e-15 9.55102737e-20 9.99998450e-01 4.69124384e-09\n",
      "  2.91080543e-31 4.56928648e-17 5.76052073e-16 5.52431900e-19\n",
      "  1.19446153e-21 7.17638393e-10 5.75367770e-13 7.05013771e-15\n",
      "  4.75095525e-27 2.36968084e-26 5.37365927e-23 1.34537292e-15\n",
      "  5.78119630e-22 4.13695158e-21 8.01326792e-08 2.10037519e-25\n",
      "  6.21991182e-18 3.73666342e-09 7.43536295e-14 1.32940476e-15\n",
      "  1.16554444e-09 5.56546447e-25 1.16787209e-24 5.25457194e-14\n",
      "  8.93666909e-19 1.18598531e-20 7.50928313e-12 9.90715137e-08\n",
      "  4.25903518e-28 1.59156789e-16 2.16841599e-07 1.57674034e-21\n",
      "  5.78694349e-11 8.84464621e-14 2.09308876e-10 3.61159366e-12\n",
      "  2.08551492e-12 4.83674040e-26 1.49457587e-15 3.81503096e-09\n",
      "  1.68345472e-18 1.63112787e-07 2.62574340e-09 2.04424255e-11\n",
      "  2.33733265e-27 5.63644076e-10 9.54518802e-15 2.24888824e-20\n",
      "  1.18459665e-21 2.56990379e-16 3.06813878e-19 9.96683672e-19\n",
      "  1.02032502e-07 2.56274932e-22 7.83206531e-07 2.93116000e-18\n",
      "  8.24420291e-16 1.08296683e-12 2.49042194e-28 1.19556768e-31\n",
      "  4.19533584e-23 3.18058712e-28]\n",
      " [2.70685216e-08 5.66915180e-21 4.29416695e-20 5.51740866e-23\n",
      "  1.21466129e-12 1.79713432e-22 2.50040233e-09 7.53033486e-12\n",
      "  6.92804322e-30 2.29563240e-16 1.15473888e-13 3.80744494e-18\n",
      "  4.94562726e-21 1.25137062e-11 8.80650674e-12 3.22106064e-14\n",
      "  4.63435984e-20 7.43446120e-23 4.11664092e-18 1.48788844e-15\n",
      "  3.26940139e-18 2.82433841e-20 5.23960509e-04 4.62700672e-22\n",
      "  6.39542380e-16 6.03544437e-11 5.10449279e-19 9.58481837e-17\n",
      "  2.09889970e-11 1.28826405e-23 1.24682508e-22 1.11558966e-14\n",
      "  8.86492451e-22 2.33894501e-13 4.77244744e-10 7.80878473e-09\n",
      "  2.59509166e-28 8.78311435e-14 6.81217303e-14 1.95775698e-21\n",
      "  2.50436075e-04 1.28892664e-17 7.62348066e-07 5.66025803e-14\n",
      "  4.51736698e-14 4.27948297e-27 2.56107188e-15 8.84166340e-09\n",
      "  4.33523903e-17 3.93184644e-13 9.99206841e-01 4.13100847e-12\n",
      "  2.48230938e-26 3.53616940e-08 3.58747862e-13 4.54195052e-20\n",
      "  1.24642109e-14 2.15071328e-13 1.31028227e-19 7.31218304e-19\n",
      "  1.13324277e-05 9.73226695e-17 6.72847182e-06 1.93098474e-14\n",
      "  9.36504085e-16 1.23038136e-13 5.74166415e-23 1.12537460e-27\n",
      "  8.53318894e-22 8.76756978e-28]\n",
      " [0.00000000e+00 0.00000000e+00 1.73810537e-37 2.79031870e-33\n",
      "  7.42798701e-26 0.00000000e+00 9.11692986e-29 1.41573942e-31\n",
      "  0.00000000e+00 2.29910757e-27 4.56898297e-23 1.64118296e-25\n",
      "  0.00000000e+00 2.74754729e-36 2.26412595e-29 1.38165432e-20\n",
      "  6.34619152e-37 0.00000000e+00 5.82682356e-34 5.03369053e-38\n",
      "  3.80932663e-29 0.00000000e+00 0.00000000e+00 5.06159947e-35\n",
      "  8.94802098e-31 3.22497812e-28 8.39282590e-36 1.20042446e-36\n",
      "  1.74754960e-33 0.00000000e+00 4.60531917e-34 4.82552440e-26\n",
      "  0.00000000e+00 1.13473915e-32 1.59925740e-23 0.00000000e+00\n",
      "  0.00000000e+00 3.66529175e-38 8.37956453e-25 0.00000000e+00\n",
      "  5.53868535e-31 3.94763052e-30 1.60075191e-24 1.90347102e-33\n",
      "  4.80114059e-26 0.00000000e+00 3.57787589e-29 2.26184973e-34\n",
      "  2.26173406e-37 3.68297952e-23 4.36453782e-27 1.53954047e-23\n",
      "  0.00000000e+00 1.00000000e+00 4.57017446e-28 0.00000000e+00\n",
      "  1.85670825e-37 2.83899089e-24 0.00000000e+00 1.30497670e-31\n",
      "  8.52616698e-28 9.35680202e-29 1.26728870e-33 5.46195801e-30\n",
      "  1.66056675e-25 2.09727990e-25 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model2.predict([X_val_inx_chars, \n",
    "                             X_val_inx_bigrams,\n",
    "                             X_val_inx_trigrams])\n",
    "print(y_predicted[:5])\n",
    "print(y_val[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kab', 'deu', 'fra', 'spa', 'epo', 'deu', 'por', 'hun', 'eng', 'heb']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_predicted, axis=-1)\n",
    "list(map(inx2lang.get, y_pred))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kab', 'deu', 'fra', 'spa', 'epo', 'deu', 'por', 'hun', 'eng', 'heb']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_symb = np.argmax(y_val, axis=-1)\n",
    "list(map(inx2lang.get, y_val_symb))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this model is simpler than the first one, it seems slightly more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.95      0.78      0.85      1212\n",
      "         afr       0.83      0.78      0.81       754\n",
      "         run       0.93      0.93      0.93       730\n",
      "         gos       0.87      0.77      0.81       662\n",
      "         pol       1.00      0.99      0.99     20176\n",
      "         uig       1.00      0.99      1.00      1530\n",
      "         fra       1.00      0.99      1.00     80124\n",
      "         lat       0.96      0.94      0.95      6570\n",
      "         mhr       0.98      0.95      0.97       844\n",
      "         nob       0.84      0.76      0.80      2784\n",
      "         dan       0.93      0.91      0.92      8765\n",
      "         nds       0.98      0.90      0.94      3471\n",
      "         vie       0.99      0.99      0.99      2093\n",
      "         ron       0.98      0.95      0.97      3867\n",
      "         fin       0.99      0.98      0.99     21335\n",
      "         hun       0.99      1.00      1.00     53914\n",
      "         ukr       0.99      0.98      0.98     30964\n",
      "         bul       0.95      0.89      0.92      4788\n",
      "         ind       0.96      0.95      0.96      2524\n",
      "         avk       0.83      0.94      0.88       805\n",
      "         vol       0.97      0.91      0.94       618\n",
      "         pes       0.99      0.99      0.99      4317\n",
      "         glg       0.73      0.66      0.69       836\n",
      "         bel       0.99      0.93      0.96      2504\n",
      "         slk       0.75      0.76      0.75       829\n",
      "         eng       1.00      1.00      1.00    253229\n",
      "         bre       0.96      0.89      0.93      1381\n",
      "         wuu       0.80      0.81      0.80       923\n",
      "         ido       0.95      0.74      0.83      1041\n",
      "         isl       0.99      0.97      0.98      2239\n",
      "         ben       1.00      0.99      0.99       996\n",
      "         cmn       0.98      0.96      0.97     12185\n",
      "         tat       0.99      0.97      0.98      2719\n",
      "        toki       1.00      1.00      1.00      6961\n",
      "         jbo       0.99      0.99      0.99      3060\n",
      "         oci       0.94      0.87      0.90      1095\n",
      "         mar       1.00      1.00      1.00     10881\n",
      "         ara       1.00      0.99      1.00      6663\n",
      "         kab       0.82      0.83      0.82     25785\n",
      "         kor       1.00      0.99      0.99      1311\n",
      "         lfn       0.92      0.83      0.87      1615\n",
      "         est       0.79      0.88      0.83       581\n",
      "         ita       0.99      1.00      0.99    148080\n",
      "         eus       0.96      0.92      0.94      1218\n",
      "         nld       0.96      0.99      0.97     20918\n",
      "         mkd       0.95      0.98      0.97     15445\n",
      "         rus       0.99      1.00      0.99    147015\n",
      "         ile       0.88      0.72      0.79      1284\n",
      "         tuk       0.98      0.91      0.94      1368\n",
      "         ber       0.90      0.89      0.90     45447\n",
      "         spa       0.98      0.98      0.98     62694\n",
      "         deu       1.00      0.99      1.00     97675\n",
      "         heb       1.00      1.00      1.00     38816\n",
      "         epo       1.00      1.00      1.00    122122\n",
      "         ces       0.98      0.95      0.97      7430\n",
      "         jpn       1.00      1.00      1.00     37553\n",
      "         tgl       0.99      0.96      0.97      2703\n",
      "         srp       0.86      0.90      0.88      6077\n",
      "         ell       1.00      1.00      1.00      6042\n",
      "         yue       0.82      0.94      0.88      1244\n",
      "         por       0.99      0.99      0.99     68908\n",
      "         hrv       0.81      0.24      0.37      1029\n",
      "         ina       0.87      0.94      0.90      4937\n",
      "         lit       0.97      0.99      0.98      7658\n",
      "         swe       0.95      0.97      0.96      6989\n",
      "         tur       1.00      1.00      1.00    136916\n",
      "         tlh       0.99      0.98      0.99      3448\n",
      "         aze       0.99      0.80      0.88      1012\n",
      "         cor       0.97      0.93      0.95       774\n",
      "         hin       0.99      0.98      0.99      2426\n",
      "\n",
      "    accuracy                           0.98   1586909\n",
      "   macro avg       0.95      0.92      0.93   1586909\n",
      "weighted avg       0.98      0.98      0.98   1586909\n",
      "\n",
      "Micro F1: 0.9836934569026957\n",
      "Macro F1 0.9291125312154187\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "print(classification_report(y_val_symb, y_pred, target_names=lang_names))\n",
    "print('Micro F1:', f1_score(y_val_symb, y_pred, average='micro'))\n",
    "print('Macro F1', f1_score(y_val_symb, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: fra\n",
      "Confusions: [   25     1     1     0     1     0 79430    27     0     1     2     2\n",
      "     0     4     2    14     0     0     0     4     0     0     1     0\n",
      "     0   130    14     0     4     0     0     0     0     1     2    46\n",
      "     0     0     4     0     5     1    99     1     9     0     0    17\n",
      "     0    18    91    29     0    23     1     0     1     3     0     0\n",
      "    63     0    35     1     0     9     2     0     0     0]\n",
      "Most confused: eng 0.0016224851480205681\n",
      "====\n",
      "Language: eng\n",
      "Confusions: [     3     13      7      2     15      0     56     34      0      0\n",
      "     14     15      0      4     12     20      0      0      3      4\n",
      "      0      0      0      0      0 252564      6      0      9      0\n",
      "      0      1      0      6      0      1      0      0     10      0\n",
      "      2      6     81      0     52      0      0     11      1     22\n",
      "     42     93      0     23      5      0      1      3      0      0\n",
      "     33      0     13      2      6     22      7      0      5      0]\n",
      "Most confused: deu 0.00036725651485414385\n",
      "====\n",
      "Language: swe\n",
      "Confusions: [   0    0    1    0    3    0    7    3    0   35  163    8    0    0\n",
      "    5    5    0    0    0    2    0    0    0    0    0   29    3    0\n",
      "    3    1    0    0    1    0    0    0    0    0    3    0    0    1\n",
      "    8    0    6    0    0    1    3    2    2   30    0    4    1    0\n",
      "    0    3    0    0    4    0    0    2 6644    5    1    0    0    0]\n",
      "Most confused: dan 0.023322363714408358\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "for language in languages:\n",
    "    if language not in lang2inx:\n",
    "        continue\n",
    "    print('Language:', language)\n",
    "    print('Confusions:', cf[lang2inx[language]])\n",
    "    print('Most confused:',\n",
    "          inx2lang[np.argsort(cf[lang2inx[language]])[-2]], \n",
    "          np.sort(cf[lang2inx[language]])[-2] / np.sum(cf[lang2inx[language]]))\n",
    "    print('====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_inx(sentence):\n",
    "    char_items = hash_chars(sentence)\n",
    "    bigram_items = hash_bigrams(sentence)\n",
    "    trigram_items = hash_trigrams(sentence)\n",
    "    return [pad_sequences([char_items[0]], padding='post', maxlen=MAXLEN_CHARS),\n",
    "            pad_sequences([bigram_items[0]], padding='post', maxlen=MAXLEN_BIGRAMS),\n",
    "            pad_sequences([trigram_items[0]], padding='post', maxlen=MAXLEN_TRIGRAMS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[208, 324, 541,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0]], dtype=int32),\n",
       " array([[4366, 3769, 8123,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0]], dtype=int32),\n",
       " array([[7381, 5853, 1734,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]], dtype=int32)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Banana\"\n",
    "extract_features_inx(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fra'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Salut les gars !\"\n",
    "preds = model2.predict(extract_features_inx(sentence))\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eng'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello guys!\"\n",
    "preds = model2.predict(extract_features_inx(sentence))\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'swe'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hejsan grabbar!\"\n",
    "preds = model2.predict(extract_features_inx(sentence))\n",
    "inx2lang[np.argmax(preds)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
