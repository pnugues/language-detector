{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Implementation of CLD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Pierre Nugues\n",
    "\n",
    "Reimplementation of Google's _Compact language detector_ (CLD3) from a high-level description. Source: ``https://github.com/google/cld3``\n",
    "\n",
    "Still missing:\n",
    "* generator or train_on_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Dataset: *Tatoeba*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As dataset, we use Tatoeba: A database of texts with language tags. The corpus is available here: https://tatoeba.org/eng/downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the dataset and we split the lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\tcmn\\t我們試試看！',\n",
       " '2\\tcmn\\t我该去睡觉了。',\n",
       " '3\\tcmn\\t你在干什麼啊？',\n",
       " '4\\tcmn\\t這是什麼啊？',\n",
       " '5\\tcmn\\t今天是６月１８号，也是Muiriel的生日！',\n",
       " '6\\tcmn\\t生日快乐，Muiriel！',\n",
       " '7\\tcmn\\tMuiriel现在20岁了。',\n",
       " '8\\tcmn\\t密码是\"Muiriel\"。',\n",
       " '9\\tcmn\\t我很快就會回來。',\n",
       " '10\\tcmn\\t我不知道。']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_raw = open('sentences.csv', encoding='utf8').read().strip()\n",
    "dataset_raw = dataset_raw.split('\\n')\n",
    "dataset_raw[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the fields and we remove possible whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8023136 texts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', 'cmn', '我們試試看！'), ('2', 'cmn', '我该去睡觉了。'), ('3', 'cmn', '你在干什麼啊？')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_raw = list(map(lambda x: tuple(x.split('\\t')), dataset_raw))\n",
    "dataset_raw = list(map(lambda x: tuple(map(str.strip, x)), dataset_raw))\n",
    "print(len(dataset_raw), 'texts')\n",
    "dataset_raw[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pad strings that are less than three characters. If not done, training will crash. We also limit the length of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN_TEXT = 200\n",
    "for i in range(len(dataset_raw)):\n",
    "    if len(dataset_raw[i][2]) == 0:\n",
    "        dataset_raw[i] = (dataset_raw[i][0], dataset_raw[i][1], dataset_raw[i][2] + '   ')\n",
    "    if len(dataset_raw[i][2]) == 1: \n",
    "        dataset_raw[i] = (dataset_raw[i][0], dataset_raw[i][1], dataset_raw[i][2] + '  ')\n",
    "    if len(dataset_raw[i][2]) == 2:\n",
    "        dataset_raw[i] = (dataset_raw[i][0], dataset_raw[i][1], dataset_raw[i][2] + ' ')\n",
    "    dataset_raw[i] = (dataset_raw[i][0], dataset_raw[i][1], dataset_raw[i][2][:MAXLEN_TEXT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle, seed\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "shuffle(dataset_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decimate the dataset to have faster training times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2608667', 'rus', 'Это самое важное, главное, что ты улыбаешься.'),\n",
       " ('5169605', 'ita', 'Smetto di lavorare attorno a mezzanotte.'),\n",
       " ('3376481', 'hun', 'Futottunk a macska után.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DECIMATE = False\n",
    "if DECIMATE:\n",
    "    dataset_raw = dataset_raw[:int(len(dataset_raw)/10)]\n",
    "dataset_raw[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The languages. Some texts have no language tag, and some others are marked with the cryptic \\\\\\\\N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many languages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = set([x[1] for x in dataset_raw])\n",
    "len(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the texts per language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_texts(dataset):\n",
    "    text_counts = {}\n",
    "    for record in dataset:\n",
    "        lang = record[1]\n",
    "        if lang in text_counts:\n",
    "            text_counts[lang] += 1\n",
    "        else:\n",
    "            text_counts[lang] = 1\n",
    "    return text_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Languages with the most examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eng', 1264754),\n",
       " ('ita', 738799),\n",
       " ('rus', 732078),\n",
       " ('tur', 684619),\n",
       " ('epo', 609518),\n",
       " ('deu', 488568),\n",
       " ('fra', 402078),\n",
       " ('por', 347430),\n",
       " ('spa', 314873),\n",
       " ('hun', 269208),\n",
       " ('ber', 226376),\n",
       " ('heb', 195329),\n",
       " ('jpn', 187684),\n",
       " ('ukr', 154466),\n",
       " ('kab', 129245),\n",
       " ('fin', 106936),\n",
       " ('nld', 104337),\n",
       " ('pol', 99754),\n",
       " ('mkd', 77778),\n",
       " ('cmn', 60993),\n",
       " ('mar', 54656),\n",
       " ('dan', 43995),\n",
       " ('lit', 38408),\n",
       " ('ces', 37596),\n",
       " ('toki', 35017)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_counts = count_texts(dataset_raw)\n",
    "langs = sorted(text_counts.keys(), key=text_counts.get, reverse=True)\n",
    "[(lang, text_counts[lang]) for lang in langs][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider languages that have more than 3,000 examples in the dataset or we only use those in French, English, and Swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eng', 'ita', 'rus', 'tur', 'epo', 'deu', 'fra', 'por', 'spa', 'hun', 'ber', 'heb', 'jpn', 'ukr', 'kab', 'fin', 'nld', 'pol', 'mkd', 'cmn', 'mar', 'dan', 'lit', 'ces', 'toki', 'swe', 'ara', 'lat', 'ell', 'srp', 'ina', 'bul', 'pes', 'ron', 'nds', 'tlh', 'jbo', 'nob', 'tat', 'tgl', 'ind', 'bel', 'hin', 'isl', 'vie', 'lfn', 'uig', 'bre', 'tuk', 'kor', 'ile', 'eus', 'cat', 'yue', 'oci', 'hrv', 'ido', 'aze', 'ben', 'glg', 'wuu', 'mhr', 'slk', 'afr', 'avk', 'cor', 'run', 'gos', 'vol', 'est']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMALL_LANGUAGE_SET = False\n",
    "considered_langs = [lang for lang in langs if text_counts[lang] > 3000]\n",
    "if SMALL_LANGUAGE_SET:\n",
    "    considered_langs = ['fra', 'eng', 'swe']\n",
    "print(considered_langs)\n",
    "LANG_NBR = len(considered_langs)\n",
    "LANG_NBR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the texts in these languages. This will form our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7934544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2608667', 'rus', 'Это самое важное, главное, что ты улыбаешься.'),\n",
       " ('5169605', 'ita', 'Smetto di lavorare attorno a mezzanotte.'),\n",
       " ('3376481', 'hun', 'Futottunk a macska után.'),\n",
       " ('240215',\n",
       "  'eng',\n",
       "  'Please accept our condolences on the death of your father.'),\n",
       " ('6482091', 'rus', 'Ты единственный, кто умеет это делать.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(filter(lambda x: x[1] in considered_langs, dataset_raw))\n",
    "print(len(dataset))\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Count Characters Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use hash codes to convert ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of codes we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARS = 4096\n",
    "MAX_BIGRAMS = 8192\n",
    "MAX_TRIGRAMS = 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the counts as in CLD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(d):\n",
    "    sum_chars = sum(d.values())\n",
    "    d = {k:v/sum_chars for k, v in d.items()}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the hash code and we add one to avoid a value of 0 as it is padding symbol in the subsequent matrices.\n",
    "\n",
    "By default, we set the characters in lowercase and we sort the ngrams by frequency order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def hash_chars(string, lc=True, freq_sort=True):\n",
    "    if lc:\n",
    "        string = string.lower()\n",
    "    hash_codes = map(lambda x: hash(x) % MAX_CHARS + 1, string)\n",
    "    d = dict(Counter(hash_codes))\n",
    "    d = normalize(d)\n",
    "    if freq_sort:\n",
    "        k, v = zip(*sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "    else:\n",
    "        k, v = zip(*d.items())\n",
    "    return k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_bigrams(string, lc=True, freq_sort=True):\n",
    "    if lc:\n",
    "        string = string.lower()\n",
    "    bigrams = [string[i:i + 2] for i in range(len(string) - 1)]\n",
    "    hash_codes = map(lambda x: hash(x) % MAX_BIGRAMS + 1, bigrams)\n",
    "    d = dict(Counter(hash_codes))\n",
    "    d = normalize(d)\n",
    "    if freq_sort:\n",
    "        k, v = zip(*sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "    else:\n",
    "        k, v = zip(*d.items())\n",
    "    return k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_trigrams(string, lc=True, freq_sort=True):\n",
    "    if lc:\n",
    "        string = string.lower()\n",
    "    trigrams = [string[i:i + 3] for i in range(len(string) - 2)]\n",
    "    hash_codes = map(lambda x: hash(x) % MAX_TRIGRAMS + 1, trigrams)\n",
    "    d = dict(Counter(hash_codes))\n",
    "    d = normalize(d)\n",
    "    if freq_sort:\n",
    "        k, v = zip(*sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "    else:\n",
    "        k, v = zip(*d.items())\n",
    "    return k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google's example in CLD3's presentation's text (``https://github.com/google/cld3``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chars: ((3232, 2755, 1698), (0.5, 0.3333333333333333, 0.16666666666666666))\n",
      "Bigrams: ((5340, 70, 4333), (0.4, 0.4, 0.2))\n",
      "Trigrams: ((1923, 6678, 2369), (0.5, 0.25, 0.25))\n"
     ]
    }
   ],
   "source": [
    "print('Chars:', hash_chars('Banana'))\n",
    "print('Bigrams:', hash_bigrams('Banana'))\n",
    "print('Trigrams:', hash_trigrams('Banana'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chars: ((2492, 402, 3662, 339, 2714, 3862, 2468, 193, 282, 119, 3165, 357, 2755, 2880, 1394), (0.15, 0.1, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05))\n",
      "Bigrams: ((7661, 3750, 2266, 1868, 3861, 3282, 7259, 2068, 7909, 3666, 6166, 5116, 2213, 464, 2423, 6111, 2403, 6853), (0.10526315789473684, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842))\n",
      "Trigrams: ((7253, 7772, 6181, 4242, 1676, 7392, 1864, 3388, 2944, 3582, 4943, 1585, 3573, 8115, 7360, 917, 6702, 6306), (0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555, 0.05555555555555555))\n"
     ]
    }
   ],
   "source": [
    "print('Chars:', hash_chars(\"Let's try something.\"))\n",
    "print('Bigrams:', hash_bigrams(\"Let's try something.\"))\n",
    "print('Trigrams:', hash_trigrams(\"Let's try something.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the $\\mathbf{X}$ lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the character, bigram, and trigram counts of the texts and we create $\\mathbf{X}$ lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7934544/7934544 [14:30<00:00, 9114.00it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "X_list_inx_chars = []\n",
    "X_list_freq_chars = []\n",
    "X_list_inx_bigrams = []\n",
    "X_list_freq_bigrams = []\n",
    "X_list_inx_trigrams = []\n",
    "X_list_freq_trigrams = []\n",
    "\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    k, v = hash_chars(dataset[i][-1])\n",
    "    X_list_inx_chars.append(k)\n",
    "    X_list_freq_chars.append(v)\n",
    "    \n",
    "    k, v = hash_bigrams(dataset[i][-1])\n",
    "    X_list_inx_bigrams.append(k)\n",
    "    X_list_freq_bigrams.append(v)\n",
    "    \n",
    "    k, v = hash_trigrams(dataset[i][-1])\n",
    "    X_list_inx_trigrams.append(k)\n",
    "    X_list_freq_trigrams.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(339, 1088, 2493, 1035, 4025, 66, 580, 1594, 2751, 3053, 83, 1118, 1077, 541, 2529, 2435, 2504, 406, 1775, 2702, 883, 1394), (2492, 282, 339, 3232, 402, 2468, 119, 2755, 3737, 3662, 2701, 357, 2714, 1097, 1394)]\n",
      "[(0.13333333333333333, 0.1111111111111111, 0.08888888888888889, 0.08888888888888889, 0.06666666666666667, 0.044444444444444446, 0.044444444444444446, 0.044444444444444446, 0.044444444444444446, 0.044444444444444446, 0.044444444444444446, 0.022222222222222223, 0.022222222222222223, 0.022222222222222223, 0.022222222222222223, 0.022222222222222223, 0.022222222222222223, 0.022222222222222223, 0.022222222222222223, 0.022222222222222223, 0.022222222222222223, 0.022222222222222223), (0.15, 0.125, 0.125, 0.125, 0.1, 0.075, 0.05, 0.05, 0.05, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025)]\n",
      "[(5189, 7498, 2475, 5312, 98, 4930, 5029, 572, 6330, 4715, 6247, 7350, 5401, 766, 5515, 6410, 30, 2586, 428, 2699, 7046, 2016, 3000, 4955, 409, 3236, 4403, 7311, 3173, 721, 1487, 296, 5155, 6075, 6797, 1389), (2687, 2213, 7622, 1212, 582, 5667, 5795, 5070, 7661, 7284, 696, 7680, 3088, 2276, 2395, 4899, 6065, 5739, 3878, 3337, 2631, 7118, 5853, 447, 5200, 4411, 3669, 5340, 5358, 6273, 999)]\n",
      "[(0.09090909090909091, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728, 0.022727272727272728), (0.07692307692307693, 0.05128205128205128, 0.05128205128205128, 0.05128205128205128, 0.05128205128205128, 0.05128205128205128, 0.05128205128205128, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564)]\n",
      "[(6946, 7267, 1852, 4930, 1106, 1185, 8091, 1708, 1501, 1228, 934, 2108, 3213, 4645, 5504, 400, 1490, 2586, 2297, 4107, 3854, 6791, 2735, 232, 8121, 5311, 7596, 1803, 4142, 2270, 4419, 1329, 471, 1144, 6625, 7711, 4491, 1306, 5879), (5984, 3642, 3573, 6923, 5077, 1622, 3896, 4771, 7773, 2128, 7057, 1533, 5, 7126, 6789, 6435, 1576, 4961, 6089, 1141, 4840, 6932, 5571, 7123, 7501, 3866, 7884, 6305, 6940, 1739, 713, 2811, 1564, 3144, 2455, 752, 1711)]\n",
      "[(0.046511627906976744, 0.046511627906976744, 0.046511627906976744, 0.046511627906976744, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372, 0.023255813953488372), (0.05263157894736842, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421)]\n"
     ]
    }
   ],
   "source": [
    "print(X_list_inx_chars[:2])\n",
    "print(X_list_freq_chars[:2])\n",
    "print(X_list_inx_bigrams[:2])\n",
    "print(X_list_freq_bigrams[:2])\n",
    "print(X_list_inx_trigrams[:2])\n",
    "print(X_list_freq_trigrams[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract all the unique ngrams (hash codes). This part is not necessary to train the model and it could be skipped. We use it to determine if we have enough hash codes and the feature vector lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = set()\n",
    "for x_list_inx_chars in X_list_inx_chars:\n",
    "    unique_chars.update(set(x_list_inx_chars))\n",
    "\n",
    "unique_bigrams = set()\n",
    "for x_list_inx_bigrams in X_list_inx_bigrams:\n",
    "    unique_bigrams.update(set(x_list_inx_bigrams))\n",
    "\n",
    "unique_trigrams = set()\n",
    "for x_list_inx_trigrams in X_list_inx_trigrams:\n",
    "    unique_trigrams.update(set(x_list_inx_trigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the hash coding capacity. Have we used all the hash codes? Will there be collisions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3520"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_char_cnt = len(unique_chars)\n",
    "unique_char_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_bigram_cnt = len(unique_bigrams)\n",
    "unique_bigram_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_trigram_cnt = len(unique_trigrams)\n",
    "unique_trigram_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of the index vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How will we align the $\\mathbf{X}$ matrices? \n",
    "\n",
    "We will compute the maximal length of the exhaustive feature lists and we use: max(median, mean) plus two standard deviations (roughly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the max length of the feature vectors for the characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vect_lengths = [len(x_list_inx_chars) for x_list_inx_chars in X_list_inx_chars]\n",
    "max(char_vect_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.24075145339165\n",
      "3.7880676669943725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "print(statistics.mean(char_vect_lengths))\n",
    "print(statistics.stdev(char_vect_lengths))\n",
    "statistics.median(char_vect_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the max length of the feature vectors for the bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vect_lengths = [len(x_list_inx_bigrams) for x_list_inx_bigrams in X_list_inx_bigrams]\n",
    "max(bigram_vect_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.839839945433535\n",
      "13.815408024391429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(statistics.mean(bigram_vect_lengths))\n",
    "print(statistics.stdev(bigram_vect_lengths))\n",
    "statistics.median(bigram_vect_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the max length of the feature vectors for the trigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_vect_lengths = [len(x_list_inx_trigrams) for x_list_inx_trigrams in X_list_inx_trigrams]\n",
    "max(trigram_vect_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.211026871865606\n",
      "18.252929704574356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(statistics.mean(trigram_vect_lengths))\n",
    "print(statistics.stdev(trigram_vect_lengths))\n",
    "statistics.median(trigram_vect_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the maximal lengths: max(median, mean) + 2 x stdev and a small margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN_CHARS = 30\n",
    "MAXLEN_BIGRAMS = 60\n",
    "MAXLEN_TRIGRAMS = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building $\\mathbf{X}$ and $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the matrices by converting the $\\mathbf{X}$ lists into arrays and padding them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $\\mathbf{X}$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pad the character sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 339, 1088, 2493, 1035, 4025,   66,  580, 1594, 2751, 3053,   83,\n",
       "        1118, 1077,  541, 2529, 2435, 2504,  406, 1775, 2702,  883, 1394,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [2492,  282,  339, 3232,  402, 2468,  119, 2755, 3737, 3662, 2701,\n",
       "         357, 2714, 1097, 1394,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_inx_chars = pad_sequences(X_list_inx_chars, padding='post', maxlen=MAXLEN_CHARS)\n",
    "X_inx_chars[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The character frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13333334, 0.11111111, 0.08888889, 0.08888889, 0.06666667,\n",
       "        0.04444445, 0.04444445, 0.04444445, 0.04444445, 0.04444445,\n",
       "        0.04444445, 0.02222222, 0.02222222, 0.02222222, 0.02222222,\n",
       "        0.02222222, 0.02222222, 0.02222222, 0.02222222, 0.02222222,\n",
       "        0.02222222, 0.02222222, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.15      , 0.125     , 0.125     , 0.125     , 0.1       ,\n",
       "        0.075     , 0.05      , 0.05      , 0.05      , 0.025     ,\n",
       "        0.025     , 0.025     , 0.025     , 0.025     , 0.025     ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_freq_chars = pad_sequences(X_list_freq_chars, padding='post', dtype='float32', maxlen=MAXLEN_CHARS)\n",
    "X_freq_chars[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for the bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5189, 7498, 2475, 5312,   98, 4930, 5029,  572, 6330, 4715, 6247,\n",
       "        7350, 5401,  766, 5515, 6410,   30, 2586,  428, 2699, 7046, 2016,\n",
       "        3000, 4955,  409, 3236, 4403, 7311, 3173,  721, 1487,  296, 5155,\n",
       "        6075, 6797, 1389,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0],\n",
       "       [2687, 2213, 7622, 1212,  582, 5667, 5795, 5070, 7661, 7284,  696,\n",
       "        7680, 3088, 2276, 2395, 4899, 6065, 5739, 3878, 3337, 2631, 7118,\n",
       "        5853,  447, 5200, 4411, 3669, 5340, 5358, 6273,  999,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_inx_bigrams = pad_sequences(X_list_inx_bigrams, padding='post', maxlen=MAXLEN_BIGRAMS)\n",
    "X_inx_bigrams[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09090909, 0.04545455, 0.04545455, 0.04545455, 0.04545455,\n",
       "        0.04545455, 0.02272727, 0.02272727, 0.02272727, 0.02272727,\n",
       "        0.02272727, 0.02272727, 0.02272727, 0.02272727, 0.02272727,\n",
       "        0.02272727, 0.02272727, 0.02272727, 0.02272727, 0.02272727,\n",
       "        0.02272727, 0.02272727, 0.02272727, 0.02272727, 0.02272727,\n",
       "        0.02272727, 0.02272727, 0.02272727, 0.02272727, 0.02272727,\n",
       "        0.02272727, 0.02272727, 0.02272727, 0.02272727, 0.02272727,\n",
       "        0.02272727, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.07692308, 0.05128205, 0.05128205, 0.05128205, 0.05128205,\n",
       "        0.05128205, 0.05128205, 0.02564103, 0.02564103, 0.02564103,\n",
       "        0.02564103, 0.02564103, 0.02564103, 0.02564103, 0.02564103,\n",
       "        0.02564103, 0.02564103, 0.02564103, 0.02564103, 0.02564103,\n",
       "        0.02564103, 0.02564103, 0.02564103, 0.02564103, 0.02564103,\n",
       "        0.02564103, 0.02564103, 0.02564103, 0.02564103, 0.02564103,\n",
       "        0.02564103, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_freq_bigrams = pad_sequences(X_list_freq_bigrams, padding='post', dtype='float32', maxlen=MAXLEN_BIGRAMS)\n",
    "X_freq_bigrams[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6946, 7267, 1852, 4930, 1106, 1185, 8091, 1708, 1501, 1228,  934,\n",
       "        2108, 3213, 4645, 5504,  400, 1490, 2586, 2297, 4107, 3854, 6791,\n",
       "        2735,  232, 8121, 5311, 7596, 1803, 4142, 2270, 4419, 1329,  471,\n",
       "        1144, 6625, 7711, 4491, 1306, 5879,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [5984, 3642, 3573, 6923, 5077, 1622, 3896, 4771, 7773, 2128, 7057,\n",
       "        1533,    5, 7126, 6789, 6435, 1576, 4961, 6089, 1141, 4840, 6932,\n",
       "        5571, 7123, 7501, 3866, 7884, 6305, 6940, 1739,  713, 2811, 1564,\n",
       "        3144, 2455,  752, 1711,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_inx_trigrams = pad_sequences(X_list_inx_trigrams, padding='post', maxlen=MAXLEN_TRIGRAMS)\n",
    "X_inx_trigrams[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04651163, 0.04651163, 0.04651163, 0.04651163, 0.02325581,\n",
       "        0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "        0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "        0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "        0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "        0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "        0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "        0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.05263158, 0.02631579, 0.02631579, 0.02631579, 0.02631579,\n",
       "        0.02631579, 0.02631579, 0.02631579, 0.02631579, 0.02631579,\n",
       "        0.02631579, 0.02631579, 0.02631579, 0.02631579, 0.02631579,\n",
       "        0.02631579, 0.02631579, 0.02631579, 0.02631579, 0.02631579,\n",
       "        0.02631579, 0.02631579, 0.02631579, 0.02631579, 0.02631579,\n",
       "        0.02631579, 0.02631579, 0.02631579, 0.02631579, 0.02631579,\n",
       "        0.02631579, 0.02631579, 0.02631579, 0.02631579, 0.02631579,\n",
       "        0.02631579, 0.02631579, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_freq_trigrams = pad_sequences(X_list_freq_trigrams, padding='post', dtype='float32', maxlen=MAXLEN_TRIGRAMS)\n",
    "X_freq_trigrams[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $\\mathbf{y}$ vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rus', 'ita', 'hun', 'eng', 'rus', 'eng', 'mkd', 'dan', 'rus', 'por']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list = [dataset[i][1] for i in range(len(dataset))]\n",
    "y_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an index of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toki': 0,\n",
       " 'ile': 1,\n",
       " 'bel': 2,\n",
       " 'fra': 3,\n",
       " 'run': 4,\n",
       " 'ces': 5,\n",
       " 'tlh': 6,\n",
       " 'dan': 7,\n",
       " 'srp': 8,\n",
       " 'tur': 9,\n",
       " 'bul': 10,\n",
       " 'kab': 11,\n",
       " 'pes': 12,\n",
       " 'hin': 13,\n",
       " 'ido': 14,\n",
       " 'yue': 15,\n",
       " 'lfn': 16,\n",
       " 'lat': 17,\n",
       " 'ina': 18,\n",
       " 'vol': 19,\n",
       " 'isl': 20,\n",
       " 'lit': 21,\n",
       " 'pol': 22,\n",
       " 'nds': 23,\n",
       " 'uig': 24,\n",
       " 'glg': 25,\n",
       " 'por': 26,\n",
       " 'deu': 27,\n",
       " 'cor': 28,\n",
       " 'tgl': 29,\n",
       " 'rus': 30,\n",
       " 'mar': 31,\n",
       " 'ita': 32,\n",
       " 'tuk': 33,\n",
       " 'ell': 34,\n",
       " 'hun': 35,\n",
       " 'spa': 36,\n",
       " 'mkd': 37,\n",
       " 'swe': 38,\n",
       " 'ben': 39,\n",
       " 'mhr': 40,\n",
       " 'ara': 41,\n",
       " 'nob': 42,\n",
       " 'gos': 43,\n",
       " 'nld': 44,\n",
       " 'hrv': 45,\n",
       " 'epo': 46,\n",
       " 'cmn': 47,\n",
       " 'ber': 48,\n",
       " 'ron': 49,\n",
       " 'oci': 50,\n",
       " 'ind': 51,\n",
       " 'aze': 52,\n",
       " 'avk': 53,\n",
       " 'afr': 54,\n",
       " 'tat': 55,\n",
       " 'slk': 56,\n",
       " 'eng': 57,\n",
       " 'est': 58,\n",
       " 'wuu': 59,\n",
       " 'cat': 60,\n",
       " 'ukr': 61,\n",
       " 'vie': 62,\n",
       " 'jbo': 63,\n",
       " 'fin': 64,\n",
       " 'bre': 65,\n",
       " 'eus': 66,\n",
       " 'kor': 67,\n",
       " 'jpn': 68,\n",
       " 'heb': 69}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_set = set(y_list)\n",
    "inx2lang = dict(enumerate(y_set))\n",
    "lang2inx = {v: k for k, v in inx2lang.items()}\n",
    "lang2inx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 32, 35]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list_num = list(map(lambda x: lang2inx[x], y_list))\n",
    "y_list_num[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode them as one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y_list_num)\n",
    "y[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Sets\n",
    "We create a training and a validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We shuffle the indices\n",
    "We shuffle them again. This is not necessary, as we already shuffled the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1794002, 2745398, 5006546, 365595, 5105077, 6536124, 2713909, 1837034, 6722572, 2354267]\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(X_inx_chars.shape[0]))\n",
    "np.random.shuffle(indices)\n",
    "print(indices[:10])\n",
    "X_inx_chars = X_inx_chars[indices, :]\n",
    "X_freq_chars = X_freq_chars[indices, :]\n",
    "X_inx_bigrams = X_inx_bigrams[indices, :]\n",
    "X_freq_bigrams = X_freq_bigrams[indices, :]\n",
    "X_inx_trigrams = X_inx_trigrams[indices, :]\n",
    "X_freq_trigrams = X_freq_trigrams[indices, :]\n",
    "y = y[indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6347635"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_examples = int(X_inx_chars.shape[0] * 0.8)\n",
    "training_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_inx_chars = X_inx_chars[:training_examples, :]\n",
    "X_train_freq_chars = X_freq_chars[:training_examples, :]\n",
    "\n",
    "X_train_inx_bigrams = X_inx_bigrams[:training_examples, :]\n",
    "X_train_freq_bigrams = X_freq_bigrams[:training_examples, :]\n",
    "\n",
    "X_train_inx_trigrams = X_inx_trigrams[:training_examples, :]\n",
    "X_train_freq_trigrams = X_freq_trigrams[:training_examples, :]\n",
    "\n",
    "y_train = y[:training_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 282  357 1698  402  339  193 2755  763 2714 3806 2468 3737 2492  119\n",
      " 1394    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.125     , 0.125     , 0.08333334, 0.08333334, 0.08333334,\n",
       "       0.08333334, 0.08333334, 0.04166667, 0.04166667, 0.04166667,\n",
       "       0.04166667, 0.04166667, 0.04166667, 0.04166667, 0.04166667,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "print(X_train_inx_chars[0])\n",
    "X_train_freq_chars[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_inx_chars = X_inx_chars[training_examples:, :]\n",
    "X_val_freq_chars = X_freq_chars[training_examples:, :]\n",
    "\n",
    "X_val_inx_bigrams = X_inx_bigrams[training_examples:, :]\n",
    "X_val_freq_bigrams = X_freq_bigrams[training_examples:, :]\n",
    "\n",
    "X_val_inx_trigrams = X_inx_trigrams[training_examples:, :]\n",
    "X_val_freq_trigrams = X_freq_trigrams[training_examples:, :]\n",
    "\n",
    "y_val = y[training_examples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Keras Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create an architecture resembling CLD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_inx_input (InputLayer)     [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bigram_inx_input (InputLayer)   [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "trigram_inx_input (InputLayer)  [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_freq_input (InputLayer)    [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 64)       262208      char_inx_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bigram_freq_input (InputLayer)  [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 60, 64)       524352      bigram_inx_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "trigram_freq_input (InputLayer) [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 70, 64)       524352      trigram_inx_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose (TensorFl [(30, None, 64)]     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           char_freq_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(3,)]               0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_1 (Tensor [(60, None, 64)]     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           bigram_freq_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(3,)]               0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_2 (Tensor [(70, None, 64)]     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           trigram_freq_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(3,)]               0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 30)]         0           char_freq_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(30, None)]         0           tf_op_layer_transpose[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_unstack (TensorFlow [(), ()]             0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_unstack_1 (TensorFl [(), (), ()]         0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 60)]         0           bigram_freq_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(60, None)]         0           tf_op_layer_transpose_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_unstack_2 (TensorFl [(), ()]             0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_unstack_3 (TensorFl [(), (), ()]         0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 70)]         0           trigram_freq_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(70, None)]         0           tf_op_layer_transpose_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_unstack_4 (TensorFl [(), ()]             0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_unstack_5 (TensorFl [(), (), ()]         0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul (TensorFlowO [(None, None)]       0           tf_op_layer_Reshape[0][0]        \n",
      "                                                                 tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_unstack[0][0]        \n",
      "                                                                 tf_op_layer_unstack_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_1 (TensorFlo [(None, None)]       0           tf_op_layer_Reshape_3[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_unstack_2[0][0]      \n",
      "                                                                 tf_op_layer_unstack_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_2 (TensorFlo [(None, None)]       0           tf_op_layer_Reshape_6[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_unstack_4[0][0]      \n",
      "                                                                 tf_op_layer_unstack_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, None, 64)]   0           tf_op_layer_MatMul[0][0]         \n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, None, 64)]   0           tf_op_layer_MatMul_1[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, None, 64)]   0           tf_op_layer_MatMul_2[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 64)]         0           tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 64)]         0           tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 64)]         0           tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 192)          0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
      "                                                                 tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          98816       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 70)           35910       dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,445,638\n",
      "Trainable params: 1,445,638\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import layers, optimizers, backend\n",
    "\n",
    "# Char frequency input\n",
    "char_freq_input = Input(shape=(MAXLEN_CHARS,), dtype='float32', name='char_freq_input')\n",
    "\n",
    "# Char index input\n",
    "char_inx_input = Input(shape=(MAXLEN_CHARS,), dtype='int32', name='char_inx_input')\n",
    "embedded_chars = layers.Embedding(MAX_CHARS + 1, 64, mask_zero=True)(char_inx_input)\n",
    "\n",
    "# The weighted mean\n",
    "flattened_chars = backend.dot(char_freq_input, embedded_chars)[0]\n",
    "\n",
    "# Bigram freq input\n",
    "bigram_freq_input = Input(shape=(MAXLEN_BIGRAMS,), dtype='float32', name='bigram_freq_input')\n",
    "\n",
    "# Bigram index input\n",
    "bigram_inx_input = Input(shape=(MAXLEN_BIGRAMS,), dtype='int32', name='bigram_inx_input')\n",
    "embedded_bigrams = layers.Embedding(MAX_BIGRAMS + 1, 64, mask_zero=True)(bigram_inx_input)\n",
    "\n",
    "# The weighted mean\n",
    "flattened_bigrams = backend.dot(bigram_freq_input, embedded_bigrams)[0]\n",
    "\n",
    "# Trigram freq input\n",
    "trigram_freq_input = Input(shape=(MAXLEN_TRIGRAMS,), dtype='float32', name='trigram_freq_input')\n",
    "\n",
    "# Trigram index input\n",
    "trigram_inx_input = Input(shape=(MAXLEN_TRIGRAMS,), dtype='int32', name='trigram_inx_input')\n",
    "embedded_trigrams = layers.Embedding(MAX_TRIGRAMS + 1, 64, mask_zero=True)(trigram_inx_input)\n",
    "\n",
    "# The weighted mean\n",
    "flattened_trigrams = backend.dot(trigram_freq_input, embedded_trigrams)[0]\n",
    "\n",
    "flattened = layers.concatenate([flattened_chars, flattened_bigrams, flattened_trigrams], axis=-1)\n",
    "\n",
    "dense_layer = layers.Dense(512, activation='relu')(flattened)\n",
    "lang_output = layers.Dense(LANG_NBR, activation='softmax')(dense_layer)\n",
    "\n",
    "model = Model([char_inx_input, char_freq_input, \n",
    "               bigram_inx_input, bigram_freq_input, \n",
    "               trigram_inx_input, trigram_freq_input], lang_output)\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history = model.fit([X_chars, X_bigrams, X_trigrams], y, \\n                    epochs=3,\\n                   validation_split=0.2)'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"history = model.fit([X_chars, X_bigrams, X_trigrams], y, \n",
    "                    epochs=3,\n",
    "                   validation_split=0.2)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6347635 samples, validate on 1586909 samples\n",
      "Epoch 1/3\n",
      "6347635/6347635 [==============================] - 1844s 290us/sample - loss: 0.1094 - acc: 0.9675 - val_loss: 0.0853 - val_acc: 0.9751\n",
      "Epoch 2/3\n",
      "6347635/6347635 [==============================] - 1583s 249us/sample - loss: 0.0789 - acc: 0.9766 - val_loss: 0.0816 - val_acc: 0.9760\n",
      "Epoch 3/3\n",
      "6347635/6347635 [==============================] - 1762s 278us/sample - loss: 0.0739 - acc: 0.9784 - val_loss: 0.0778 - val_acc: 0.9778\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_train_inx_chars, X_train_freq_chars, \n",
    "     X_train_inx_bigrams, X_train_freq_bigrams, \n",
    "     X_train_inx_trigrams, X_train_freq_trigrams], \n",
    "    y_train, \n",
    "    epochs=3,\n",
    "    validation_data=(\n",
    "        [X_val_inx_chars, X_val_freq_chars, \n",
    "         X_val_inx_bigrams, X_val_freq_bigrams,\n",
    "         X_val_inx_trigrams, X_val_freq_trigrams], \n",
    "        y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting and Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1586909/1586909 [==============================] - 118s 75us/sample - loss: 0.0778 - acc: 0.9778\n",
      "Scores: [0.0777907051800631, 0.9778311]\n",
      "loss: 7.78%\n",
      "acc: 97.78%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate([X_val_inx_chars, X_val_freq_chars, \n",
    "                         X_val_inx_bigrams, X_val_freq_bigrams,\n",
    "                         X_val_inx_trigrams, X_val_freq_trigrams], y_val)\n",
    "print('Scores:', scores)\n",
    "list(map(lambda x: print(\"%s: %.2f%%\" % (x[0], x[1] * 100)), zip(model.metrics_names, scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the whole validation set and we get the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.04574880e-14 3.90093231e-13 1.09105097e-12 1.66777966e-10\n",
      "  1.89596563e-17 3.15424228e-13 1.68895632e-13 1.35844103e-12\n",
      "  4.96976219e-14 1.96622052e-09 4.14903707e-13 6.19145706e-02\n",
      "  7.00953988e-19 1.00578479e-16 2.01899409e-15 1.24947997e-19\n",
      "  2.00285308e-16 3.31493527e-14 4.17139793e-13 7.11285510e-22\n",
      "  2.82210753e-15 3.97695270e-17 1.92978324e-13 3.36266892e-15\n",
      "  1.37285040e-12 2.73302206e-19 5.79867154e-10 8.77869652e-11\n",
      "  3.67478807e-15 4.53468182e-13 8.72249686e-08 3.85859407e-18\n",
      "  2.75173967e-10 1.03844492e-15 2.86783560e-18 2.00533745e-09\n",
      "  2.14996666e-11 8.21473011e-12 2.40458890e-12 2.46160194e-20\n",
      "  1.04294897e-11 2.22734910e-16 7.62901027e-12 9.55917833e-20\n",
      "  6.91279309e-14 1.78477545e-16 2.98484189e-11 6.98477980e-16\n",
      "  9.38085258e-01 2.44814185e-15 1.46439675e-16 1.35915944e-15\n",
      "  1.42681165e-16 9.80109785e-15 1.28481627e-21 5.52593301e-12\n",
      "  2.91562399e-16 2.07551407e-10 7.34579811e-18 1.85386941e-17\n",
      "  1.19242561e-12 9.24781862e-10 3.01527609e-16 2.15328650e-15\n",
      "  2.10625703e-14 1.02017362e-11 1.34838243e-14 1.18909240e-18\n",
      "  1.02967165e-16 3.50661052e-17]\n",
      " [9.99999881e-01 5.33399713e-11 1.05748003e-23 2.74878482e-13\n",
      "  2.39131475e-11 4.21307130e-12 2.88012590e-16 4.69676427e-17\n",
      "  3.08750456e-11 1.38094466e-10 4.60679476e-19 5.89353080e-11\n",
      "  6.90072717e-27 1.49235436e-25 9.15455739e-16 4.90687378e-30\n",
      "  7.48623657e-13 2.22071688e-12 1.83967789e-11 6.08938686e-18\n",
      "  8.96916488e-18 2.73987153e-16 2.86208799e-08 1.01464105e-13\n",
      "  3.61662083e-25 1.46615745e-14 2.92545231e-13 2.95962040e-13\n",
      "  6.17734691e-14 1.66417891e-07 8.77591232e-18 1.90860652e-23\n",
      "  2.26179950e-10 2.58580267e-13 4.88352824e-24 1.44576151e-09\n",
      "  5.26528196e-11 4.66752377e-18 2.97587747e-13 4.04867879e-32\n",
      "  5.02841762e-21 3.33921814e-25 3.82603889e-17 6.27902022e-25\n",
      "  1.86805425e-14 2.47505575e-12 3.60923347e-10 1.21838119e-26\n",
      "  5.15080990e-12 1.63253759e-14 1.32253393e-13 3.93838364e-13\n",
      "  1.55338377e-18 7.44227683e-12 1.45999398e-24 4.71379083e-20\n",
      "  2.74174867e-12 3.51416330e-15 7.15347920e-13 2.10047713e-30\n",
      "  2.06673041e-14 3.55534329e-15 9.31406238e-15 1.64118899e-10\n",
      "  1.35541177e-12 2.77019591e-13 1.01941143e-16 7.23331093e-26\n",
      "  1.42525928e-32 1.13975603e-21]\n",
      " [1.74193758e-27 7.41542215e-32 5.05838880e-30 1.08409047e-16\n",
      "  2.52282797e-38 3.49178133e-30 9.90990296e-35 1.79426040e-11\n",
      "  8.43245102e-23 3.36228922e-18 0.00000000e+00 2.04406690e-18\n",
      "  4.80874363e-35 0.00000000e+00 5.67241100e-31 0.00000000e+00\n",
      "  2.64139414e-28 2.35280410e-19 9.71604386e-27 3.55270796e-33\n",
      "  3.59903431e-24 1.00207479e-20 3.26705090e-22 1.09667246e-24\n",
      "  1.00928230e-37 0.00000000e+00 6.02699097e-18 4.54965907e-16\n",
      "  0.00000000e+00 7.07189845e-34 3.15720203e-20 1.39668609e-38\n",
      "  1.44222566e-16 0.00000000e+00 6.54958122e-32 2.31978057e-15\n",
      "  1.00342763e-15 2.56289683e-32 1.81042985e-20 3.84089411e-26\n",
      "  0.00000000e+00 0.00000000e+00 1.88879877e-14 1.45346515e-31\n",
      "  3.23135451e-22 1.40128891e-27 1.00000000e+00 2.28133185e-32\n",
      "  6.21479819e-18 1.19992167e-27 1.82032590e-35 6.36732229e-34\n",
      "  0.00000000e+00 1.02054950e-35 1.58823051e-36 2.14041026e-38\n",
      "  6.95423577e-36 2.35899973e-16 3.68499847e-33 0.00000000e+00\n",
      "  4.16548224e-34 6.32139866e-23 0.00000000e+00 7.30204600e-24\n",
      "  4.13055587e-21 1.68589802e-27 9.92742632e-34 9.36067898e-32\n",
      "  2.61332926e-37 7.67387104e-33]\n",
      " [3.34941682e-22 3.01649608e-14 2.19948049e-19 4.88175944e-09\n",
      "  1.89382852e-13 2.50327088e-16 1.05955241e-14 3.10582442e-20\n",
      "  2.01641794e-11 9.29601465e-11 9.58970453e-20 1.39969535e-14\n",
      "  6.56918719e-28 9.67228123e-30 1.26645588e-13 5.84611766e-22\n",
      "  1.30205629e-11 2.46168064e-08 1.50683932e-09 3.12100910e-16\n",
      "  7.98209439e-17 1.27104119e-10 5.13197679e-14 4.50376418e-22\n",
      "  0.00000000e+00 2.77684586e-09 2.63101274e-08 9.63001668e-15\n",
      "  6.30757721e-14 3.59284675e-17 8.13157180e-12 1.98582298e-26\n",
      "  1.00000000e+00 8.51911374e-20 9.10598805e-20 1.29131331e-10\n",
      "  1.99170547e-08 2.51463780e-11 1.45406745e-19 1.52855405e-26\n",
      "  7.40090571e-22 3.30589912e-24 1.20368528e-20 8.29285925e-22\n",
      "  4.17268449e-20 8.66858939e-15 4.85112883e-09 2.26192397e-22\n",
      "  1.29472455e-13 7.39760900e-13 9.80852555e-09 5.06133950e-16\n",
      "  2.16140507e-17 1.85275610e-19 1.27797574e-30 4.27032900e-22\n",
      "  5.91473260e-16 2.43597975e-10 2.40722502e-16 5.81643790e-24\n",
      "  1.70555792e-10 4.10186750e-14 1.72097573e-17 1.30319285e-13\n",
      "  3.87032247e-12 2.42034214e-12 1.19343432e-13 9.71781783e-28\n",
      "  7.45575792e-24 1.42770955e-30]\n",
      " [1.73626806e-24 2.73891322e-14 6.04882133e-27 8.65211331e-13\n",
      "  1.42091292e-22 3.02918057e-14 6.98907776e-20 5.97041776e-20\n",
      "  9.37768705e-18 8.78721551e-15 4.82724278e-31 6.79554258e-16\n",
      "  1.29363614e-24 1.59102323e-32 1.41974071e-17 4.43891897e-28\n",
      "  2.81771301e-10 9.11282260e-15 1.39887410e-13 7.90530579e-25\n",
      "  5.46934285e-21 6.82593994e-16 2.93960074e-15 3.32407406e-22\n",
      "  2.74838509e-29 2.62602784e-09 1.81503076e-10 1.99549991e-16\n",
      "  1.64442119e-27 3.21644920e-22 1.65261397e-21 4.43375959e-27\n",
      "  1.36305078e-10 1.84724254e-21 9.65895344e-26 9.28716993e-15\n",
      "  1.00000000e+00 1.41912530e-22 2.33675757e-19 3.46770633e-35\n",
      "  9.57628340e-36 8.30712002e-19 4.17411107e-23 8.50715230e-30\n",
      "  3.72832067e-19 3.61052220e-21 2.39232543e-13 6.79124896e-21\n",
      "  5.50551324e-14 1.24490881e-16 2.33346870e-13 1.91138841e-17\n",
      "  1.51093118e-25 2.47589947e-17 6.36569164e-35 2.99440660e-27\n",
      "  1.25108418e-15 2.06584085e-13 6.66207994e-21 5.01756261e-25\n",
      "  3.41855100e-09 4.64202951e-24 7.36944514e-20 1.26031681e-19\n",
      "  1.68757405e-16 1.66115548e-18 4.08528672e-20 9.91535604e-26\n",
      "  8.92749390e-24 5.38319976e-26]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict([X_val_inx_chars, X_val_freq_chars, \n",
    "                             X_val_inx_bigrams, X_val_freq_bigrams,\n",
    "                             X_val_inx_trigrams, X_val_freq_trigrams])\n",
    "print(y_predicted[:5])\n",
    "print(y_val[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names of the predicted and true classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ber', 'toki', 'epo', 'ita', 'spa', 'fin', 'ita', 'rus', 'por', 'epo']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_predicted, axis=-1)\n",
    "list(map(inx2lang.get, y_pred))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ber', 'toki', 'epo', 'ita', 'spa', 'fin', 'ita', 'rus', 'por', 'epo']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_symb = np.argmax(y_val, axis=-1)\n",
    "list(map(inx2lang.get, y_val_symb))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The detailed F1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the precision and recall by language as well as the micro and macro F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_names = sorted(list(lang2inx.keys()), key=lambda x: lang2inx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        toki       1.00      0.98      0.99      7064\n",
      "         ile       0.77      0.79      0.78      1330\n",
      "         bel       0.96      0.93      0.95      2529\n",
      "         fra       0.99      0.99      0.99     80048\n",
      "         run       0.95      0.88      0.91       695\n",
      "         ces       0.96      0.96      0.96      7408\n",
      "         tlh       0.98      0.99      0.98      3379\n",
      "         dan       0.89      0.92      0.90      8820\n",
      "         srp       0.78      0.93      0.85      6063\n",
      "         tur       1.00      1.00      1.00    136529\n",
      "         bul       0.96      0.84      0.89      4723\n",
      "         kab       0.83      0.73      0.78     25803\n",
      "         pes       0.99      0.99      0.99      4287\n",
      "         hin       0.99      0.96      0.97      2514\n",
      "         ido       0.82      0.73      0.77      1005\n",
      "         yue       0.89      0.94      0.91      1082\n",
      "         lfn       0.83      0.82      0.82      1708\n",
      "         lat       0.94      0.91      0.92      6574\n",
      "         ina       0.87      0.85      0.86      4837\n",
      "         vol       0.97      0.91      0.94       676\n",
      "         isl       0.99      0.96      0.97      2184\n",
      "         lit       0.97      0.98      0.98      7765\n",
      "         pol       1.00      0.99      0.99     19924\n",
      "         nds       0.96      0.90      0.93      3545\n",
      "         uig       1.00      0.99      1.00      1536\n",
      "         glg       0.76      0.52      0.62       893\n",
      "         por       0.97      0.98      0.98     69466\n",
      "         deu       0.99      0.99      0.99     98447\n",
      "         cor       0.98      0.94      0.96       749\n",
      "         tgl       0.97      0.96      0.97      2681\n",
      "         rus       0.99      0.99      0.99    145756\n",
      "         mar       0.99      1.00      0.99     10984\n",
      "         ita       0.99      0.99      0.99    148209\n",
      "         tuk       0.97      0.88      0.93      1316\n",
      "         ell       1.00      1.00      1.00      6166\n",
      "         hun       0.99      1.00      0.99     53718\n",
      "         spa       0.97      0.97      0.97     63474\n",
      "         mkd       0.92      0.98      0.95     15457\n",
      "         swe       0.95      0.96      0.95      7137\n",
      "         ben       1.00      1.00      1.00       959\n",
      "         mhr       0.98      0.90      0.94       832\n",
      "         ara       0.99      1.00      1.00      6693\n",
      "         nob       0.82      0.67      0.74      2761\n",
      "         gos       0.87      0.68      0.76       673\n",
      "         nld       0.96      0.97      0.97     20997\n",
      "         hrv       0.78      0.13      0.23      1053\n",
      "         epo       0.99      0.99      0.99    121592\n",
      "         cmn       0.98      0.98      0.98     12188\n",
      "         ber       0.86      0.91      0.88     45177\n",
      "         ron       0.99      0.92      0.95      3816\n",
      "         oci       0.83      0.84      0.84      1127\n",
      "         ind       0.92      0.96      0.94      2489\n",
      "         aze       0.98      0.84      0.90       977\n",
      "         avk       0.96      0.85      0.90       730\n",
      "         afr       0.98      0.56      0.72       800\n",
      "         tat       0.98      0.96      0.97      2738\n",
      "         slk       0.72      0.75      0.73       829\n",
      "         eng       0.99      1.00      1.00    253476\n",
      "         est       0.89      0.76      0.82       580\n",
      "         wuu       0.85      0.80      0.82       848\n",
      "         cat       0.78      0.81      0.79      1183\n",
      "         ukr       0.99      0.96      0.97     30924\n",
      "         vie       0.99      0.99      0.99      2134\n",
      "         jbo       0.98      0.97      0.97      3051\n",
      "         fin       0.99      0.98      0.99     21201\n",
      "         bre       0.89      0.90      0.89      1403\n",
      "         eus       0.86      0.93      0.89      1164\n",
      "         kor       1.00      0.98      0.99      1355\n",
      "         jpn       1.00      1.00      1.00     37712\n",
      "         heb       1.00      1.00      1.00     38966\n",
      "\n",
      "    accuracy                           0.98   1586909\n",
      "   macro avg       0.94      0.90      0.91   1586909\n",
      "weighted avg       0.98      0.98      0.98   1586909\n",
      "\n",
      "Micro F1: 0.9778311169701602\n",
      "Macro F1 0.9125815837901784\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "print(classification_report(y_val_symb, y_pred, target_names=lang_names))\n",
    "print('Micro F1:', f1_score(y_val_symb, y_pred, average='micro'))\n",
    "print('Macro F1', f1_score(y_val_symb, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toki': 0, 'ile': 1, 'bel': 2, 'fra': 3, 'run': 4, 'ces': 5, 'tlh': 6, 'dan': 7, 'srp': 8, 'tur': 9, 'bul': 10, 'kab': 11, 'pes': 12, 'hin': 13, 'ido': 14, 'yue': 15, 'lfn': 16, 'lat': 17, 'ina': 18, 'vol': 19, 'isl': 20, 'lit': 21, 'pol': 22, 'nds': 23, 'uig': 24, 'glg': 25, 'por': 26, 'deu': 27, 'cor': 28, 'tgl': 29, 'rus': 30, 'mar': 31, 'ita': 32, 'tuk': 33, 'ell': 34, 'hun': 35, 'spa': 36, 'mkd': 37, 'swe': 38, 'ben': 39, 'mhr': 40, 'ara': 41, 'nob': 42, 'gos': 43, 'nld': 44, 'hrv': 45, 'epo': 46, 'cmn': 47, 'ber': 48, 'ron': 49, 'oci': 50, 'ind': 51, 'aze': 52, 'avk': 53, 'afr': 54, 'tat': 55, 'slk': 56, 'eng': 57, 'est': 58, 'wuu': 59, 'cat': 60, 'ukr': 61, 'vie': 62, 'jbo': 63, 'fin': 64, 'bre': 65, 'eus': 66, 'kor': 67, 'jpn': 68, 'heb': 69}\n",
      "[[ 6936     5     0 ...     0     0     0]\n",
      " [    0  1045     0 ...     0     0     0]\n",
      " [    0     0  2362 ...     0     0     0]\n",
      " ...\n",
      " [    0     0     0 ...  1333     5     0]\n",
      " [    0     0     0 ...     1 37692     0]\n",
      " [    0     0     0 ...     0     0 38966]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(lang2inx)\n",
    "cf = confusion_matrix(y_val_symb, y_pred)\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent confusions for some languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['fra', 'eng', 'swe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: fra\n",
      "Confusions: [    0    31     0 79236     0     5     2     5     1    11     0     5\n",
      "     0     0     6     0     4    28    64     0     0     0     1     1\n",
      "     0    12   108    34     0     0     1     0    55     0     0    21\n",
      "    76     0     4     0     0     1     1     2     7     1    53     0\n",
      "     7     2    53     1     0     2     0     0     0   151     0     0\n",
      "    24     0     1     4     5    15     7     0     0     0]\n",
      "Most confused: eng 0.0018863681790925444\n",
      "====\n",
      "Language: eng\n",
      "Confusions: [     2      4      0     52      4      3      8     21     10     46\n",
      "      0     11      0      0      4      0      3     31     11      1\n",
      "      0      3      8      6      0      2     51    104      1      7\n",
      "      2      1     54      1      0     51     47      0      7      0\n",
      "      0      1      3     10     61      0     27      0     19      0\n",
      "      6      3      0      1      2      0      0 252745      2      0\n",
      "      5      0      1      1     12     10      8      0      0      3]\n",
      "Most confused: deu 0.00041029525477757266\n",
      "====\n",
      "Language: swe\n",
      "Confusions: [   0    2    0    1    0    0    0  114    4   11    0    2    0    0\n",
      "    1    0    0    0    3    0    2    1    0    4    0    0    3   27\n",
      "    1    3    0    0   12    2    0    8    6    0 6825    0    0    0\n",
      "   32    0   12    0   10    0    4    0    1    2    0    0    0    1\n",
      "    1   29    0    0    2    0    0    0    4    4    3    0    0    0]\n",
      "Most confused: dan 0.015973097940311057\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "for language in languages:\n",
    "    if language not in lang2inx:\n",
    "        continue\n",
    "    print('Language:', language)\n",
    "    print('Confusions:', cf[lang2inx[language]])\n",
    "    print('Most confused:',\n",
    "          inx2lang[np.argsort(cf[lang2inx[language]])[-2]], \n",
    "          np.sort(cf[lang2inx[language]])[-2] / np.sum(cf[lang2inx[language]]))\n",
    "    print('====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us use the model to predict some short texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sentence):\n",
    "    char_items = hash_chars(sentence)\n",
    "    bigram_items = hash_bigrams(sentence)\n",
    "    trigram_items = hash_trigrams(sentence)\n",
    "    return [pad_sequences([char_items[0]], padding='post', maxlen=MAXLEN_CHARS),\n",
    "            pad_sequences([char_items[1]], dtype='float32', padding='post', maxlen=MAXLEN_CHARS),\n",
    "            pad_sequences([bigram_items[0]], padding='post', maxlen=MAXLEN_BIGRAMS),\n",
    "            pad_sequences([bigram_items[1]], dtype='float32', padding='post', maxlen=MAXLEN_BIGRAMS),\n",
    "            pad_sequences([trigram_items[0]], padding='post', maxlen=MAXLEN_TRIGRAMS),\n",
    "            pad_sequences([trigram_items[1]], dtype='float32', padding='post', maxlen=MAXLEN_TRIGRAMS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[3232, 2755, 1698,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]], dtype=int32),\n",
       " array([[0.5       , 0.33333334, 0.16666667, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "       dtype=float32),\n",
       " array([[5340,   70, 4333,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0]], dtype=int32),\n",
       " array([[0.4, 0.4, 0.2, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "         0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "         0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "         0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "         0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]], dtype=float32),\n",
       " array([[1923, 6678, 2369,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]], dtype=int32),\n",
       " array([[0.5 , 0.25, 0.25, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "         0.  , 0.  , 0.  , 0.  ]], dtype=float32)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Banana\"\n",
    "extract_features(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fra'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Salut les gars !\"\n",
    "preds = model.predict(extract_features(sentence))\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eng'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hi guys!\"\n",
    "preds = model.predict(extract_features(sentence))\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'swe'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hejsan grabbar!\"\n",
    "preds = model.predict(extract_features(sentence))\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simplifed model, where we do not use the ngram frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_inx_input (InputLayer)     [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bigram_inx_input (InputLayer)   [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "trigram_inx_input (InputLayer)  [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 30, 64)       262208      char_inx_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 60, 64)       524352      bigram_inx_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 70, 64)       524352      trigram_inx_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 64)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          98816       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 70)           35910       dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,445,638\n",
      "Trainable params: 1,445,638\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Char index input\n",
    "char_inx_input = Input(shape=(MAXLEN_CHARS,), dtype='int32', name='char_inx_input')\n",
    "embedded_chars = layers.Embedding(MAX_CHARS + 1, 64, mask_zero=True)(char_inx_input)\n",
    "\n",
    "# The mean\n",
    "flattened_chars = layers.GlobalAveragePooling1D()(embedded_chars)\n",
    "#flattened_chars = layers.Lambda(lambda x: backend.mean(x, axis=1))(embedded_chars)\n",
    "\n",
    "# Bigram index input\n",
    "bigram_inx_input = Input(shape=(MAXLEN_BIGRAMS,), dtype='int32', name='bigram_inx_input')\n",
    "embedded_bigrams = layers.Embedding(MAX_BIGRAMS + 1, 64, mask_zero=True)(bigram_inx_input)\n",
    "\n",
    "# The mean\n",
    "flattened_bigrams = layers.GlobalAveragePooling1D()(embedded_bigrams)\n",
    "#flattened_bigrams = layers.Lambda(lambda x: backend.mean(x, axis=1))(embedded_bigrams)\n",
    "\n",
    "# Trigram index input\n",
    "trigram_inx_input = Input(shape=(MAXLEN_TRIGRAMS,), dtype='int32', name='trigram_inx_input')\n",
    "embedded_trigrams = layers.Embedding(MAX_TRIGRAMS + 1, 64, mask_zero=True)(trigram_inx_input)\n",
    "\n",
    "# The mean\n",
    "flattened_trigrams = layers.GlobalAveragePooling1D()(embedded_trigrams)\n",
    "#flattened_trigrams = layers.Lambda(lambda x: backend.mean(x, axis=1))(embedded_trigrams)\n",
    "\n",
    "flattened = layers.concatenate([flattened_chars, flattened_bigrams, flattened_trigrams], axis=-1)\n",
    "\n",
    "dense_layer = layers.Dense(512, activation='relu')(flattened)\n",
    "lang_output = layers.Dense(LANG_NBR, activation='softmax')(dense_layer)\n",
    "\n",
    "model2 = Model([char_inx_input, \n",
    "               bigram_inx_input, \n",
    "               trigram_inx_input], lang_output)\n",
    "model2.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6347635 samples, validate on 1586909 samples\n",
      "Epoch 1/3\n",
      "6347635/6347635 [==============================] - 1669s 263us/sample - loss: 0.0736 - acc: 0.9771 - val_loss: 0.0543 - val_acc: 0.9827\n",
      "Epoch 2/3\n",
      "6347635/6347635 [==============================] - 1615s 254us/sample - loss: 0.0506 - acc: 0.9840 - val_loss: 0.0548 - val_acc: 0.9837\n",
      "Epoch 3/3\n",
      "6347635/6347635 [==============================] - 1678s 264us/sample - loss: 0.0479 - acc: 0.9852 - val_loss: 0.0559 - val_acc: 0.9837\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(\n",
    "    [X_train_inx_chars, \n",
    "     X_train_inx_bigrams, \n",
    "     X_train_inx_trigrams], \n",
    "    y_train, \n",
    "    epochs=3,\n",
    "    validation_data=(\n",
    "        [X_val_inx_chars, \n",
    "         X_val_inx_bigrams,\n",
    "         X_val_inx_trigrams], \n",
    "        y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate this new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1586909/1586909 [==============================] - 67s 42us/sample - loss: 0.0559 - acc: 0.9837\n",
      "Scores: [0.055934741905906714, 0.9836752]\n",
      "loss: 5.59%\n",
      "acc: 98.37%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model2.evaluate([X_val_inx_chars, \n",
    "                         X_val_inx_bigrams,\n",
    "                         X_val_inx_trigrams], y_val)\n",
    "print('Scores:', scores)\n",
    "list(map(lambda x: print(\"%s: %.2f%%\" % (x[0], x[1] * 100)), zip(model.metrics_names, scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.62915260e-16 7.68185580e-22 1.53335678e-17 2.29747193e-10\n",
      "  9.84093776e-19 2.59905504e-14 2.49798751e-16 3.80058707e-16\n",
      "  3.60504758e-13 4.88998558e-15 1.78505417e-16 1.51086096e-02\n",
      "  4.34674492e-27 1.89399909e-17 7.38861472e-21 7.34128584e-18\n",
      "  1.06722382e-14 2.28205441e-18 2.39637676e-25 3.07811060e-26\n",
      "  1.40160723e-24 9.31882673e-16 1.90341788e-14 9.61095079e-20\n",
      "  3.64583900e-24 6.41069195e-24 3.70318671e-14 1.09952720e-14\n",
      "  2.85399516e-19 9.84719169e-15 1.39628380e-13 5.12328168e-19\n",
      "  1.47078625e-13 6.22210840e-18 8.35612300e-14 1.47256111e-12\n",
      "  1.29385863e-17 1.57711300e-18 4.91019808e-20 5.62566245e-20\n",
      "  4.93918974e-18 7.11364422e-21 2.64097594e-18 3.74421269e-20\n",
      "  1.69922843e-16 1.25943206e-21 5.95967936e-13 6.22249160e-13\n",
      "  9.84891415e-01 4.47145114e-20 7.13274611e-21 2.22884329e-17\n",
      "  3.84933435e-24 7.91127299e-20 3.45859720e-25 3.99961844e-18\n",
      "  3.47619964e-19 1.09384966e-11 7.12028270e-17 1.19794302e-18\n",
      "  3.27979132e-21 1.29149295e-15 6.68191329e-20 3.66134553e-12\n",
      "  2.47281555e-16 4.29517537e-17 9.66839885e-18 5.45113551e-23\n",
      "  3.63765737e-16 3.59774753e-12]\n",
      " [1.00000000e+00 1.11848898e-11 8.70278784e-14 4.41013718e-15\n",
      "  2.42073546e-15 3.09034330e-12 1.57106538e-18 5.76224139e-17\n",
      "  3.73450104e-10 6.45221073e-13 9.78000322e-27 4.38687385e-11\n",
      "  9.63900664e-18 5.63055743e-19 1.24922297e-17 3.68231485e-18\n",
      "  2.58788127e-16 6.64230320e-16 1.84745448e-13 8.31304780e-19\n",
      "  8.15574093e-27 3.65723019e-17 1.05411657e-09 1.86876334e-18\n",
      "  4.77484790e-30 1.58606026e-17 6.68135079e-12 2.63149248e-17\n",
      "  3.10630308e-17 3.49256446e-09 4.89895832e-17 1.22206423e-17\n",
      "  3.54265339e-10 1.72250875e-14 5.31654912e-18 1.47954159e-13\n",
      "  2.16011774e-11 4.96795253e-16 8.14179451e-17 2.45728600e-26\n",
      "  2.17721025e-25 1.51417188e-20 2.17777955e-21 2.03308224e-22\n",
      "  7.99364686e-16 2.27181945e-12 7.07087167e-12 7.26706253e-16\n",
      "  3.84928200e-12 1.23728771e-14 1.07521558e-15 4.66318061e-11\n",
      "  2.63263915e-23 2.20248353e-14 1.39027465e-24 1.30016063e-18\n",
      "  1.42319090e-15 8.03577722e-16 1.38818460e-12 3.78905428e-20\n",
      "  3.40279774e-15 1.93495782e-19 7.71101696e-19 4.39724923e-09\n",
      "  1.15883621e-13 6.69694368e-19 2.01930927e-14 1.61150668e-29\n",
      "  1.00789538e-18 1.09737359e-20]\n",
      " [6.48535801e-26 1.04569265e-19 1.05824901e-34 3.76679602e-16\n",
      "  2.85753837e-31 1.48382550e-21 3.51780288e-35 3.22509785e-15\n",
      "  2.38952177e-16 7.31408781e-17 0.00000000e+00 3.30554687e-16\n",
      "  1.19442928e-17 5.31544070e-23 8.05561672e-13 4.99971182e-27\n",
      "  7.03307004e-21 1.08435300e-20 9.45051534e-18 8.69349279e-30\n",
      "  2.08284430e-37 2.10886999e-20 1.86586864e-21 1.34714963e-26\n",
      "  1.41457367e-38 4.92066549e-32 1.11680886e-19 1.09938533e-19\n",
      "  3.73231070e-31 1.49468649e-30 3.49125190e-28 1.99091940e-34\n",
      "  7.95998815e-17 7.81583695e-23 2.75598067e-33 1.47772309e-16\n",
      "  2.49867109e-17 1.04161062e-29 4.23349437e-22 2.44777010e-27\n",
      "  9.95091411e-38 9.63512640e-22 2.80364782e-16 2.17585195e-37\n",
      "  1.89615467e-22 5.19828182e-20 1.00000000e+00 1.11945598e-22\n",
      "  2.08459821e-17 1.49372060e-26 3.72899184e-25 4.27922182e-27\n",
      "  0.00000000e+00 7.00685152e-29 5.07328007e-33 3.60286950e-30\n",
      "  3.29943261e-26 5.05525558e-18 6.44305641e-22 1.58038912e-32\n",
      "  1.04050633e-22 1.40293194e-33 0.00000000e+00 6.22112819e-19\n",
      "  6.41862267e-22 2.18355042e-23 2.46712383e-23 6.81982373e-31\n",
      "  3.70293133e-36 4.16478736e-32]\n",
      " [2.43472944e-22 6.88474997e-17 5.88776946e-29 3.13623216e-12\n",
      "  3.33699684e-21 1.04048012e-19 7.21661490e-32 2.07185305e-22\n",
      "  1.05811343e-16 6.29982766e-16 8.97856185e-37 2.55547855e-16\n",
      "  1.89458671e-21 7.78138708e-23 5.47629650e-13 1.28831878e-22\n",
      "  4.91787980e-13 4.45565540e-09 1.61925662e-09 7.29134103e-21\n",
      "  2.04153629e-31 3.78369948e-18 8.74836168e-20 1.29021321e-28\n",
      "  1.49891032e-32 1.28922081e-11 2.22577415e-10 7.73609418e-18\n",
      "  2.41996227e-28 1.74872815e-20 2.59711165e-21 7.00173563e-25\n",
      "  1.00000000e+00 1.35493590e-20 1.08151795e-22 2.18666743e-16\n",
      "  7.63895891e-13 2.90785088e-28 4.35757613e-23 3.55760466e-25\n",
      "  4.12901324e-28 2.12859324e-25 4.01320197e-26 6.70503153e-31\n",
      "  5.06639384e-26 4.71035835e-20 5.93436151e-12 1.46294303e-17\n",
      "  3.98766340e-15 3.54026695e-15 7.03622752e-16 6.03000036e-19\n",
      "  2.43318632e-31 1.70828403e-20 0.00000000e+00 1.70359687e-22\n",
      "  4.40703929e-24 2.03323008e-14 1.06969045e-19 1.75986197e-22\n",
      "  6.17051474e-15 6.14831874e-35 7.15291957e-32 1.10687544e-12\n",
      "  9.16193933e-14 3.98823743e-17 7.78296308e-15 3.69847786e-32\n",
      "  7.00227712e-23 1.92531211e-27]\n",
      " [9.70057611e-32 2.24385751e-16 0.00000000e+00 3.40518701e-15\n",
      "  3.26713813e-37 1.22638459e-16 1.92292369e-30 7.83267904e-21\n",
      "  1.69164627e-25 9.68189943e-26 8.10839377e-29 1.55328927e-18\n",
      "  2.70565104e-22 6.32128919e-27 9.48193453e-25 9.88241840e-28\n",
      "  4.86826285e-22 1.00587531e-25 4.65896297e-20 0.00000000e+00\n",
      "  5.92553334e-29 1.57941120e-34 1.86221172e-23 4.26499257e-28\n",
      "  1.65151944e-26 4.53543336e-08 1.39710500e-11 1.04699042e-18\n",
      "  8.68417617e-38 6.62053296e-28 1.75864290e-27 2.92708831e-24\n",
      "  1.96886439e-13 3.33306829e-27 3.59789327e-25 1.23462199e-17\n",
      "  1.00000000e+00 4.40616205e-33 9.36220739e-26 5.29188690e-32\n",
      "  0.00000000e+00 7.39887548e-14 1.19818600e-23 0.00000000e+00\n",
      "  2.45641573e-25 1.73988666e-33 1.29129325e-18 1.21359059e-17\n",
      "  2.36715893e-15 7.92109878e-31 2.05005267e-14 9.91734574e-29\n",
      "  0.00000000e+00 1.64568666e-22 0.00000000e+00 9.19487939e-38\n",
      "  1.08456445e-19 7.45347800e-18 2.84696926e-34 5.11044946e-21\n",
      "  6.26420929e-11 0.00000000e+00 3.22050813e-25 2.97089229e-17\n",
      "  5.48820821e-31 3.10995553e-24 3.09634954e-27 3.07101543e-32\n",
      "  6.26023253e-22 4.38973829e-25]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model2.predict([X_val_inx_chars, \n",
    "                             X_val_inx_bigrams,\n",
    "                             X_val_inx_trigrams])\n",
    "print(y_predicted[:5])\n",
    "print(y_val[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ber', 'toki', 'epo', 'ita', 'spa', 'fin', 'ita', 'rus', 'por', 'epo']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_predicted, axis=-1)\n",
    "list(map(inx2lang.get, y_pred))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ber', 'toki', 'epo', 'ita', 'spa', 'fin', 'ita', 'rus', 'por', 'epo']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_symb = np.argmax(y_val, axis=-1)\n",
    "list(map(inx2lang.get, y_val_symb))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this model is simpler than the first one, it seems slightly more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        toki       1.00      1.00      1.00      7064\n",
      "         ile       0.91      0.68      0.78      1330\n",
      "         bel       0.97      0.95      0.96      2529\n",
      "         fra       1.00      0.99      0.99     80048\n",
      "         run       0.88      0.96      0.92       695\n",
      "         ces       0.97      0.96      0.97      7408\n",
      "         tlh       0.98      0.99      0.99      3379\n",
      "         dan       0.94      0.88      0.91      8820\n",
      "         srp       0.87      0.88      0.88      6063\n",
      "         tur       1.00      1.00      1.00    136529\n",
      "         bul       0.94      0.90      0.92      4723\n",
      "         kab       0.83      0.81      0.82     25803\n",
      "         pes       0.99      1.00      0.99      4287\n",
      "         hin       0.99      0.98      0.99      2514\n",
      "         ido       0.94      0.74      0.83      1005\n",
      "         yue       0.93      0.93      0.93      1082\n",
      "         lfn       0.79      0.91      0.85      1708\n",
      "         lat       0.93      0.96      0.95      6574\n",
      "         ina       0.89      0.92      0.91      4837\n",
      "         vol       0.95      0.93      0.94       676\n",
      "         isl       0.99      0.97      0.98      2184\n",
      "         lit       0.99      0.98      0.99      7765\n",
      "         pol       0.99      1.00      0.99     19924\n",
      "         nds       0.98      0.91      0.94      3545\n",
      "         uig       1.00      1.00      1.00      1536\n",
      "         glg       0.61      0.71      0.66       893\n",
      "         por       0.98      0.99      0.99     69466\n",
      "         deu       1.00      1.00      1.00     98447\n",
      "         cor       0.94      0.96      0.95       749\n",
      "         tgl       0.98      0.96      0.97      2681\n",
      "         rus       0.99      1.00      0.99    145756\n",
      "         mar       1.00      1.00      1.00     10984\n",
      "         ita       0.99      1.00      0.99    148209\n",
      "         tuk       0.97      0.92      0.94      1316\n",
      "         ell       1.00      1.00      1.00      6166\n",
      "         hun       0.99      1.00      1.00     53718\n",
      "         spa       0.99      0.97      0.98     63474\n",
      "         mkd       0.96      0.97      0.96     15457\n",
      "         swe       0.97      0.95      0.96      7137\n",
      "         ben       1.00      1.00      1.00       959\n",
      "         mhr       0.98      0.90      0.94       832\n",
      "         ara       1.00      0.99      1.00      6693\n",
      "         nob       0.69      0.86      0.77      2761\n",
      "         gos       0.94      0.69      0.80       673\n",
      "         nld       0.97      0.98      0.98     20997\n",
      "         hrv       0.73      0.26      0.38      1053\n",
      "         epo       1.00      1.00      1.00    121592\n",
      "         cmn       0.98      0.99      0.98     12188\n",
      "         ber       0.89      0.91      0.90     45177\n",
      "         ron       0.96      0.97      0.96      3816\n",
      "         oci       0.87      0.89      0.88      1127\n",
      "         ind       0.96      0.97      0.97      2489\n",
      "         aze       0.93      0.89      0.91       977\n",
      "         avk       0.94      0.82      0.88       730\n",
      "         afr       0.90      0.77      0.83       800\n",
      "         tat       0.96      0.98      0.97      2738\n",
      "         slk       0.83      0.72      0.77       829\n",
      "         eng       1.00      1.00      1.00    253476\n",
      "         est       0.84      0.84      0.84       580\n",
      "         wuu       0.89      0.82      0.85       848\n",
      "         cat       0.87      0.88      0.87      1183\n",
      "         ukr       0.98      0.98      0.98     30924\n",
      "         vie       1.00      0.99      0.99      2134\n",
      "         jbo       0.99      0.99      0.99      3051\n",
      "         fin       0.99      0.99      0.99     21201\n",
      "         bre       0.94      0.90      0.92      1403\n",
      "         eus       0.96      0.92      0.94      1164\n",
      "         kor       0.99      0.99      0.99      1355\n",
      "         jpn       1.00      1.00      1.00     37712\n",
      "         heb       1.00      1.00      1.00     38966\n",
      "\n",
      "    accuracy                           0.98   1586909\n",
      "   macro avg       0.94      0.92      0.93   1586909\n",
      "weighted avg       0.98      0.98      0.98   1586909\n",
      "\n",
      "Micro F1: 0.9836751823828588\n",
      "Macro F1 0.9294797696366933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "print(classification_report(y_val_symb, y_pred, target_names=lang_names))\n",
    "print('Micro F1:', f1_score(y_val_symb, y_pred, average='micro'))\n",
    "print('Macro F1', f1_score(y_val_symb, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: fra\n",
      "Confusions: [    0    31     0 79236     0     5     2     5     1    11     0     5\n",
      "     0     0     6     0     4    28    64     0     0     0     1     1\n",
      "     0    12   108    34     0     0     1     0    55     0     0    21\n",
      "    76     0     4     0     0     1     1     2     7     1    53     0\n",
      "     7     2    53     1     0     2     0     0     0   151     0     0\n",
      "    24     0     1     4     5    15     7     0     0     0]\n",
      "Most confused: eng 0.0018863681790925444\n",
      "====\n",
      "Language: eng\n",
      "Confusions: [     2      4      0     52      4      3      8     21     10     46\n",
      "      0     11      0      0      4      0      3     31     11      1\n",
      "      0      3      8      6      0      2     51    104      1      7\n",
      "      2      1     54      1      0     51     47      0      7      0\n",
      "      0      1      3     10     61      0     27      0     19      0\n",
      "      6      3      0      1      2      0      0 252745      2      0\n",
      "      5      0      1      1     12     10      8      0      0      3]\n",
      "Most confused: deu 0.00041029525477757266\n",
      "====\n",
      "Language: swe\n",
      "Confusions: [   0    2    0    1    0    0    0  114    4   11    0    2    0    0\n",
      "    1    0    0    0    3    0    2    1    0    4    0    0    3   27\n",
      "    1    3    0    0   12    2    0    8    6    0 6825    0    0    0\n",
      "   32    0   12    0   10    0    4    0    1    2    0    0    0    1\n",
      "    1   29    0    0    2    0    0    0    4    4    3    0    0    0]\n",
      "Most confused: dan 0.015973097940311057\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "for language in languages:\n",
    "    if language not in lang2inx:\n",
    "        continue\n",
    "    print('Language:', language)\n",
    "    print('Confusions:', cf[lang2inx[language]])\n",
    "    print('Most confused:',\n",
    "          inx2lang[np.argsort(cf[lang2inx[language]])[-2]], \n",
    "          np.sort(cf[lang2inx[language]])[-2] / np.sum(cf[lang2inx[language]]))\n",
    "    print('====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_inx(sentence):\n",
    "    char_items = hash_chars(sentence)\n",
    "    bigram_items = hash_bigrams(sentence)\n",
    "    trigram_items = hash_trigrams(sentence)\n",
    "    return [pad_sequences([char_items[0]], padding='post', maxlen=MAXLEN_CHARS),\n",
    "            pad_sequences([bigram_items[0]], padding='post', maxlen=MAXLEN_BIGRAMS),\n",
    "            pad_sequences([trigram_items[0]], padding='post', maxlen=MAXLEN_TRIGRAMS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[3232, 2755, 1698,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]], dtype=int32),\n",
       " array([[5340,   70, 4333,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0]], dtype=int32),\n",
       " array([[1923, 6678, 2369,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]], dtype=int32)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Banana\"\n",
    "extract_features_inx(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fra'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Salut les gars !\"\n",
    "preds = model2.predict(extract_features_inx(sentence))\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eng'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello guys!\"\n",
    "preds = model2.predict(extract_features_inx(sentence))\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'swe'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hejsan grabbar!\"\n",
    "preds = model2.predict(extract_features_inx(sentence))\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
