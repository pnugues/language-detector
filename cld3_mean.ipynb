{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Implementation of CLD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Pierre Nugues\n",
    "\n",
    "Reimplementation of Google's _Compact language detector_ (CLD3) from a high-level description. Source: ``https://github.com/google/cld3``\n",
    "\n",
    "Still missing:\n",
    "* weighted average of embeddings, instead of average\n",
    "* generator or train_on_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Dataset: *Tatoeba*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tatoeba is a database of texts with language tags. The corpus is available here: https://tatoeba.org/eng/downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the dataset and we split the lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\tcmn\\t我們試試看！',\n",
       " '2\\tcmn\\t我该去睡觉了。',\n",
       " '3\\tcmn\\t你在干什麼啊？',\n",
       " '4\\tcmn\\t這是什麼啊？',\n",
       " '5\\tcmn\\t今天是６月１８号，也是Muiriel的生日！',\n",
       " '6\\tcmn\\t生日快乐，Muiriel！',\n",
       " '7\\tcmn\\tMuiriel现在20岁了。',\n",
       " '8\\tcmn\\t密码是\"Muiriel\"。',\n",
       " '9\\tcmn\\t我很快就會回來。',\n",
       " '10\\tcmn\\t我不知道。']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_raw = open('sentences.csv', encoding='utf8').read().strip()\n",
    "dataset_raw = dataset_raw.split('\\n')\n",
    "dataset_raw[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the fields and we remove possible whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8023136 texts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', 'cmn', '我們試試看！'), ('2', 'cmn', '我该去睡觉了。'), ('3', 'cmn', '你在干什麼啊？')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_raw = list(map(lambda x: tuple(x.split('\\t')), dataset_raw))\n",
    "dataset_raw = list(map(lambda x: tuple(map(str.strip, x)), dataset_raw))\n",
    "print(len(dataset_raw), 'texts')\n",
    "dataset_raw[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pad strings that are less than three characters. If not done, training will crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_raw)):\n",
    "    if len(dataset_raw[i][2]) == 0:\n",
    "        dataset_raw[i] = (dataset_raw[i][0], dataset_raw[i][1], dataset_raw[i][2] + '   ')\n",
    "    if len(dataset_raw[i][2]) == 1: \n",
    "        dataset_raw[i] = (dataset_raw[i][0], dataset_raw[i][1], dataset_raw[i][2] + '  ')\n",
    "    if len(dataset_raw[i][2]) == 2:\n",
    "        dataset_raw[i] = (dataset_raw[i][0], dataset_raw[i][1], dataset_raw[i][2] + ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle, seed\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "shuffle(dataset_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decimate the dataset to have faster training times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('424134', 'rus', 'Столкнулись два грузовика.'),\n",
       " ('911425', 'nds', 'Se hett en Dochter, de Mary heet.'),\n",
       " ('3884161', 'tur', 'Yakışıklı olduğumu düşünüyor musun?')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DECIMATE = True\n",
    "if DECIMATE:\n",
    "    dataset_raw = dataset_raw[:int(len(dataset_raw)/10)]\n",
    "dataset_raw[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The languages. Some texts have no language, and some others are marked with \\\\\\\\N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = set([x[1] for x in dataset_raw])\n",
    "len(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the texts per language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_texts(dataset):\n",
    "    text_counts = {}\n",
    "    for record in dataset:\n",
    "        lang = record[1]\n",
    "        if lang in text_counts:\n",
    "            text_counts[lang] += 1\n",
    "        else:\n",
    "            text_counts[lang] = 1\n",
    "    return text_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Languages with the most examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eng', 126397),\n",
       " ('ita', 73657),\n",
       " ('rus', 73470),\n",
       " ('tur', 67894),\n",
       " ('epo', 61384),\n",
       " ('deu', 49034),\n",
       " ('fra', 40191),\n",
       " ('por', 34628),\n",
       " ('spa', 31495),\n",
       " ('hun', 26840),\n",
       " ('ber', 22608),\n",
       " ('heb', 19673),\n",
       " ('jpn', 18936),\n",
       " ('ukr', 15496),\n",
       " ('kab', 12891),\n",
       " ('fin', 10540),\n",
       " ('nld', 10362),\n",
       " ('pol', 9972),\n",
       " ('mkd', 7813),\n",
       " ('cmn', 6066),\n",
       " ('mar', 5519),\n",
       " ('dan', 4412),\n",
       " ('lit', 3825),\n",
       " ('ces', 3736),\n",
       " ('toki', 3567)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_counts = count_texts(dataset_raw)\n",
    "langs = sorted(text_counts.keys(), key=text_counts.get, reverse=True)\n",
    "[(lang, text_counts[lang]) for lang in langs][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider languages that have more than 3,000 examples in the dataset or we only use those in French, English, and Swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eng', 'ita', 'rus', 'tur', 'epo', 'deu', 'fra', 'por', 'spa', 'hun', 'ber', 'heb', 'jpn', 'ukr', 'kab', 'fin', 'nld', 'pol', 'mkd', 'cmn', 'mar', 'dan', 'lit', 'ces', 'toki', 'ara', 'swe', 'lat', 'ell', 'srp']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMALL_LANGUAGE_SET = False\n",
    "considered_langs = [lang for lang in langs if text_counts[lang] > 3000]\n",
    "if SMALL_LANGUAGE_SET:\n",
    "    considered_langs = ['fra', 'eng', 'swe']\n",
    "print(considered_langs)\n",
    "LANG_NBR = len(considered_langs)\n",
    "LANG_NBR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the texts in these languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('424134', 'rus', 'Столкнулись два грузовика.'),\n",
       " ('3884161', 'tur', 'Yakışıklı olduğumu düşünüyor musun?'),\n",
       " ('6875215', 'hun', 'Nem szenvedtem már eleget?'),\n",
       " ('7488095', 'spa', '¿Viste mi diccionario de italiano?'),\n",
       " ('5672242',\n",
       "  'epo',\n",
       "  'Tomo diras, ke li estas tro laca por relegi la lecionojn.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(filter(lambda x: x[1] in considered_langs, dataset_raw))\n",
    "print(len(dataset))\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Count Characters Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use hash codes to convert ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of codes we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARS = 4096\n",
    "MAX_BIGRAMS = 4096\n",
    "MAX_TRIGRAMS = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the counts as in CLD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(d):\n",
    "    sum_chars = sum(d.values())\n",
    "    d = {k:v/sum_chars for k, v in d.items()}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the hash code and we add one to avoid a value of 0 as it is padding symbol in the subsequent matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def hash_chars(string, lc=True):\n",
    "    if lc:\n",
    "        string = string.lower()\n",
    "    hash_codes = map(lambda x: hash(x) % MAX_CHARS + 1, string)\n",
    "    d = dict(Counter(hash_codes))\n",
    "    d = normalize(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_bigrams(string, lc=True):\n",
    "    if lc:\n",
    "        string = string.lower()\n",
    "    bigrams = [string[i:i + 2] for i in range(len(string) - 1)]\n",
    "    hash_codes = map(lambda x: hash(x) % MAX_BIGRAMS + 1, bigrams)\n",
    "    d = dict(Counter(hash_codes))\n",
    "    d = normalize(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_trigrams(string, lc=True):\n",
    "    if lc:\n",
    "        string = string.lower()\n",
    "    trigrams = [string[i:i + 3] for i in range(len(string) - 2)]\n",
    "    hash_codes = map(lambda x: hash(x) % MAX_TRIGRAMS + 1, trigrams)\n",
    "    d = dict(Counter(hash_codes))\n",
    "    d = normalize(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1654: 0.05,\n",
       " 2304: 0.1,\n",
       " 218: 0.15,\n",
       " 2550: 0.05,\n",
       " 2485: 0.1,\n",
       " 3562: 0.1,\n",
       " 2327: 0.05,\n",
       " 3459: 0.05,\n",
       " 3208: 0.05,\n",
       " 1064: 0.05,\n",
       " 902: 0.05,\n",
       " 933: 0.05,\n",
       " 3095: 0.05,\n",
       " 881: 0.05,\n",
       " 2384: 0.05}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_chars(\"Let's try something.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1965: 0.05263157894736842,\n",
       " 829: 0.10526315789473684,\n",
       " 3227: 0.05263157894736842,\n",
       " 904: 0.05263157894736842,\n",
       " 2287: 0.05263157894736842,\n",
       " 2979: 0.05263157894736842,\n",
       " 1347: 0.05263157894736842,\n",
       " 500: 0.05263157894736842,\n",
       " 2934: 0.05263157894736842,\n",
       " 4036: 0.05263157894736842,\n",
       " 3216: 0.05263157894736842,\n",
       " 1015: 0.05263157894736842,\n",
       " 2406: 0.05263157894736842,\n",
       " 3975: 0.05263157894736842,\n",
       " 2177: 0.05263157894736842,\n",
       " 882: 0.05263157894736842,\n",
       " 3160: 0.05263157894736842,\n",
       " 4038: 0.05263157894736842}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_bigrams(\"Let's try something.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1688: 0.05555555555555555,\n",
       " 654: 0.05555555555555555,\n",
       " 3663: 0.05555555555555555,\n",
       " 807: 0.05555555555555555,\n",
       " 392: 0.05555555555555555,\n",
       " 1911: 0.05555555555555555,\n",
       " 1619: 0.05555555555555555,\n",
       " 3270: 0.05555555555555555,\n",
       " 209: 0.05555555555555555,\n",
       " 3876: 0.05555555555555555,\n",
       " 623: 0.05555555555555555,\n",
       " 89: 0.05555555555555555,\n",
       " 2482: 0.05555555555555555,\n",
       " 3280: 0.05555555555555555,\n",
       " 3686: 0.05555555555555555,\n",
       " 1009: 0.05555555555555555,\n",
       " 414: 0.05555555555555555,\n",
       " 2474: 0.05555555555555555}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_trigrams(\"Let's try something.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the ngrams in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the character, bigram, and trigram counts to the texts. The format is:\n",
    "`(text_id, language_id, text, char_cnt, bigram_cnt, trigram_cnt)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('424134',\n",
       "  'rus',\n",
       "  'Столкнулись два грузовика.',\n",
       "  {1238: 0.07692307692307693,\n",
       "   2141: 0.038461538461538464,\n",
       "   3533: 0.07692307692307693,\n",
       "   734: 0.07692307692307693,\n",
       "   2495: 0.07692307692307693,\n",
       "   834: 0.038461538461538464,\n",
       "   2720: 0.07692307692307693,\n",
       "   3473: 0.07692307692307693,\n",
       "   972: 0.038461538461538464,\n",
       "   3562: 0.07692307692307693,\n",
       "   163: 0.038461538461538464,\n",
       "   1963: 0.07692307692307693,\n",
       "   21: 0.07692307692307693,\n",
       "   3268: 0.038461538461538464,\n",
       "   754: 0.038461538461538464,\n",
       "   439: 0.038461538461538464,\n",
       "   2384: 0.038461538461538464},\n",
       "  {1337: 0.04,\n",
       "   2120: 0.04,\n",
       "   2578: 0.04,\n",
       "   2501: 0.04,\n",
       "   610: 0.04,\n",
       "   1297: 0.04,\n",
       "   1174: 0.04,\n",
       "   197: 0.04,\n",
       "   1318: 0.04,\n",
       "   3016: 0.04,\n",
       "   1897: 0.04,\n",
       "   1492: 0.04,\n",
       "   1026: 0.04,\n",
       "   1681: 0.04,\n",
       "   1808: 0.04,\n",
       "   2214: 0.04,\n",
       "   3251: 0.04,\n",
       "   1094: 0.04,\n",
       "   3057: 0.04,\n",
       "   791: 0.04,\n",
       "   1417: 0.04,\n",
       "   1354: 0.04,\n",
       "   3159: 0.04,\n",
       "   479: 0.04,\n",
       "   83: 0.04},\n",
       "  {2389: 0.041666666666666664,\n",
       "   3029: 0.041666666666666664,\n",
       "   4051: 0.041666666666666664,\n",
       "   1655: 0.041666666666666664,\n",
       "   1843: 0.041666666666666664,\n",
       "   2569: 0.041666666666666664,\n",
       "   3251: 0.041666666666666664,\n",
       "   3513: 0.041666666666666664,\n",
       "   2104: 0.041666666666666664,\n",
       "   1547: 0.041666666666666664,\n",
       "   2186: 0.041666666666666664,\n",
       "   139: 0.041666666666666664,\n",
       "   2469: 0.041666666666666664,\n",
       "   458: 0.041666666666666664,\n",
       "   2874: 0.041666666666666664,\n",
       "   3386: 0.041666666666666664,\n",
       "   1820: 0.041666666666666664,\n",
       "   1067: 0.041666666666666664,\n",
       "   3828: 0.041666666666666664,\n",
       "   2773: 0.041666666666666664,\n",
       "   1062: 0.041666666666666664,\n",
       "   2142: 0.041666666666666664,\n",
       "   2704: 0.041666666666666664,\n",
       "   626: 0.041666666666666664}),\n",
       " ('3884161',\n",
       "  'tur',\n",
       "  'Yakışıklı olduğumu düşünüyor musun?',\n",
       "  {3459: 0.05714285714285714,\n",
       "   2850: 0.02857142857142857,\n",
       "   1862: 0.05714285714285714,\n",
       "   917: 0.08571428571428572,\n",
       "   94: 0.05714285714285714,\n",
       "   1654: 0.05714285714285714,\n",
       "   3562: 0.08571428571428572,\n",
       "   3208: 0.05714285714285714,\n",
       "   961: 0.05714285714285714,\n",
       "   3737: 0.14285714285714285,\n",
       "   45: 0.02857142857142857,\n",
       "   1064: 0.05714285714285714,\n",
       "   1796: 0.08571428571428572,\n",
       "   3095: 0.05714285714285714,\n",
       "   2327: 0.02857142857142857,\n",
       "   2485: 0.02857142857142857,\n",
       "   3480: 0.02857142857142857},\n",
       "  {2147: 0.029411764705882353,\n",
       "   2641: 0.029411764705882353,\n",
       "   270: 0.029411764705882353,\n",
       "   2620: 0.029411764705882353,\n",
       "   2472: 0.029411764705882353,\n",
       "   3742: 0.058823529411764705,\n",
       "   2276: 0.029411764705882353,\n",
       "   982: 0.029411764705882353,\n",
       "   711: 0.029411764705882353,\n",
       "   2150: 0.029411764705882353,\n",
       "   464: 0.029411764705882353,\n",
       "   1850: 0.029411764705882353,\n",
       "   273: 0.029411764705882353,\n",
       "   2569: 0.029411764705882353,\n",
       "   381: 0.029411764705882353,\n",
       "   2881: 0.029411764705882353,\n",
       "   3536: 0.058823529411764705,\n",
       "   96: 0.029411764705882353,\n",
       "   1324: 0.029411764705882353,\n",
       "   2494: 0.029411764705882353,\n",
       "   1355: 0.029411764705882353,\n",
       "   593: 0.029411764705882353,\n",
       "   3828: 0.029411764705882353,\n",
       "   2250: 0.029411764705882353,\n",
       "   4073: 0.029411764705882353,\n",
       "   2566: 0.029411764705882353,\n",
       "   1673: 0.029411764705882353,\n",
       "   159: 0.029411764705882353,\n",
       "   2574: 0.029411764705882353,\n",
       "   2320: 0.029411764705882353,\n",
       "   3423: 0.029411764705882353,\n",
       "   3712: 0.029411764705882353},\n",
       "  {3699: 0.030303030303030304,\n",
       "   1488: 0.030303030303030304,\n",
       "   3524: 0.030303030303030304,\n",
       "   1013: 0.030303030303030304,\n",
       "   1977: 0.030303030303030304,\n",
       "   2519: 0.030303030303030304,\n",
       "   2324: 0.030303030303030304,\n",
       "   2975: 0.030303030303030304,\n",
       "   23: 0.030303030303030304,\n",
       "   3975: 0.030303030303030304,\n",
       "   1057: 0.030303030303030304,\n",
       "   3167: 0.030303030303030304,\n",
       "   3628: 0.030303030303030304,\n",
       "   1236: 0.030303030303030304,\n",
       "   2069: 0.030303030303030304,\n",
       "   846: 0.030303030303030304,\n",
       "   2029: 0.030303030303030304,\n",
       "   679: 0.030303030303030304,\n",
       "   3870: 0.030303030303030304,\n",
       "   2485: 0.030303030303030304,\n",
       "   237: 0.030303030303030304,\n",
       "   3312: 0.030303030303030304,\n",
       "   3721: 0.030303030303030304,\n",
       "   1039: 0.030303030303030304,\n",
       "   2819: 0.030303030303030304,\n",
       "   1046: 0.030303030303030304,\n",
       "   709: 0.030303030303030304,\n",
       "   2128: 0.030303030303030304,\n",
       "   3816: 0.030303030303030304,\n",
       "   280: 0.030303030303030304,\n",
       "   722: 0.030303030303030304,\n",
       "   2430: 0.030303030303030304,\n",
       "   1010: 0.030303030303030304})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_feat = list(map(lambda x: x + (hash_chars(x[-1]), hash_bigrams(x[-1]), hash_trigrams(x[-1])), \n",
    "                              dataset))\n",
    "dataset_feat[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1238, 2141, 3533, 734, 2495, 834, 2720, 3473, 972, 3562, 163, 1963, 21, 3268, 754, 439, 2384])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_feat[0][3].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1337, 2120, 2578, 2501, 610, 1297, 1174, 197, 1318, 3016, 1897, 1492, 1026, 1681, 1808, 2214, 3251, 1094, 3057, 791, 1417, 1354, 3159, 479, 83])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_feat[0][4].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract all the unique ngrams (hash codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = set([item for text in dataset_feat for item in list(text[3].keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_bigrams = set([item for text in dataset_feat for item in list(text[4].keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_trigrams = set([item for text in dataset_feat for item in list(text[5].keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Keras Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of the embedding indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2711"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_char_cnt = len(unique_chars)\n",
    "unique_char_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the max length of the feature vectors for the characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def length(i, j):\n",
    "    return len(dataset_feat[i][j].keys())\n",
    "\n",
    "max([length(i, 3) for i in range(len(dataset_feat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.28117962381451\n",
      "3.7617453433411927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "print(statistics.mean([length(i, 3) for i in range(len(dataset_feat))]))\n",
    "print(statistics.stdev([length(i, 3) for i in range(len(dataset_feat))]))\n",
    "statistics.median([length(i, 3) for i in range(len(dataset_feat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_bigram_cnt = len(unique_bigrams)\n",
    "unique_bigram_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the max length of the feature vectors for the bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([length(i, 4) for i in range(len(dataset_feat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.966491684376354\n",
      "14.0720490075001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(statistics.mean([length(i, 4) for i in range(len(dataset_feat))]))\n",
    "print(statistics.stdev([length(i, 4) for i in range(len(dataset_feat))]))\n",
    "statistics.median([length(i, 4) for i in range(len(dataset_feat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_trigram_cnt = len(unique_trigrams)\n",
    "unique_trigram_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the max length of the feature vectors for the trigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "681"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([length(i, 5) for i in range(len(dataset_feat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.37771063955763\n",
      "19.158929900629644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(statistics.mean([length(i, 5) for i in range(len(dataset_feat))]))\n",
    "print(statistics.stdev([length(i, 5) for i in range(len(dataset_feat))]))\n",
    "statistics.median([length(i, 5) for i in range(len(dataset_feat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN_CHARS = 20\n",
    "MAXLEN_BIGRAMS = 50\n",
    "MAXLEN_TRIGRAMS = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bigram_input (InputLayer)       [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "trigram_input (InputLayer)      [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 20, 64)       262208      char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 50, 64)       262208      bigram_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 70, 64)       262208      trigram_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 64)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 64)           0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          98816       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 30)           15390       dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 900,830\n",
      "Trainable params: 900,830\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import layers, optimizers, backend\n",
    "\n",
    "# Char input\n",
    "char_input = Input(shape=(20,), dtype='int32', name='char_input')\n",
    "embedded_chars = layers.Embedding(MAX_CHARS + 1, 64, mask_zero=True)(char_input)\n",
    "flattened_chars = layers.GlobalAveragePooling1D()(embedded_chars)\n",
    "#flattened_chars = layers.Lambda(lambda x: backend.mean(x, axis=1))(embedded_chars)\n",
    "\n",
    "# Bigram input\n",
    "bigram_input = Input(shape=(50,), dtype='int32', name='bigram_input')\n",
    "embedded_bigrams = layers.Embedding(MAX_BIGRAMS + 1, 64, mask_zero=True)(bigram_input)\n",
    "flattened_bigrams = layers.GlobalAveragePooling1D()(embedded_bigrams)\n",
    "#flattened_bigrams = layers.Lambda(lambda x: backend.mean(x, axis=1))(embedded_bigrams)\n",
    "\n",
    "# Trigram input\n",
    "trigram_input = Input(shape=(70,), dtype='int32', name='trigram_input')\n",
    "embedded_trigrams = layers.Embedding(MAX_TRIGRAMS + 1, 64, mask_zero=True)(trigram_input)\n",
    "flattened_trigrams = layers.GlobalAveragePooling1D()(embedded_trigrams)\n",
    "#flattened_trigrams = layers.Lambda(lambda x: backend.mean(x, axis=1))(embedded_trigrams)\n",
    "\n",
    "flattened = layers.concatenate([flattened_chars, flattened_bigrams, flattened_trigrams], axis=-1)\n",
    "dense_layer = layers.Dense(512, activation='relu')(flattened)\n",
    "# dense_layer = layers.Dropout(0.6)(dense_layer)\n",
    "lang_output = layers.Dense(LANG_NBR, activation='softmax')(dense_layer)\n",
    "model = Model([char_input, bigram_input, trigram_input], lang_output)\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building $\\mathbf{X}$ and $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $\\mathbf{X}$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the keys (hash codes) from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1238, 2141, 3533, 734, 2495, 834, 2720, 3473, 972, 3562, 163, 1963, 21, 3268, 754, 439, 2384], [3459, 2850, 1862, 917, 94, 1654, 3562, 3208, 961, 3737, 45, 1064, 1796, 3095, 2327, 2485, 3480]]\n"
     ]
    }
   ],
   "source": [
    "X_list_chars = [list(dataset_feat[i][3].keys()) for i in range(len(dataset_feat))]\n",
    "print(X_list_chars[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1337, 2120, 2578, 2501, 610, 1297, 1174, 197, 1318, 3016, 1897, 1492, 1026, 1681, 1808, 2214, 3251, 1094, 3057, 791, 1417, 1354, 3159, 479, 83], [2147, 2641, 270, 2620, 2472, 3742, 2276, 982, 711, 2150, 464, 1850, 273, 2569, 381, 2881, 3536, 96, 1324, 2494, 1355, 593, 3828, 2250, 4073, 2566, 1673, 159, 2574, 2320, 3423, 3712]]\n"
     ]
    }
   ],
   "source": [
    "X_list_bigrams = [list(dataset_feat[i][4].keys()) for i in range(len(dataset_feat))]\n",
    "print(X_list_bigrams[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2389, 3029, 4051, 1655, 1843, 2569, 3251, 3513, 2104, 1547, 2186, 139, 2469, 458, 2874, 3386, 1820, 1067, 3828, 2773, 1062, 2142, 2704, 626], [3699, 1488, 3524, 1013, 1977, 2519, 2324, 2975, 23, 3975, 1057, 3167, 3628, 1236, 2069, 846, 2029, 679, 3870, 2485, 237, 3312, 3721, 1039, 2819, 1046, 709, 2128, 3816, 280, 722, 2430, 1010]]\n"
     ]
    }
   ],
   "source": [
    "X_list_trigrams = [list(dataset_feat[i][5].keys()) for i in range(len(dataset_feat))]\n",
    "print(X_list_trigrams[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we pad the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1238, 2141, 3533,  734, 2495,  834, 2720, 3473,  972, 3562,  163,\n",
       "        1963,   21, 3268,  754,  439, 2384,    0,    0,    0],\n",
       "       [3459, 2850, 1862,  917,   94, 1654, 3562, 3208,  961, 3737,   45,\n",
       "        1064, 1796, 3095, 2327, 2485, 3480,    0,    0,    0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_chars = pad_sequences(X_list_chars, padding='post', maxlen=MAXLEN_CHARS)\n",
    "X_chars[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1337, 2120, 2578, 2501,  610, 1297, 1174,  197, 1318, 3016, 1897,\n",
       "        1492, 1026, 1681, 1808, 2214, 3251, 1094, 3057,  791, 1417, 1354,\n",
       "        3159,  479,   83,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [2147, 2641,  270, 2620, 2472, 3742, 2276,  982,  711, 2150,  464,\n",
       "        1850,  273, 2569,  381, 2881, 3536,   96, 1324, 2494, 1355,  593,\n",
       "        3828, 2250, 4073, 2566, 1673,  159, 2574, 2320, 3423, 3712,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bigrams = pad_sequences(X_list_bigrams, padding='post', maxlen=MAXLEN_BIGRAMS)\n",
    "X_bigrams[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2389, 3029, 4051, 1655, 1843, 2569, 3251, 3513, 2104, 1547, 2186,\n",
       "         139, 2469,  458, 2874, 3386, 1820, 1067, 3828, 2773, 1062, 2142,\n",
       "        2704,  626,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [3699, 1488, 3524, 1013, 1977, 2519, 2324, 2975,   23, 3975, 1057,\n",
       "        3167, 3628, 1236, 2069,  846, 2029,  679, 3870, 2485,  237, 3312,\n",
       "        3721, 1039, 2819, 1046,  709, 2128, 3816,  280,  722, 2430, 1010,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trigrams = pad_sequences(X_list_trigrams, padding='post', maxlen=MAXLEN_TRIGRAMS)\n",
    "X_trigrams[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $\\mathbf{y}$ vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rus', 'tur', 'hun', 'spa', 'epo', 'jpn', 'eng', 'epo', 'heb', 'jpn']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list = [dataset_feat[i][1] for i in range(len(dataset_feat))]\n",
    "y_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eng': 0,\n",
       " 'por': 1,\n",
       " 'fin': 2,\n",
       " 'epo': 3,\n",
       " 'ukr': 4,\n",
       " 'nld': 5,\n",
       " 'hun': 6,\n",
       " 'ber': 7,\n",
       " 'tur': 8,\n",
       " 'rus': 9,\n",
       " 'ita': 10,\n",
       " 'cmn': 11,\n",
       " 'jpn': 12,\n",
       " 'lit': 13,\n",
       " 'ara': 14,\n",
       " 'dan': 15,\n",
       " 'ces': 16,\n",
       " 'spa': 17,\n",
       " 'heb': 18,\n",
       " 'toki': 19,\n",
       " 'swe': 20,\n",
       " 'ell': 21,\n",
       " 'mkd': 22,\n",
       " 'deu': 23,\n",
       " 'mar': 24,\n",
       " 'srp': 25,\n",
       " 'fra': 26,\n",
       " 'lat': 27,\n",
       " 'kab': 28,\n",
       " 'pol': 29}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_set = set(y_list)\n",
    "inx2lang = dict(enumerate(y_set))\n",
    "lang2inx = {v: k for k, v in inx2lang.items()}\n",
    "lang2inx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 8, 6]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list_num = list(map(lambda x: lang2inx[x], y_list))\n",
    "y_list_num[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode them as one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y_list_num)\n",
    "y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"history = model.fit([X_chars, X_bigrams, X_trigrams], y, \n",
    "                    epochs=3,\n",
    "                   validation_split=0.2)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We shuffle the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[347592, 272856, 708757, 679749, 20495, 471207, 10591, 12316, 174363, 90159]\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(X_chars.shape[0]))\n",
    "np.random.shuffle(indices)\n",
    "print(indices[:10])\n",
    "X_chars = X_chars[indices, :]\n",
    "X_bigrams = X_bigrams[indices, :]\n",
    "X_trigrams = X_trigrams[indices, :]\n",
    "y = np.array(y)[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = int(X_chars.shape[0] * 0.8)\n",
    "\n",
    "X_train_chars = X_chars[:training_examples, :]\n",
    "X_train_bigrams = X_bigrams[:training_examples, :]\n",
    "X_train_trigrams = X_trigrams[:training_examples, :]\n",
    "\n",
    "y_train = y[:training_examples]\n",
    "\n",
    "X_val_chars = X_chars[training_examples:, :]\n",
    "X_val_bigrams = X_bigrams[training_examples:, :]\n",
    "X_val_trigrams = X_trigrams[training_examples:, :]\n",
    "\n",
    "y_val = y[training_examples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2664, 2850,  933, 3562,  218, 2304, 3095, 2976, 3208, 1654, 1802,\n",
       "       2327, 3373,  961, 3737, 2384,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_chars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 605318 samples, validate on 151330 samples\n",
      "Epoch 1/3\n",
      "605318/605318 [==============================] - 150s 248us/sample - loss: 0.1298 - acc: 0.9599 - val_loss: 0.0680 - val_acc: 0.9773\n",
      "Epoch 2/3\n",
      "605318/605318 [==============================] - 169s 279us/sample - loss: 0.0495 - acc: 0.9832 - val_loss: 0.0590 - val_acc: 0.9813\n",
      "Epoch 3/3\n",
      "605318/605318 [==============================] - 175s 288us/sample - loss: 0.0378 - acc: 0.9867 - val_loss: 0.0604 - val_acc: 0.9811\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_chars, X_train_bigrams, X_train_trigrams], \n",
    "                    y_train, \n",
    "                    epochs=3,\n",
    "                    validation_data=([X_val_chars, X_val_bigrams, X_val_trigrams], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.8083164e-07 2.8835956e-10 2.3617963e-10 1.8801352e-10 1.0107810e-13\n",
      "  1.2335891e-11 6.7405428e-09 6.4613956e-01 6.6538174e-05 2.0980557e-09\n",
      "  3.3848793e-10 1.9140409e-06 6.9774308e-07 2.5517316e-08 2.7330510e-10\n",
      "  2.2839211e-09 1.9437271e-11 1.2512748e-10 2.6236424e-13 3.4532625e-16\n",
      "  2.7099106e-10 7.3032055e-14 5.9510047e-13 7.2904336e-09 1.3161155e-14\n",
      "  4.7616758e-11 1.3070505e-09 5.3681006e-08 3.5379082e-01 9.8696962e-13]\n",
      " [6.7519935e-15 2.0320501e-08 1.1011691e-08 3.1073802e-07 9.9984944e-01\n",
      "  5.5877412e-12 5.6106331e-10 2.8939251e-09 1.1841821e-11 5.6283725e-06\n",
      "  1.0962262e-07 5.2893211e-07 5.0946074e-09 4.7430365e-10 9.9601803e-12\n",
      "  2.3856728e-17 1.2085843e-07 2.8845143e-08 1.4241751e-04 7.9641209e-13\n",
      "  1.5744631e-12 1.9381034e-17 3.9636756e-09 7.5677650e-14 2.9867769e-07\n",
      "  1.0283632e-06 1.5745734e-13 4.0543791e-15 1.2137009e-11 3.4414029e-09]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]]\n",
      "151330/151330 [==============================] - 6s 41us/sample - loss: 0.0604 - acc: 0.98111s \n",
      "Scores: [0.06037130538926727, 0.98114717]\n",
      "loss: 6.04%\n",
      "acc: 98.11%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = model.predict([X_val_chars, X_val_bigrams, X_val_trigrams])\n",
    "print(y_predicted[:2])\n",
    "print(y_val[:2])\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate([X_val_chars, X_val_bigrams, X_val_trigrams], y_val)\n",
    "print('Scores:', scores)\n",
    "list(map(lambda x: print(\"%s: %.2f%%\" % (x[0], x[1] * 100)), zip(model.metrics_names, scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indices of the predicted and true classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ber', 'ukr', 'eng', 'tur', 'deu', 'ita', 'tur', 'rus', 'kab', 'ber']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_predicted, axis=-1)\n",
    "list(map(inx2lang.get, y_pred))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ber', 'ukr', 'eng', 'tur', 'deu', 'ita', 'tur', 'rus', 'kab', 'ber']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_symb = np.argmax(y_val, axis=-1)\n",
    "list(map(inx2lang.get, y_val_symb))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The detailed F1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eng': 0,\n",
       " 'por': 1,\n",
       " 'fin': 2,\n",
       " 'epo': 3,\n",
       " 'ukr': 4,\n",
       " 'nld': 5,\n",
       " 'hun': 6,\n",
       " 'ber': 7,\n",
       " 'tur': 8,\n",
       " 'rus': 9,\n",
       " 'ita': 10,\n",
       " 'cmn': 11,\n",
       " 'jpn': 12,\n",
       " 'lit': 13,\n",
       " 'ara': 14,\n",
       " 'dan': 15,\n",
       " 'ces': 16,\n",
       " 'spa': 17,\n",
       " 'heb': 18,\n",
       " 'toki': 19,\n",
       " 'swe': 20,\n",
       " 'ell': 21,\n",
       " 'mkd': 22,\n",
       " 'deu': 23,\n",
       " 'mar': 24,\n",
       " 'srp': 25,\n",
       " 'fra': 26,\n",
       " 'lat': 27,\n",
       " 'kab': 28,\n",
       " 'pol': 29}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang2inx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_names = sorted(list(lang2inx.keys()), key=lambda x: lang2inx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         eng       0.99      1.00      1.00     25105\n",
      "         por       0.97      0.99      0.98      6952\n",
      "         fin       0.99      0.99      0.99      2175\n",
      "         epo       0.99      1.00      0.99     12157\n",
      "         ukr       0.96      0.97      0.97      3069\n",
      "         nld       0.98      0.97      0.98      2086\n",
      "         hun       0.99      0.99      0.99      5308\n",
      "         ber       0.80      0.93      0.86      4594\n",
      "         tur       1.00      0.99      1.00     13468\n",
      "         rus       0.99      0.99      0.99     14629\n",
      "         ita       0.99      0.99      0.99     14844\n",
      "         cmn       0.95      0.93      0.94      1239\n",
      "         jpn       0.99      0.99      0.99      3874\n",
      "         lit       0.96      0.98      0.97       743\n",
      "         ara       0.99      0.99      0.99       714\n",
      "         dan       0.94      0.96      0.95       887\n",
      "         ces       0.97      0.97      0.97       785\n",
      "         spa       0.99      0.96      0.97      6303\n",
      "         heb       1.00      1.00      1.00      3959\n",
      "        toki       1.00      0.99      1.00       717\n",
      "         swe       0.98      0.91      0.94       663\n",
      "         ell       1.00      0.99      1.00       616\n",
      "         mkd       0.97      0.97      0.97      1563\n",
      "         deu       1.00      0.99      0.99      9864\n",
      "         mar       0.99      1.00      1.00      1129\n",
      "         srp       0.92      0.88      0.90       571\n",
      "         fra       0.99      0.99      0.99      8074\n",
      "         lat       0.95      0.90      0.92       654\n",
      "         kab       0.83      0.61      0.70      2594\n",
      "         pol       0.99      0.99      0.99      1994\n",
      "\n",
      "    accuracy                           0.98    151330\n",
      "   macro avg       0.97      0.96      0.96    151330\n",
      "weighted avg       0.98      0.98      0.98    151330\n",
      "\n",
      "0.9811471618317583\n",
      "0.9639535722835301\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "print(classification_report(y_val_symb, y_pred, target_names=lang_names))\n",
    "print(f1_score(y_val_symb, y_pred, average='micro'))\n",
    "print(f1_score(y_val_symb, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eng': 0, 'por': 1, 'fin': 2, 'epo': 3, 'ukr': 4, 'nld': 5, 'hun': 6, 'ber': 7, 'tur': 8, 'rus': 9, 'ita': 10, 'cmn': 11, 'jpn': 12, 'lit': 13, 'ara': 14, 'dan': 15, 'ces': 16, 'spa': 17, 'heb': 18, 'toki': 19, 'swe': 20, 'ell': 21, 'mkd': 22, 'deu': 23, 'mar': 24, 'srp': 25, 'fra': 26, 'lat': 27, 'kab': 28, 'pol': 29}\n",
      "[[25059     6     1     2     0     4     5     5     1     0     7     0\n",
      "      0     0     0     3     0     2     0     0     0     0     0     1\n",
      "      0     0     6     0     3     0]\n",
      " [    0  6879     1     7     0     0     0     1     2     0    21     0\n",
      "      0     1     0     1     1    27     0     0     0     0     0     0\n",
      "      0     0     2     8     1     0]\n",
      " [    1     0  2144     4     0     2     0     2     4     0     4     0\n",
      "      1     6     0     0     0     0     0     0     0     0     0     1\n",
      "      0     0     3     1     2     0]\n",
      " [    7     8     2 12113     0     0     1     3     0     0     7     0\n",
      "      0     2     0     1     0     6     0     0     0     0     0     0\n",
      "      0     2     1     2     2     0]\n",
      " [    0     0     0     1  2978     0     0     0     0    70     0     0\n",
      "      1     0     0     0     0     0     2     0     0     0    11     0\n",
      "      0     6     0     0     0     0]\n",
      " [   19     0     0     3     0  2028     2     2     2     0     0     0\n",
      "      0     0     0     9     0     0     0     0     1     1     0    17\n",
      "      0     0     1     1     0     0]\n",
      " [    5     3     3     3     0     2  5274     9     0     0     0     1\n",
      "      0     0     0     1     0     0     0     0     1     0     0     1\n",
      "      0     1     1     0     2     1]\n",
      " [    3     2     0     1     0     1     0  4285     1     0     2     1\n",
      "      1     0     0     1     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0   296     0]\n",
      " [    9     5     5     3     0     1     4    10 13398     0     5     2\n",
      "      0     7     0     3     0     2     0     0     2     0     0     1\n",
      "      0     3     2     2     3     1]\n",
      " [    0     0     0     0   102     0     2     1     1 14482     1     5\n",
      "      1     0     0     0     0     0     0     0     0     0    25     1\n",
      "      0     8     0     0     0     0]\n",
      " [   12    24     1    11     0     0     0     3     2     0 14754     1\n",
      "      0     1     0     2     0    19     0     0     2     0     0     0\n",
      "      0     3     9     0     0     0]\n",
      " [    0     1     2     1     2     0     0     3     2     3     2  1149\n",
      "     48     0     3     0     3     1     8     0     1     0     0     1\n",
      "      5     2     0     0     2     0]\n",
      " [    1     0     0     0     0     0     0     0     1     1     0    32\n",
      "   3836     0     0     0     0     0     1     0     0     0     0     0\n",
      "      1     0     0     0     0     1]\n",
      " [    2     1     0     4     0     1     0     1     1     0     1     0\n",
      "      0   725     0     0     2     1     0     0     0     0     0     0\n",
      "      0     1     0     2     0     1]\n",
      " [    0     0     0     0     0     0     0     0     0     1     0     5\n",
      "      2     0   704     0     0     0     1     0     0     0     0     0\n",
      "      0     0     1     0     0     0]\n",
      " [    7     0     0     0     0     1     2     3     1     0     1     0\n",
      "      0     1     0   853     0     1     0     0     8     0     0     8\n",
      "      0     0     0     1     0     0]\n",
      " [    0     1     0     1     0     1     3     1     1     0     1     0\n",
      "      0     2     0     0   763     2     0     1     0     0     0     0\n",
      "      0     3     1     1     0     3]\n",
      " [   16   115     0    16     0     0     5     3     0     0    60     1\n",
      "      0     2     1     0     1  6062     0     0     0     0     0     1\n",
      "      0     0    12     8     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     5\n",
      "      1     0     0     0     0     0  3953     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [    0     0     1     2     0     0     0     0     2     0     0     0\n",
      "      0     0     0     0     0     0     0   712     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [    9     0     3     2     0     2     2     3     1     0     0     0\n",
      "      0     2     0    27     0     0     0     0   606     0     0     3\n",
      "      0     2     0     0     1     0]\n",
      " [    0     0     0     0     0     0     0     1     0     0     0     1\n",
      "      0     0     0     0     1     0     0     0     0   612     0     0\n",
      "      0     0     1     0     0     0]\n",
      " [    0     0     0     0    16     0     0     1     0    18     0     1\n",
      "      0     0     0     0     0     0     0     0     0     0  1513     0\n",
      "      0    14     0     0     0     0]\n",
      " [   18     1     2     8     0    21     1     5     1     0     0     0\n",
      "      0     2     0     4     0     0     0     0     0     0     0  9795\n",
      "      0     0     2     2     1     1]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      1     0     0     0     0     0     0     0     0     0     0     0\n",
      "   1128     0     0     0     0     0]\n",
      " [    0     1     1     5     3     4     2     0     1    10     2     0\n",
      "      0     3     0     1    13     0     0     0     0     0    17     0\n",
      "      0   504     0     1     0     3]\n",
      " [    9     7     0     1     0     0     0     2     0     0    10     0\n",
      "      0     3     0     0     0     7     0     0     0     0     0     0\n",
      "      0     0  8028     4     3     0]\n",
      " [    9     5     1    11     0     2     0     4     4     0    16     1\n",
      "      0     0     0     0     0     5     0     1     0     0     0     1\n",
      "      0     0     3   590     1     0]\n",
      " [    2     1     0     0     0     0     0  1013     4     0     0     0\n",
      "      0     0     0     2     0     0     0     0     0     1     0     0\n",
      "      0     0     0     1  1570     0]\n",
      " [    5     1     0     1     0     0     1     0     0     0     1     0\n",
      "      0     1     0     1     2     0     0     0     0     0     0     0\n",
      "      0     1     0     0     0  1980]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(lang2inx)\n",
    "cf = confusion_matrix(y_val_symb, y_pred)\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent confusions for some languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['fra', 'eng', 'swe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: fra\n",
      "Confusions: [   9    7    0    1    0    0    0    2    0    0   10    0    0    3\n",
      "    0    0    0    7    0    0    0    0    0    0    0    0 8028    4\n",
      "    3    0]\n",
      "Most confused: ita 0.0012385434728758979\n",
      "====\n",
      "Language: eng\n",
      "Confusions: [25059     6     1     2     0     4     5     5     1     0     7     0\n",
      "     0     0     0     3     0     2     0     0     0     0     0     1\n",
      "     0     0     6     0     3     0]\n",
      "Most confused: ita 0.00027882891854212307\n",
      "====\n",
      "Language: swe\n",
      "Confusions: [  9   0   3   2   0   2   2   3   1   0   0   0   0   2   0  27   0   0\n",
      "   0   0 606   0   0   3   0   2   0   0   1   0]\n",
      "Most confused: dan 0.04072398190045249\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "for language in languages:\n",
    "    if language not in lang2inx:\n",
    "        continue\n",
    "    print('Language:', language)\n",
    "    print('Confusions:', cf[lang2inx[language]])\n",
    "    print('Most confused:',\n",
    "          inx2lang[np.argsort(cf[lang2inx[language]])[-2]], \n",
    "          np.sort(cf[lang2inx[language]])[-2] / np.sum(cf[lang2inx[language]]))\n",
    "    print('====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fra'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Salut les gars !\"\n",
    "preds= model.predict(\n",
    "    [pad_sequences([list(hash_chars(sentence).keys())], padding='post', maxlen=MAXLEN_CHARS),\n",
    "    pad_sequences([list(hash_bigrams(sentence).keys())], padding='post', maxlen=MAXLEN_BIGRAMS),\n",
    "    pad_sequences([list(hash_trigrams(sentence).keys())], padding='post', maxlen=MAXLEN_TRIGRAMS)])\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eng'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hi guys!\"\n",
    "preds= model.predict(\n",
    "    [pad_sequences([list(hash_chars(sentence).keys())], padding='post', maxlen=MAXLEN_CHARS),\n",
    "    pad_sequences([list(hash_bigrams(sentence).keys())], padding='post', maxlen=MAXLEN_BIGRAMS),\n",
    "    pad_sequences([list(hash_trigrams(sentence).keys())], padding='post', maxlen=MAXLEN_TRIGRAMS)])\n",
    "inx2lang[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hun'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hejsan grabbar!\"\n",
    "preds= model.predict(\n",
    "    [pad_sequences([list(hash_chars(sentence).keys())], padding='post', maxlen=MAXLEN_CHARS),\n",
    "    pad_sequences([list(hash_bigrams(sentence).keys())], padding='post', maxlen=MAXLEN_BIGRAMS),\n",
    "    pad_sequences([list(hash_trigrams(sentence).keys())], padding='post', maxlen=MAXLEN_TRIGRAMS)])\n",
    "inx2lang[np.argmax(preds)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
